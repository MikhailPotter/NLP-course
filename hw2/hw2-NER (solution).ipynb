{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ6XVMZYzRgU"
      },
      "source": [
        "#### Домашнее задание 2\n",
        "## Named Entety Recognition and Event Extraction from Literary Fiction\n",
        "\n",
        "## ГРУППА: Гончаров Михаил, Павлова Арина, Разумовский Дмитрий\n",
        "\n",
        "В этом домашнем задании вы будете работать с корпусом LitBank. Корпус собран из популярных художественных произведений на английском языке и содержит разметку по именованным сущностям и событиям. Объем корпуса таков: 100 текстов по примерно 2000 слов каждый.\n",
        "\n",
        "Корпус описан в статьях:\n",
        "* David Bamman, Sejal Popat, Sheng Shen, An Annotated Dataset of Literary Entities http://people.ischool.berkeley.edu/~dbamman/pubs/pdf/naacl2019_literary_entities.pdf\n",
        "* Matthew Sims, Jong Ho Park, David Bamman, Literary Event Detection,  http://people.ischool.berkeley.edu/~dbamman/pubs/pdf/acl2019_literary_events.pdf\n",
        "\n",
        "Корпус доступен в репозитории проекта:  https://github.com/dbamman/litbank\n",
        "\n",
        "Статья и код, использованный для извлечения именованных сущностей:\n",
        "* Meizhi Ju, Makoto Miwa and Sophia Ananiadou, A Neural Layered Model for Nested Named Entity Recognition, https://github.com/meizhiju/layered-bilstm-crf\n",
        "\n",
        "Структура корпуса устроена так.\n",
        "Первый уровень:\n",
        "* entities -- разметка по сущностям\n",
        "* events -- разметка по сущностям\n",
        "\n",
        "\n",
        "В корпусе используются 6 типов именованных сущностей: PER, LOC, ORG, FAC, GPE, VEH (имена, локации, организации, помещения, топонимы, средства перемещния), допускаются вложенные сущности.\n",
        "\n",
        "События выражается одним словом - *триггером*, которое может быть глагом, прилагательным и существительным. В корпусе описаны события, которые действительно происходят и не имеют гипотетического характера.\n",
        "Пример: she *walked* rapidly and resolutely, здесь *walked* -- триггер события. Типы событий не заданы.\n",
        "\n",
        "\n",
        "\n",
        "Второй уровень:\n",
        "* brat -- рабочие файлы инструмента разметки brat, ann-файлы содержат разметку, txt-файлы – сырые тексты\n",
        "* tsv -- tsv-файлы содержат разметку в IOB формате,\n",
        "\n",
        "\n",
        "В статье и репозитории вы найдете идеи, которые помогут вам выполнить домашнее задание. Их стоит воспринимать как руководство к действию, и не стоит их копировать и переиспользовать. Обученные модели использовать не нужно, код для их обучения можно использовать как подсказку.\n",
        "\n",
        "## ПРАВИЛА\n",
        "\n",
        "1. Домашнее задание можно выполнять в группе до 3-х человек.\n",
        "2. Домашнее задание сдается через anytask.\n",
        "3. Домашнее задание оформляется в виде отчета либо в .pdf файле, либо ipython-тетрадке.\n",
        "4. Отчет должен содержать: нумерацию заданий и пунктов, которые вы выполнили, код решения, и понятное пошаговое описание того, что вы сделали. Отчет должен быть написан в академическом стиле, без излишнего использования сленга и с соблюдением норм русского языка.\n",
        "5. Не стоит копировать фрагменты лекций, статей и Википедии в ваш отчет.\n",
        "6. Отчеты, состоящие исключительно из кода, не будут проверены и будут автоматически оценены нулевой оценкой.\n",
        "7. Плагиат и любое недобросоветсное цитирование приводит к обнуление оценки."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CchqBqgFY7hM"
      },
      "source": [
        "## Часть 1. [2 балла] Эксплоративный анализ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OC8NgjS7zRgW",
        "outputId": "50dd4d95-a527-4a68-8ebb-1c044f087351"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'litbank'...\n",
            "remote: Enumerating objects: 1187, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 1187 (delta 26), reused 22 (delta 22), pack-reused 1149\u001b[K\n",
            "Receiving objects: 100% (1187/1187), 40.65 MiB | 17.43 MiB/s, done.\n",
            "Resolving deltas: 100% (152/152), done.\n",
            "Updating files: 100% (1423/1423), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/dbamman/litbank.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhOOyUXc0WcI"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gReQhzEz5_X"
      },
      "outputs": [],
      "source": [
        "root = '/content/litbank/entities/brat'\n",
        "files = []\n",
        "\n",
        "for dirpath, dirnames, filenames in os.walk(root):\n",
        "  for filename in filenames:\n",
        "    if filename.endswith('ann'):\n",
        "      files.append(filename)\n",
        "\n",
        "files = sorted(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZkySQnW1Lc5"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from collections import defaultdict, Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kgw_rXAUZBOl"
      },
      "source": [
        "1. Найдите топ 10 (по частоте) именованных сущностей каждого из 6 типов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "sn3SegJA1Dyi",
        "outputId": "04163ca0-2e8d-4375-e860-9039ff781d85"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:01<00:00, 90.89it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0b55964c-8f19-4985-a123-b6312c203393\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>T1</td>\n",
              "      <td>PER</td>\n",
              "      <td>Anthony Patch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>T3</td>\n",
              "      <td>PER</td>\n",
              "      <td>intelligent men</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>T7</td>\n",
              "      <td>PER</td>\n",
              "      <td>a man</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>T8</td>\n",
              "      <td>PER</td>\n",
              "      <td>the crusaders</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>T9</td>\n",
              "      <td>PER</td>\n",
              "      <td>Anthony</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>T162</td>\n",
              "      <td>LOC</td>\n",
              "      <td>the world</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>T168</td>\n",
              "      <td>PER</td>\n",
              "      <td>the world</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>T169</td>\n",
              "      <td>PER</td>\n",
              "      <td>any one who interrupted him at play</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>T170</td>\n",
              "      <td>PER</td>\n",
              "      <td>every one who came into his bedroom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>T171</td>\n",
              "      <td>GPE</td>\n",
              "      <td>there</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>171 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0b55964c-8f19-4985-a123-b6312c203393')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0b55964c-8f19-4985-a123-b6312c203393 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0b55964c-8f19-4985-a123-b6312c203393');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f5b932cd-d471-48d8-b41b-d6e7354837a3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f5b932cd-d471-48d8-b41b-d6e7354837a3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f5b932cd-d471-48d8-b41b-d6e7354837a3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "        0    1                                    2\n",
              "0      T1  PER                        Anthony Patch\n",
              "1      T3  PER                      intelligent men\n",
              "2      T7  PER                                a man\n",
              "3      T8  PER                        the crusaders\n",
              "4      T9  PER                              Anthony\n",
              "..    ...  ...                                  ...\n",
              "166  T162  LOC                            the world\n",
              "167  T168  PER                            the world\n",
              "168  T169  PER  any one who interrupted him at play\n",
              "169  T170  PER  every one who came into his bedroom\n",
              "170  T171  GPE                                there\n",
              "\n",
              "[171 rows x 3 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ent = defaultdict(Counter)\n",
        "for file in tqdm(files):\n",
        "    data = pd.read_csv(os.path.join(root, file), sep='\\t', header = None, quoting=3)\n",
        "    data[1] = data[1].apply(lambda x: x.split(' ')[0])\n",
        "    for i, row in data.iterrows():\n",
        "        ent[row[1]][row[2]] += 1\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaNuLsgQ2Xrb",
        "outputId": "4384b823-aef6-48c8-8fea-a30b12964510"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAC\n",
            "('home', 65)\n",
            "('the house', 52)\n",
            "('here', 39)\n",
            "('there', 39)\n",
            "('the room', 34)\n",
            "('the garden', 23)\n",
            "('the street', 14)\n",
            "('the hall', 13)\n",
            "('the road', 13)\n",
            "('the place', 12)\n",
            "\n",
            "\n",
            "LOC\n",
            "('the world', 72)\n",
            "('the sea', 27)\n",
            "('the river', 22)\n",
            "('the country', 20)\n",
            "('there', 18)\n",
            "('the earth', 16)\n",
            "('sea', 16)\n",
            "('the valley', 13)\n",
            "('this world', 12)\n",
            "('the woods', 9)\n",
            "\n",
            "\n",
            "GPE\n",
            "('London', 40)\n",
            "('England', 32)\n",
            "('there', 21)\n",
            "('the town', 21)\n",
            "('New York', 16)\n",
            "('town', 14)\n",
            "('France', 14)\n",
            "('Europe', 12)\n",
            "('the country', 10)\n",
            "('Rome', 10)\n",
            "\n",
            "\n",
            "VEH\n",
            "('the ship', 11)\n",
            "('the car', 9)\n",
            "('the train', 6)\n",
            "('the boat', 4)\n",
            "('boats', 4)\n",
            "('the carriage', 3)\n",
            "('ships', 3)\n",
            "('a carriage', 3)\n",
            "('the waggon', 3)\n",
            "('the coach', 3)\n",
            "\n",
            "\n",
            "PER\n",
            "('Mr.', 148)\n",
            "('Miss', 133)\n",
            "('Mrs.', 132)\n",
            "('sir', 50)\n",
            "('Sir', 45)\n",
            "('men', 40)\n",
            "('my mother', 40)\n",
            "('Cameron', 38)\n",
            "('his wife', 37)\n",
            "('Mr', 37)\n",
            "\n",
            "\n",
            "ORG\n",
            "('the army', 7)\n",
            "('the Church', 4)\n",
            "('the Committee of Public Safety', 4)\n",
            "('the Colonial Office', 4)\n",
            "('college', 3)\n",
            "('Harvard', 3)\n",
            "('Carston , Waite and Co.', 2)\n",
            "('the hospital', 2)\n",
            "('the C.C.H.', 2)\n",
            "(\"the Bank of Leichardt 's Land\", 2)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for _ in ent.keys():\n",
        "    print(_)\n",
        "    print(*ent[_].most_common(10), sep='\\n')\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6XRQNvqZGM9"
      },
      "source": [
        "2. Найдите топ 10 (по частоте) частотных триггеров событий."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmDyPvAk-QM8"
      },
      "outputs": [],
      "source": [
        "root = '/content/litbank/events/brat'\n",
        "files = []\n",
        "\n",
        "for dirpath, dirnames, filenames in os.walk(root):\n",
        "  for filename in filenames:\n",
        "    if filename.endswith('ann'):\n",
        "      files.append(filename)\n",
        "files = sorted(files)\n",
        "events = Counter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI4eJhMs-48I",
        "outputId": "e2e91197-b30a-4d38-9e04-cfc9765284d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 149.43it/s]\n"
          ]
        }
      ],
      "source": [
        "for file in tqdm(files):\n",
        "    try:\n",
        "        data = pd.read_csv(os.path.join(root, file), sep='\\t', header = None, quoting=3)\n",
        "    except pd.errors.EmptyDataError:\n",
        "        continue\n",
        "    data[1] = data[1].apply(lambda x: x.split(' ')[0])\n",
        "    for i, row in data.iterrows():\n",
        "        events[row[2]] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFm7oWF2-92r",
        "outputId": "4f9008dd-80e6-47d7-fe4c-14f342f257f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('said', 464)\n",
            "('came', 95)\n",
            "('looked', 92)\n",
            "('went', 92)\n",
            "('asked', 69)\n",
            "('heard', 63)\n",
            "('saw', 59)\n",
            "('cried', 59)\n",
            "('took', 56)\n",
            "('turned', 55)\n"
          ]
        }
      ],
      "source": [
        "print(*events.most_common(10), sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XERZZyHFZJCW"
      },
      "source": [
        "3. Кластеризуйте все уникальные триггеры событий, используя эмбеддинги слов и любой алгоритм кластеризации (например, агломеративный иерархический алгоритм кластеризации) и попробуйте проинтерпретировать кластеры: есть ли очевидные типы событий?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7nUkEav_AjO"
      },
      "outputs": [],
      "source": [
        "root = '/content/litbank/events/tsv'\n",
        "files = sorted(list(os.walk(root))[0][-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oa9H110udv3z"
      },
      "outputs": [],
      "source": [
        "from string import punctuation\n",
        "import csv\n",
        "import random\n",
        "\n",
        "def train_dev_test_split(data, train_percent = 80, dev_percent = 10, test_percent = 10):\n",
        "  random.shuffle(data)\n",
        "  train_size = int(len(data) * train_percent / 100)\n",
        "  train_data = data[:train_size]\n",
        "  dev_size = int(len(data) * dev_percent / 100)\n",
        "  dev_data = data[train_size:train_size+dev_size]\n",
        "  test_size = int(len(data) * test_percent / 100)\n",
        "  test_data = data[train_size+dev_size:]\n",
        "  return train_data, dev_data, test_data\n",
        "\n",
        "def prepare_sents(root, files):\n",
        "    sents, labels = [], []\n",
        "    for f in tqdm(files):\n",
        "        path = os.path.join(root, f)\n",
        "        df = pd.read_csv(path, sep='\\t', quoting=csv.QUOTE_NONE, header=None)\n",
        "        words = list(df[0])\n",
        "        labels = list(df[1])\n",
        "\n",
        "        sentences = [[]]\n",
        "        labels_sent = [[]]\n",
        "\n",
        "        for i in range(len(words)):\n",
        "            if words[i] not in ['.', '!', '?', '...', '', ',']:\n",
        "                if words[i] in punctuation:\n",
        "                    continue\n",
        "                sentences[-1].append(words[i].lower())\n",
        "                labels_sent[-1].append(labels[i])\n",
        "            elif sentences[-1] != []:\n",
        "                sentences.append([])\n",
        "                labels_sent.append([])\n",
        "\n",
        "        if sentences[-1] == []:\n",
        "            sentences = sentences[:-1]\n",
        "            labels_sent = labels_sent[:-1]\n",
        "        sents += sentences\n",
        "        labels += labels_sent\n",
        "\n",
        "    return sents, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXGiRWdFE6Xe",
        "outputId": "1f104b1c-0604-4dbb-b715-97180717d4a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 102.98it/s]\n"
          ]
        }
      ],
      "source": [
        "sentences, labels_sentences = prepare_sents(root, files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMN0trW7mZlv"
      },
      "outputs": [],
      "source": [
        "train_data, dev_data, test_data = train_dev_test_split(files, 80, 10, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaZbEnINdRfo",
        "outputId": "84430991-b0e8-433b-d5c8-07b00614cf02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.23.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4199769 sha256=895710abf47d2db207918edc3c98351d2c24894077e68022c5fb38ae4a2791b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.11.1\n"
          ]
        }
      ],
      "source": [
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og86Fsxuq_ny"
      },
      "outputs": [],
      "source": [
        "with open('train_text.txt', 'w') as f:\n",
        "    for s in sentences:\n",
        "        f.write(' '.join(s))\n",
        "        f.write('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_5SzSoYrWmP"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "ft = fasttext.train_unsupervised('train_text.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUPLXslodjPX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "trigger_embeddings = []\n",
        "triggers = []\n",
        "\n",
        "for trigger in events.keys():\n",
        "    triggers.append(trigger)\n",
        "    trigger_embeddings.append(ft.get_word_vector(trigger.lower()))\n",
        "\n",
        "trigger_embeddings = np.array(trigger_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "uvTkEpJbj8XY",
        "outputId": "b8c76f30-0d17-4bb3-fa10-4920f061a8fa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-11 {color: black;background-color: white;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AgglomerativeClustering(n_clusters=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AgglomerativeClustering</label><div class=\"sk-toggleable__content\"><pre>AgglomerativeClustering(n_clusters=3)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "AgglomerativeClustering(n_clusters=3)"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "clustering = AgglomerativeClustering(n_clusters = 3)\n",
        "clustering.fit(trigger_embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB08yMNsj94n",
        "outputId": "bff6eeea-8dd0-46c5-c11d-e31de5c76938"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cluster 1: ['Smoke', 'pinching', 'addressed', 'contemplation', 'engaged', 'mentioned', 'come', 'keeps', 'observed', 'correcting', 'acquired', 'infection', 'see', 'pretence', 'temper', 'born', 'married', 'died', 'issue', 'Married', 'lost', 'settled', 'death', 'introduction', 'excursions', 'seen', 'expected', 'retired', 'brought', 'resolved', 'precautions', 'provided', 'entertained', 'ball', 'held', 'illumined', 'appearances', 'projected', 'revel', 'glare', 'glitter', 'fancies', 'writhed', 'causing', 'voice', 'echoes', 'chime', 'die', 'laughter', 'depart', 'swells', 'appals', 'peal', 'indulged', 'reaches', 'beat', 'ceased', 'music', 'quieted', 'cessation', 'rumour', 'presence', 'murmur', 'disapprobation', 'surprise', 'terror', 'horror', 'disgust', 'made', 'disconcert', 'meditation', 'told', 'pause', 'performance', 'harken', 'constrained', 'aware', 'nod', 'inclined', 'came', 'met', 'said', 'reply', 'hesitation', 'discovery', 'give', 'concluded', 'explained', 'asked', 'doubts', 'gone', 'appeared', 'pursued', 'seeing', 'order', 'call', 'telling', 'announcement', 'contemplated', 'continued', 'been', 'put', 'produced', 'admit', 'record', 'add', 'conferred', 'received', 'hint', 'insisted', 'assured', 'gesticulating', 'thrust', 'appeal', 'saw', 'resolving', 'presented', 'declare', 'act', 'advent', 'deemed', 'favor', 'sent', 'awakened', 'repeated', 'wandered', 'play', 'pretended', 'angry', 'heard', 'say', 'answered', 'cried', 'party', 'cholera', 'ill', 'dead', 'run', 'hid', 'forgotten', 'slept', 'crept', 'went', 'shut', 'frightened', 'sleepy', 'carried', 'wondered', 'noise', 'hungry', 'thought', 'asleep', 'exclaimed', 'saying', 'neglected', 'turn', 'noticed', 'muttered', 'proud', 'took', 'list', 'answer', 'command', 'assenting', 'declared', 'persuade', 'assure', 'interrupted', 'catching', 'conversation', 'murmured', 'collected', 'Promoted', 'promoted', 'meeting', 'affecting', 'visiting', 'resumed', 'success', 'inquired', 'confessed', 'intervened', 'combed', 'chosen', 'happened', 'reading', 'considering', 'hear', 'occurred', 'TOOK', 'pop', 'wonder', 'tried', 'disappointment', 'fell', 'fall', 'got', 'began', 'spoke', 'hope', 'wish', 'dream', 'trying', 'wondering', 'think', 'taste', 'finished', 'nervous', 'Coming', 'gossiping', 'returned', 'introduced', 'drove', 'measure', 'inspection', 'caught', 'regarded', 'hot', 'bellowed', 'civility', 'stare', 'pronounced', 'bent', 'revealing', 'discovered', 'shook', 'disturbed', 'reminds', 'remarked', 'agreed', 'joke', 'take', 'go', 'remember', 'says', 'set', 'learn', 'taken', 'promised', 'dinner', 'observing', 'pipe', 'supposed', 'talk', 'spoken', 'described', 'speaking', 'played', 'amusement', 'increased', 'mist', 'joined', 'remarking', 'exchanged', 'replied', 'compressing', 'danger', 'remark', 'speculation', 'written', 'read', 'decided', 'attended', 'dealing', 'interposed', 'consolation', 'increase', 'possessed', 'care', 'calling', 'wished', 'trembled', 'idea', 'gesture', 'tottered', 'impudence', 'answering', 'did', 'poise', 'cry', 'hit', 'feelings', 'sharp', 'erection', 'vanished', 'accepted', 'promise', 'resentment', 'regard', 'disappeared', 'gave', 'crashing', 'speech', 'silent', 'thoughtful', 'sensation', 'sickness', 'off', 'buttered', 'smile', 'donned', 'teased', 'writing', 'temptation', 'shower', 'enjoy', 'answers', 'completing', 'recorded', 'travelled', 'astonished', 'rush', 'beginning', 'coming', 'consented', 'burst', 'let', 'invitation', 'flatter', 'visit', 'Observing', 'coughing', 'tear', 'paid', 'astonishment', 'joy', 'sorry', 'afraid', 'pointed', 'shake', 'expressed', 'question', 'dip', 'roll', 'going', 'impression', 'smothered', 'cut', 'torn', 'arose', 'split', 'whispered', 'harkened', 'fluttered', 'power', 'fear', 'note', 'smoke', 'flew', 'motion', 'forgot', 'bewildered', 'fascinated', 'feeling', 'haste', 'sank', 'sleep', 'remembered', 'bethought', 'vision', 'determination', 'alert', 'laughing', 'asserted', 'noting', 'flitted', 'draught', 'still', 'mischief', 'pointing', 'admitted', 'supper', 'address', 'refuse', 'request', 'difference', 'concerned', 'event', 'believe', 'sally', 'separated', 'planted', 'orders', 'excesses', 'confusion', 'conquest', 'slaughter', 'break', 'dread', 'attendance', 'proclamation', 'words', 'Preserved', 'removed', 'commanded', 'possession', 'won', 'revert', 'sit', 'recalled', 'marriage', 'maintain', 'exertions', 'ideas', 'animated', 'grief', 'loss', 'repeating', 'exclamation', 'playing', 'directions', 'croquet', 'lit', 'approach', 'reminded', 'given', 'understanding', 'laugh', 'adventure', 'realized', 'proposed', 'prepared', 'descended', 'felt', 'understood', 'wanted', 'gathered', 'reflected', 'fan', 'performing', 'amused', 'utterances', 'assumed', 'recommending', 'imitation', 'tears', 'accident', 'circumstance', 'returning', 'journey', 'desire', 'work', 'shown', 'fancy', 'observe', 'fail', 'tell', 'caused', 'remove', 'deduction', 'wrote', 'endeavouring', 'comes', 'out', 'plumes', 'connected', 'thunders', 'preferred', 'leave', 'preparing', 'attracted', 'recollection', 'emotion', 'proceed', 'determined', 'designs', 'proceeded', 'illuminated', 'composure', 'intention', 'compliments', 'begged', 'marrying', 'bid', 'offered', 'cast', 'discoloured', 'addressing', 'thinking', 'scent', 'catch', 'effect', 'insistence', 'roar', 'disappearance', 'insist', 'expression', 'speak', 'discharged', 'approaching', 'shudder', 'protest', 'accents', 'protested', 'belied', 'sigh', 'supported', 'assisted', 'desirable', 'proffered', 'bit', 'cluttered', 'listen', 'place', 'chord', 'shattering', 'resented', 'anticipated', 'shot', 'victory', 'recoiling', 'cleared', 'tapping', 'working', 'Reaching', 'begun', 'pretend', 'learned', 'knocking', 'learning', 'breakfast', 'served', 'selected', 'perceived', 'compliment', 'jump', 'acquaintance', 'observation', 'bought', 'responded', 'write', 'claim', 'attention', 'gratification', 'inherited', 'sale', 'lamentations', 'sold', 'appearance', 'hold', 'visitation', 'shaken', 'consequence', 'scattered', 'present', 'reproach', 'inference', 'quitted', 'had', 'wonders', 'committed', 'departed', 'professed', 'apparition', 'imagined', 'arrive', 'chinked', 'doubled', 'twist', 'fury', 'anger', 'quick', 'calculated', 'attempted', 'expecting', 'twisted', 'deposition', 'fever', 'mistake', 'anticipating', 'trip', 'excitement', 'erected', 'wakes', 'whine', 'rushing', 'quivered', 'cracks', 'spent', 'change', 'rest', 'satisfied', 'shone', 'sombre', 'appear', 'regret', 'thoughts', 'kiss', 'touch', 'conjectures', 'termed', 'conscious', 'intuition', 'agreeable', 'affected', 'realised', 'interest', 'offer', 'decline', 'ask', 'flutter', 'reassured', 'fought', 'objections', 'woke', 'gong', 'sought', 'persuasive', 'consider', 'intend', 'sunny', 'accept', 'return', 'receiving', 'projecting', 'tease', 'severity', 'predicament', 'ascent', 'tearful', 'do', 'meant', 'guise', 'leant', 'repressing', 'sympathized', 'repressed', 'persisted', 'displeasure', 'toy', 'decision', 'reversed', 'revealed', 'lateness', 'expect', 'glad', 'appointed', 'enjoying', 'advice', 'trotted', 'chase', 'cantered', 'away', 'dashing', 'getting', 'raise', 'swear', 'deaths', 'send', 'clenching', 'drive', 'reported', 'INCIDENT', 'untied', 'lustre', 'reflection', 'conjecture', 'actions', 'reverie', 'indifference', 'trail', 'weep', 'cower', 'wishing', 'busy', 'warranted', 'dissatisfaction', 'exchange', 'recount', 'puff', 'spat', 'desired', 'confess', 'persuaded', 'annoyed', 'toil', 'miss', 'excel', 'pushing', 'pleasure', 'apply', 'laid', 'conclusions', 'suggest', 'supposing', 'conclusion', 'presentation', 'vanishes', 'inferences', 'beg', 'occupation', 'becomes', 'practising', 'exercises', 'goes', 'amuse', 'keep', 'hush', 'expedition', 'liked', 'restoration', 'prevented', 'excuse', 'interested', 'suggested', 'breakfasted', 'word', 'lunch', 'settlement', 'occupied', 'sentence', 'warrant', 'shame', 'action', 'bore', 'disappearing', 'calls', 'swept', 'deadened', 'wave', 'became', 'attested', 'sputtering', 'flame', 'glittered', 'comin', 'Silence', 'query', 'Sent', 'glimpse', 'visited', 'search', 'find', 'drink', 'refused', 'questions', 'happy', 'guessed', 'ready', 'anxious', 'opportunity', 'permission', 'enjoyed', 'funeral', 'luncheon', 'agreement', 'affectionate', 'expressions', 'know', 'opposition', 'outbreak', 'invisible', 'jet', 'compared', 'activity', 'projection', 'gleam', 'established', 'suffering', 'inserted', 'tore', 'inspected', 'phrased', 'absent', 'sadness', 'situation', 'resolutions', 'outbreaks', 'apparent', 'forgetting', 'sorrow', 'inspiration', 'reversal', 'hopes', 'arrival', 'reposing', 'troublesome', 'lessening', 'heaviness', 'indignation', 'suffered', 'wept', 'mingle', 'lessened', 'eat', 'avenged', 'mention', 'kept', 'runs', 'fluttering', 'flashing', 'dodged', 'shoulder', 'smell', 'taught', 'cold', 'confused', 'worked', 'objected', 'contented', 'staid', 'whisper', 'presentiment', 'engagement', 'exertion', 'pity', 'quietly', 'familiar', 'whistle', 'Ceasing', 'shave', 'pull', 'giving', 'Laughter', 'asks', 'Laughing', 'prompted', 'feel', 'consorted', 'touching', 'Recognition', 'qualification', 'protection', 'appointment', 'presenting', 'preparation', 'sense', 'response', 'comparison', 'recognition', 'case', 'deserted', 'madness', 'showered', 'pool', 'coughed', 'contracted', 'cough', 'producing', 'breathing', 'dusted', 'Surprised', 'dumb', 'tea', 'distinguish', 'traversed', 'recognized', 'sentenced', 'task', 'remarks', 'converged', 'seeking', 'punishment', 'reflections', 'clink', 'reach', 'move', 'halt', 'preparations', 'movements', 'grateful', 'solitude', 'traveled', 'march', 'travel', 'reserve', 'respond', 'realization', 'quiver', 'force', 'seep', 'complied', 'agree', 'complaining', 'advised', 'softened', 'lecture', 'snow', 'counsel', 'persuasions', 'discourse', 'resolution', 'concern', 'rise', 'sick', 'reflect', 'surprised', 'advances', 'recover', 'messing', 'done', 'extended', 'tangent', 'eaten', 'drunk', 'explanation', 'animation', 'ARRIVAL', 'fortune', 'INTRODUCED', 'reposed', 'Getting', 'hitching', 'confusing', 'guess', 'pictured', 'meditating', 'bellied', 'love', 'indignant', 'rid', 'bell', 'averred', 'fit', 'tells', 'discontinued', 'perplexity', 'uneasiness', 'remorse', 'complying', 'recommended', 'instigation', 'endeavoured', 'unkindness', 'Disappointed', 'burnt', 'anguish', 'illness', 'recovery', 'remind', 'elopement', 'seething', 'escape', 'departure', 'positive', 'report', 'consternation', 'narration', 'convinced', 'condescended', 'adhered', 'premised', 'Going', 'dreary', 'BORN', 'ushered', 'advertise', 'proof', 'perspective', 'shuddered', 'applied', 'tremble', 'shiver', 'silence', 'milk', 'disappointed', 'destroyed', 'plan', 'fill', 'discouraged', 'arrest', 'Said', 'scare', 'flattered', 'forgive', 'advertised', 'pay', 'separation', 'desponding', 'welcomed', 'pressing', 'begin', 'express', 'uttered', 'preceded', 'relieving', 'conjectured', 'admire', 'distinguished', 'chatter', 'clatter', 'bestow', 'attempting', 'caress', 'imagining', 'interpose', 'demand', 'used', 'treatment', 'restoring', 'doubt', 'ease', 'mad', 'tiresome', 'lonesome', 'shivers', 'licks', 'Says', 'itching', 'itch', 'reckoned', 'comfortable', 'operations', 'hard', 'execution', 'crime', 'commissioned', 'washing', 'crushing', 'Words', 'assisting', 'pleased', 'exaltation', 'reaction', 'incident', 'operation', 'KISS', 'gives', 'sail', 'voyage', 'inform', 'computation', 'hurt', 'falls', 'bind', 'shout', 'design', 'knew', 'oration', 'support', 'promises', 'supplied', 'sign', 'signs', 'tempted', 'remembrance', 'interpreted', 'imaginations', 'considered', 'posture', 'show', 'blisters', 'repetitions', 'torrent', 'violence', 'smart', 'marks', 'Gets', 'commencement', 'agitation', 'own', 'complaints', 'acquainted', 'secure', 'confessing', 'bestowed', 'solicited', 'enterprise', 'notion', 'choose', 'inclining', 'glimmer', 'munching', 'smells', 'pondered', 'reined', 'experiment', 'Noticing', 'conducted', 'sentiments', 'accosted', 'doubted', 'saved', 'disaster', 'spread', 'certainty', 'objects', 'shelved', 'premise', 'duel', 'chill', 'curiosity', 'account', 'pretending', 'inhabited', 'carving', 'explosion', 'letting', 'confession', 'wrought', 'dallied']\n",
            "Cluster 0: ['drizzle', 'jostling', 'slipping', 'deposits', 'accumulating', 'rolls', 'wheezing', 'looming', 'tripping', 'yawning', 'tickled', 'still-born', 'improved', 'confirmed', 'encouraged', 'purchased', 'summoned', 'provisioned', 'masquerade', 'fête', 'dreams', 'piquancy', 'phantasm', 'unsuited', 'floats', 'writhe', 'gaieties', 'evolutions', 'buzz', 'assume', 'vows', 'tremulousness', 'swung', 'stiff-frozen', 'pestilence', 'smacking', 'Concluding', 'retailing', 'depositing', 'quickened', 'estimate', 'investigations', 'abrogation', 'fervid', 'submission', 'crosser', 'stammered', 'slunk', 'wrung', 'panic', 'wails', 'rustling', 'gliding', 'footsteps', 'awaiting', 'unpopular', 'anxiety', 'ordered', 'chuckled', 'nodded', 'ensconced', 'helped', 'Reproved', 'resume', 'demobbed', 'Armistice', 'knelt', 'Adverb', 'disapproval', 'examined', 'irritating', 'tickling', 'sneered', 'sprinkled', 'slurred', 'curl', 'undid', 'ignoring', 'swore', 'jingling', 'discussed', 'eyeing', 'gamble', 'tribute', 'fleered', 'jerked', 'offense', 'plodding', 'whistling', 'leapt', 'awkwardness', 'thickened', 'suspended', 'reddened', 'depicted', 'displaying', 'control', 'demanded', 'rejecting', 'detected', 'controlling', 'dispensed', 'surveying', 'regaining', 'climax', 'newly-risen', 'retreated', 'defeat', 'emerged', 'shrunken', 'verbiage', 'juggled', 'wrapping', 'disputed', 'gulps', 'outlined', 'glint', 'effaced', 'varnished', 'abuse', 'scolding', 'adjusting', 'raptures', 'trimming', 'emptied', 'faltered', 'clasping', 'hugging', 'soaked', 'sinister', 'wheeled', 'melted', 'murmuring', 'hesitated', 'impelled', 'shrinking', 'twirled', 'flickered', 'sang', 'dazzled', 'whirl', 'awoke', 'stumbling', 'exhausted', 'sifting', 'abstracted', 'quizzical', 'scoffed', 'scornful', 'daydream', 'fumbling', 'Untying', 'swaying', 'dripping', 'puzzled', 'misinterpreted', 'assault', 'wrangle', 'plunder', 'frenzy', 'riot', 'disgraced', 'jests', 'catchwords', 'plundering', 'transported', 'worshipped', 'predicted', 'polluted', 'sacrilege', 'entrusted', 'wedding', 'inquiries', 'adverb', 'withdrew', 'bustling', 'crinkled', 'drag', 'plunge', 'surveyed', 'sparkled', 'yawned', 'instructed', 'shrugged', 'nodding', 'puffs', 'undressed', 'submitted', 'catastrophe', 'settling', 'risen', 'problem', 'indicated', 'deduce', 'rubbed', 'scraped', 'outburst', 'signalised', 'sauntered', 'gratified', 'cavalcade', 'recollected', 'beholding', 'witnessed', 'dismounted', 'overtook', 'ingratiate', 'enquired', 'rivetted', 'odour', 'perfume', 'elevated', 'whorls', 'abducted', 'feigning', 'aloof', 'lurked', 'clutching', 'seizure', 'filtering', 'thrill', 'offended', 'irritability', 'celebrated', 'sniffing', 'helplessness', 'researches', 'draft', 'infuriated', 'approval', 'whirring', 'rippling', 'partial', 'swayed', 'alluding', 'waxing', 'pronouncing', 'rejoined', 'thanked', 'astride', 'betook', 'escorted', 'bidding', 'breathless', 'grimace', 'groan', 'adjured', 'fix', 'handshook', 'communicative', 'kick', 'episode', 'organizing', 'affirmative', 'lolling', 'ebbed', 'jolted', 'shriek', 'grumbled', 'nursing', 'growl', 'assailed', 'trucked', 'deposited', 'trekked', 'transhipped', 'suspected', 'ascertained', 'referring', 'cleansed', 'shrank', 'contact', 'wizzening', 'trudged', 'pistol-shots', 'screeching', 'screeches', 'haze', 'replaced', 'fidgetting', 'prevailed', 'sprung', 'amazement', 'acknowledge', 'dash', 'witticism', 'whimpering', 'tucked', 'deploring', 'escapade', 'longing', 'explored', 'hypnotized', 'aghas', 'snowy', 'sparkling', 'grumbling', 'tiptoed', 'deliberate', 'wringing', 'wipe', 'mewed', 'unwound', 'stabbed', 'saddened', 'generosity', 'peevish', 'wrangled', 'thumped', 'censured', 'mumbled', 'apologizing', 'exclaiming', 'beckoned', 'informed', 'whinnied', 'galloping', 'imprisonment', 'spoilt', 'puzzle', 'bearing', 'beheld', 'standstill', 'hopping', 'survey', 'espial', 'dispute', 'retreating', 'exhorted', 'prayer', 'sprawled', 'lengthening', 'spurring', 'flogged', 'emerge', 'unbarring', 'refilled', 'ridden', 'encountered', 'tilt', 'testify', 'errand', 'ladling', 'crammed', 'beckon', 'emerges', 'sneezing', 'clicketing', 'reappears', 'inflict', 'knocked', 'disengaged', 'adapted', 'dropping', 'squatted', 'investigate', 'scrambled', 'assembled', 'borne', 'scandal', 'repelled', 'judging', 'yelp', 'afterglow', 'moan', 'moving', 'thuds', 'Rustlings', 'footstep', 'crash', 'efforts', 'crackling', 'shuffled', 'queried', 'heerd', 'raindrops', 'ejaculated', 'hoof-beats', 'howling', 'implored', 'cuddled', 'sob', 'sobbed', 'pulse', 'deprived', 'lurking', 'display', 'fragrance', 'confronted', 'palpitating', 'eruption', 'vigil', 'vibrating', 'flash', 'swimming', 'flaming', 'dapple', 'intoxicating', 'heat-haze', 'sun-sparkles', 'transplanted', 'plucked', 'substitution', 'typing', 'snap', 'whimperings', 're-addressing', 'recognised', 'tackled', 're-entered', 'revolved', 'enlightened', 'solaced', 'rehearsed', 'ceremony', 'crisis', 'tea-service', 'overthrow', 'dismissed', 'grievance', 'embarrassment', 'figurative usage', 'recollect', 'exploit', 'battle', 'swarming', 'thud', 'scrimmage', 'rubbing', 'stamping', 'nap', 'overheard', 'coaxed', 'bark', 'exhibited', 'newly-seen', 'sobs', 'gurgling', 'moaning', 'stirring', 'kills', 'begging', 'postpone', 'disembarked', 'invoked', 'evasion', 'delay', 'enquiry', 'rejoinder', 'shirked', 'forsaken', 'impulse', 'compromised', 'fragrant', 'chirping', 'down-spirited', 'chirp', 'scrape', 'engaging', 'tattoo', 'echoing', 'snapping', 'controlled', 'soothed', 'cosseted', 'breaths', 'brisker', 'knitting', 'jaunting', 'headache', 'jolt', 'registered', 'pardoned', 'misdemeanor', 'Retracing', 'varnishing', 'effusion', 'gravity', 'quoted', 'retraced', 'flickering', 'campfire', 'sparks', 'echoed', 'pads', 'vigi', 'digging', 'slaps', 'dug', 'amazed', 'pitying', 'moist', 'sniff', 'examining', 'crackled', 'lecturing', 'expostulated', 'prompting', 'terrified', 'abandoning', 'entreaties', 'snowfall', 'melting', 'whisked', 'mixing', 'stabs', 'delaying', 'rigid', 'unfolded', \"Talkin '\", 'smouldered', 'acknowledgment', 'snatching', 'upsettled', 'embarrassed', 'flourishing', 'directed', 'dawdled', 'scanned', 'suffused', 'wooing', 'leaping', 'wistfulness', 'fleeing', 'giggling', 'yawn', 'fidgeting', 'apologize', 'mole-catching', 'uncurled', 'inscription', 'tipsify', 'imputes', 'detained', 'apprehend', 'bequeathed', 'abandon', 'applying', 'aims', 'detaining', 'infatuated', 'conjuring', 'enraged', 'foresee', 'carnage', 'aghast', 'howl', 'blunder', 'suspense', 'await', 'trot', 'pursuing', 'dismay', 'riveted', 'whooping', 'apprehension', 'grasping', 'provision', 'chuses', 'placing', 'gasping', 'sneezed', 'rub', 'imprinted', 'pausing', 'badged', 'ticketed', 'retiring', 'adopted', 'arguing', 'encouragement', 'debate', 'advocated', 'clamored', 'bids', 'disdaining', 'discussions', 'crawled', 'wreathed', 'rebellion', 'clangoring', 'rejoicing', 'enlisted', 'ecstasy', 'trails', 'punching', 'resurrected', 'seize', 'suspicion', 'miscarried', 'withdrawn', 'raffle', 'affronted', 'foreboding', 'withdraw', 'sheltered', 'soliloquised', 'ejaculation', 'squealing', 'haunted', 'decamp', 'sneaking', 'gnarl', 'provoked', 'punch', 'guardianship', 'irritated', 'climbed', 'yelping', 'subsided', 'overwhelmed', 'tempest', 'a-bothering', 'down-hearted', 'crawling', 'shriveled', 'tiptoeing', 'snore', 'executed', 'prophecy', 'grasp', 'brackish', 'wrested', 'trodden', 'unearthed', 'scouring', 'oaths', 'regained', 'snarl', 'mutiny', 'breakdown', 'consumption', 'tipsy', 'essayed', 'rejoiced', 'dreamy', 'shipwrecked', 'swims', 'shift', 'trusted', 'overset', 'flurry', 'swam', 'wrench', 'volley', 'threatenings', 'demands', 'signify', 'learnt', 'slung', 'tokens', 'relaxing', 'conjecturing', 'daubed', 'consulted', 'dispelled', 'dedicated', 'abandoned', 'undertaking', 'enthusiasm', 'puffing', 'detect', 'dragging', 'chirps', 'drags', 'float', 'pacified', 'kindled', 'trod', 'Placing', 'vapor', 'unnerved', 'uplifted', 'deepen', 'admonishing', 'diminished', 'discussion', 'smuggled', 'ascended', 'propitiated', 'plodded', 'rebuking', 'ascending', 'whack', 'ploughing', 'summons', 'shrieked', 'snorting', 'screaming', 'demolishing', 'discuss', 'misadventure', 'obtained', 'grunting', 'purchasing', 'graduated']\n",
            "Cluster 2: ['losing', 'sliding', 'sticking', 'Fog', 'fog', 'flows', 'creeping', 'hovering', 'drooping', 'peeping', 'Gas', 'lighted', 'gas', 'groping', 'running', 'plants', 'drones', 'blew', 'handled', 'lowering', 'adding', 'birth', 'inserting', 'forced', 'found', 'invited', 'tidings', 'uniting', 'entered', 'welded', 'fire', 'streamed', 'look', 'stalked', 'taking', 'strikes', 'endured', 'stream', 'waning', 'sounding', 'revelled', 'looked', 'smiled', 'chiming', 'painted', 'sound', 'rang', 'pale', 'passed', 'clang', 'strokes', 'walking', 'rode', 'hummed', 'halted', 'turned', 'hunting', 'tracing', 'walked', 'sat', 'frowned', 'declining', 'stretched', 'intimated', 'handed', 'passion', 'kicked', 'hurried', 'stuck', 'muttering', 'grinding', 'talking', 'stared', 'lifted', 'wailing', 'clutched', 'shivering', 'grew', 'gasped', 'ran', 'wailed', 'dying', 'sounds', 'ate', 'drank', 'drowsy', 'cries', 'lay', 'hurrying', 'waiting', 'watching', 'slipped', 'watched', 'talked', 'opened', 'frowning', 'startled', 'drawing', 'wakened', 'turning', 'stayed', 'struck', 'sinking', 'lined', 'clung', 'start', 'looking', 'flushed', 'lowered', 'closed', 'lain', 'greeted', 'blocked', 'blocking', 'pounced', 'sighed', 'hinted', 'led', 'rose', 'added', 'rousing', 'poured', 'bite', 'left', 'check', 'row', 'wounded', 'appealed', 'launched', 'tired', 'peeped', 'started', 'flashed', 'falling', 'fallen', 'jumped', 'sight', 'fitted', 'longed', 'finding', 'brightened', 'waited', 'dozing', 'pulled', 'sprang', 'followed', 'glances', 'sifted', 'drew', 'leaned', 'smoked', 'fanning', 'glance', 'lift', 'straightened', 'scratched', 'ripped', 'mounted', 'stated', 'Ambling', 'laughed', 'blazing', 'following', 'breaking', 'rapped', 'called', 'lingering', 'threw', 'crossed', 'flapped', 'stopped', 'glared', 'oath', 'hearing', 'pouring', 'flung', 'walk', 'reached', 'leaning', 'broke', 'cursed', 'glanced', 'stirred', 'hits', 'lying', 'built', 'checked', 'announced', 'silenced', 'wind', 'wandering', 'clouds', 'rain', 'gathering', 'drawn', 'studied', 'cloud', 'sweeping', 'blast', 'formed', 'paused', 'seating', 'thrusting', 'mused', 'fetched', 'stand', 'striking', 'cutting', 'bled', 'pain', 'glancing', 'pinning', 'blows', 'listened', 'coined', 'poised', 'tossed', 'raised', 'folded', 'drawled', 'washed', 'Looking', 'opening', 'entering', 'vowed', 'delight', 'breath', 'blossomed', 'flowered', 'delighted', 'disclosed', 'feeding', 'growled', 'seized', 'pleaded', 'staring', 'licking', 'tilted', 'keeping', 'hugged', 'limped', 'picking', 'lamed', 'stung', 'gained', 'sullen', 'dropped', 'listening', 'stepped', 'closing', 'hung', 'starting', 'fearing', 'danced', 'song', 'shouting', 'peering', 'dancing', 'twined', 'panting', 'peered', 'blurred', 'crowding', 'fled', 'voices', 'toiled', 'struggling', 'trembling', 'drifted', 'waving', 'flying', 'gazed', 'dreaming', 'challenged', 'bridled', 'soft', 'waking', 'glinting', 'clapped', 'bounded', 'touched', 'shock', 'rained', 'brightening', 'hopped', 'nestled', 'faded', 'crouched', 'warmth', 'cheer', 'fared', 'storming', 'treating', 'forded', 'fighting', 'loaded', 'yelling', 'crowded', 'rushed', 'throng', 'shouted', 'stripped', 'escaped', 'completed', 'breathed', 'bowed', 'broken', 'perished', 'placed', 'STORMING', 'Extracted', 'stored', 'chatted', 'flow', 'hoped', 'Seating', 'quitting', 'mass', 'gaze', 'advancing', 'seated', 'facing', 'holding', 'lifting', 'kissed', 'rolled', 'saving', 'strolled', 'entrance', 'piled', 'leaving', 'watered', 'pass', 'pacing', 'waved', 'changed', 'lighting', 'throwing', 'covered', 'ended', 'differing', 'wagged', 'sunset', 'blushed', 'blush', 'driving', 'Arriving', 'sending', 'open', 'parting', 'arrived', 'tapped', 'fixed', 'painting', 'shaking', 'wiped', 'drop', 'blotted', 'hushed', 'smoking', 'shouldering', 'circling', 'tossing', 'strolling', 'pressed', 'darkened', 'showed', 'rising', 'leading', 'declined', 'making', 'picked', 'moved', 'roamed', 'regarding', 'eating', 'grunted', 'putting', 'Smoking', 'mount', 'arranged', 'breeze', 'smelling', 'drinking', 'buried', 'poked', 'counted', 'attacked', 'straightening', 'converted', 'springing', 'kicking', 'scattering', 'standing', 'step', 'commenced', 'appearing', 'asking', 'faced', 'charmed', 'tightened', 'shutting', 'rage', 'grappled', 'struggled', 'glazed', 'flagged', 'choked', 'hiding', 'struggle', 'thrown', 'filing', 'rattled', 'stormed', 'raged', 'allowed', 'carted', 'mauling', 'headed', 'cheated', 'spending', 'overcharged', 'weighed', 'excited', 'burned', 'fair', 'stood', 'tooting', 'braying', 'wrong', 'whining', 'shouts', 'gazing', 'killed', 'lamented', 'smelled', 'drifting', 'flood', 'toying', 'stir', 'boarded', 'clacked', 'passing', 'pulling', 'marked', 'aroused', 'clew', 'ache', 'smiling', 'mingling', 'growing', 'planned', 'studying', 'dragged', 'racket', 'dressed', 'plunged', 'clashed', 'drowned', 'shivered', 'crying', 'mewing', 'clinging', 'chased', 'stooped', 'tied', 'bared', 'blowing', 'darted', 'colder', 'tearing', 'tying', 'laying', 'shocked', 'parted', 'forward', 'filling', 'bow', 'box', 'frost', 'pricked', 'snorted', 'barking', 'straight', 'fright', 'leaped', 'end', 'whipped', 'bleeding', 'down', 'groaning', 'riding', 'bang', 'hissing', 'glow', 'deed', 'smiles', 'steps', 'descending', 'stepping', 'handing', 'gaining', 'fastening', 'Locking', 'flocking', 'grant', 'rowing', 'pace', 'stroke', 'approached', 'discovering', 'sounded', 'carrying', 'paced', 'ring', 'mastered', 'earned', 'stretching', 'stop', 'watch', 'setting', 'pushed', 'wasted', 'stopping', 'insisting', 'ruined', 'blazed', 'judged', 'urged', 'marking', 'skirted', 'advanced', 'hunted', 'Look', 'foamed', 'boiled', 'searched', 'pumped', 'fastened', 'Stretching', 'winked', 'glided', 'blight', 'pierced', 'twittering', 'flooded', 'flaring', 'strode', 'fading', 'brewing', 'trotting', 'Peering', 'loom', 'clinking', 'jingled', 'curses', 'strain', 'heaves', 'click', 'moaned', 'pattered', 'cracked', 'showing', 'backed', 'Left', 'mixed', 'follow', 'dried', 'bade', 'raced', 'landed', 'pride', 'storm', 'streamer', 'blaze', 'jetting', 'whispering', 'heightened', 'altered', 'appended', 'tramped', 'planked', 'despatched', 'scented', 'sorted', 'sketched', 'sped', 'weeping', 'swallowed', 'draw', 'bewitched', 'charge', 'driven', 'sitting', 'chilled', 'brood', 'brooding', 'dining', 'mots', 'bidden', 'sleeping', 'intoned', 'Halted', 'blessed', 'crosses', 'whistles', 'skipped', 'propped', 'dipped', 'lathered', 'shaved', 'raving', 'Leaning', 'clearing', 'fretted', 'pinched', 'Drawing', 'linked', 'clacking', 'Parried', 'nearing', 'stolen', 'winced', 'wound', 'muffled', 'snapped', 'forged', 'rented', 'Hearing', 'threatened', 'shoveled', 'whistled', 'bending', 'swinging', 'sowing', 'spoiled', 'twitched', 'rung', 'serving', 'Leaving', 'paces', 'air', 'light', 'hovered', 'fanned', 'paling', 'crack', 'halting', 'packed', 'camped', 'bring', 'grasped', 'curious', 'astounded', 'filled', 'dark', 'pecking', 'stroking', 'reproached', 'blow', 'staggered', 'stamped', 'brisked', 'dripped', 'cooked', 'gaping', 'warmed', 'dumped', 'singing', 'dined', 'lingered', 'thrilled', 'mantled', 'flitting', 'blinking', 'flared', 'arched', 'Trailing', 'glowed', 'rammed', 'patted', 'stroked', 'cured', 'up', 'packing', 'extracted', 'blushing', 'denied', 'surging', 'allowing', 'slip', 'located', 'glimpses', 'topped', 'arming', 'strapped', 'darkness', 'hoping', 'shots', 'charging', 'shooting', 'routing', 'Riding', 'BIRTH', 'named', 'burying', 'beamed', 'struggles', 'imposed', 'rustled', 'warm', 'tasting', 'chafed', 'dress', 'opposed', 'bustled', 'comments', 'winds', 'milking', 'primed', 'peeled', 'beating', 'whirled', 'snatched', 'through', 'stealing', 'forestalled', 'vexed', 'overlooked', 'missed', 'stumbled', 'strike', 'glowing', 'frown', 'wincing', 'watering', 'checking', 'striding', 'dived', 'winking', 'faces', 'proceeding', 'issued', 'parrying', 'heaving', 'scared', 'flipped', 'stooping', 'scrouched', 'itched', 'breathe', 'build', 'blasted', 'elated', 'appalled', 'sailed', 'chartered', 'tripped', 'overturning', 'drenched', 'felled', 'white', 'firing', 'lodged', 'limping', 'wounding', 'glowering', 'bored', 'sampled', 'hunched', 'bound', 'curled', 'spied', 'rowed', 'tide', 'abated', 'awaked', 'roared', 'ventured', 'loosened', 'flight', 'striving', 'acted', 'warning', 'treated', 'disposed', 'mingled', 'passage', 'over', 'entreated', 'hired', 'stifles', 'steadying', 'groped', 'skinning', 'gloom', 'acting', 'thrilling', 'Shaking', 'heighten', 'awed', 'speculating', 'routed', 'clinched', 'declaring', 'cheering', 'collared', 'directing', 'bursting', 'choking', 'warned', 'stampeded', 'rolling', 'tales', 'charged', 'attack', 'christened', 'sweating']\n"
          ]
        }
      ],
      "source": [
        "clusters = {}\n",
        "for word, cluster in zip(triggers, clustering.labels_):\n",
        "    if cluster not in clusters:\n",
        "        clusters[cluster] = [word]\n",
        "    else:\n",
        "        clusters[cluster].append(word)\n",
        "\n",
        "for cluster, words in clusters.items():\n",
        "    print(f\"Cluster {cluster}: {words}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaKopGIbZMGo"
      },
      "source": [
        "[бонус] Визуализируйте полученные кластеры с помощью TSNE или UMAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MC7fJLyWkdMi"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBYQGGDcklSD"
      },
      "outputs": [],
      "source": [
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "embed_tsne = tsne.fit_transform(trigger_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtnUgSX6lW0Y"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "_0IYghiolCI9",
        "outputId": "3c3bf0c1-94b1-455d-dbc9-1e4673feff73"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"b5760feb-f65c-4ed1-8267-6651e8fa49a4\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b5760feb-f65c-4ed1-8267-6651e8fa49a4\")) {                    Plotly.newPlot(                        \"b5760feb-f65c-4ed1-8267-6651e8fa49a4\",                        [{\"hovertemplate\":\"x=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003etext=%{text}\\u003cbr\\u003ecolor=%{marker.color}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":[1,0,0,2,0,2,0,2,0,2,2,2,0,2,2,2,0,1,2,2,0,2,2,1,1,1,0,2,2,1,0,1,2,1,2,2,1,2,1,0,1,1,1,1,2,1,1,1,1,1,0,0,2,1,2,2,1,1,1,1,2,1,2,0,2,1,0,1,2,0,2,0,1,2,1,2,1,0,1,1,1,1,0,1,1,1,2,1,2,2,0,1,0,1,1,0,0,0,1,2,1,2,1,2,1,1,1,1,2,1,1,0,1,0,2,2,1,1,1,0,1,1,2,1,1,0,1,1,2,1,1,0,1,1,1,1,1,1,1,0,2,2,0,2,1,0,1,1,2,1,1,1,2,2,2,2,0,2,1,0,2,1,0,2,1,1,1,1,2,2,1,2,2,1,1,1,2,1,1,0,1,2,1,0,1,0,1,2,2,0,1,1,1,0,2,1,1,1,1,2,2,1,1,1,1,1,0,0,1,1,1,1,0,1,1,1,0,1,1,1,2,0,1,1,1,1,1,1,1,1,1,1,2,1,0,0,2,2,1,0,2,1,1,1,2,1,2,2,1,2,2,2,1,1,0,1,1,2,2,2,2,2,2,1,1,2,1,1,2,1,0,1,1,2,1,2,2,2,1,1,1,2,1,2,0,1,1,1,2,2,0,0,2,2,2,2,2,2,1,2,2,1,1,2,1,2,2,1,0,1,2,2,2,0,2,2,1,1,2,1,2,2,1,1,2,2,1,1,1,2,2,2,0,1,0,2,1,1,0,0,2,0,2,1,1,2,1,1,1,2,1,0,2,2,2,2,0,2,1,1,2,2,1,1,0,1,1,1,1,1,0,1,1,2,0,2,2,1,1,0,2,2,1,1,1,1,1,2,2,1,2,1,1,1,1,1,2,1,1,1,1,1,1,2,2,1,1,2,0,2,1,1,2,1,2,2,0,2,1,1,1,1,2,2,1,1,2,1,2,0,1,2,1,2,2,1,2,2,1,1,2,0,2,0,0,0,0,0,1,1,1,0,1,0,1,2,0,0,1,2,2,1,2,1,0,0,0,2,0,0,1,1,2,2,0,0,1,1,2,0,1,1,1,0,2,0,2,2,2,2,1,2,1,1,1,1,2,1,1,1,1,1,2,2,2,2,1,1,2,1,2,0,1,2,1,0,1,1,2,0,2,0,1,2,1,2,2,1,2,1,2,1,1,0,1,1,2,2,0,1,1,0,1,2,0,1,2,1,0,1,2,0,2,0,0,1,1,2,2,2,2,0,1,1,2,2,2,2,2,2,2,0,2,1,1,1,1,2,1,2,2,1,0,1,1,2,1,1,2,1,1,2,2,2,2,0,1,1,2,2,0,0,0,2,1,1,0,1,1,0,2,0,1,2,0,2,1,0,0,2,2,1,1,1,0,0,1,1,1,1,1,1,1,1,2,2,0,1,1,1,2,1,1,1,1,1,1,1,0,1,2,2,1,2,1,1,2,2,2,1,2,0,1,2,2,1,1,1,0,1,2,1,0,1,1,0,1,1,1,0,0,1,1,2,2,2,2,2,1,0,2,1,1,2,1,1,1,2,0,2,0,2,2,1,0,1,0,1,2,1,2,1,2,0,2,1,0,0,0,2,2,1,1,2,2,2,1,2,2,1,2,0,0,1,0,2,1,1,1,2,2,1,2,2,0,0,1,0,1,0,1,1,0,2,0,2,2,2,2,0,1,2,2,1,2,1,0,2,2,1,0,1,1,1,2,2,1,2,2,0,1,0,1,2,2,0,1,0,2,2,0,1,2,0,0,2,2,1,0,0,1,2,2,2,2,1,1,2,2,1,1,2,2,0,1,2,2,2,1,1,0,1,1,2,1,0,1,1,2,0,1,1,2,1,2,1,1,2,0,1,1,0,1,0,0,0,0,0,1,2,1,2,2,1,2,1,1,2,2,2,1,1,0,0,2,2,1,2,0,0,2,0,2,1,2,1,2,2,1,2,0,0,1,1,1,2,1,1,2,1,2,1,0,1,1,0,1,1,2,2,0,1,0,1,0,2,1,1,0,2,2,1,2,2,0,0,1,1,1,1,0,1,1,0,1,0,1,2,2,1,0,1,1,0,1,2,2,2,1,1,1,0,1,2,1,2,0,2,1,2,1,1,0,1,2,1,0,0,1,1,1,1,1,2,2,1,0,0,1,2,0,1,1,0,2,1,0,1,0,1,1,1,2,2,1,1,1,1,0,1,2,2,0,2,1,1,2,2,1,0,1,0,1,1,0,2,0,2,1,0,1,0,2,1,1,1,2,0,2,1,1,1,1,2,0,1,1,0,2,1,2,0,2,1,0,2,2,2,1,2,1,2,2,1,1,1,2,0,1,0,2,1,1,2,2,1,1,1,2,0,0,2,1,1,1,0,1,1,2,1,2,2,1,1,2,0,1,1,1,1,1,1,2,2,1,1,0,2,2,0,1,0,0,1,2,0,1,1,2,0,1,2,0,1,2,0,1,1,2,0,1,2,0,2,0,1,0,0,0,1,2,1,0,2,0,0,2,1,1,0,1,0,1,0,1,0,2,1,1,1,2,1,1,2,2,0,1,2,0,0,1,0,2,2,2,2,2,1,2,1,2,1,1,1,1,1,0,1,1,0,1,1,1,1,2,1,2,0,1,0,1,1,1,1,2,1,2,0,2,0,1,2,0,1,0,1,0,0,1,1,1,1,1,1,0,0,0,1,1,1,1,0,2,2,2,2,1,2,1,0,0,2,2,0,0,1,1,2,2,2,0,1,1,2,2,0,2,1,1,0,2,2,0,2,2,0,0,2,2,1,2,1,0,2,2,2,0,2,0,0,0,1,1,1,1,2,0,1,0,0,1,2,1,1,2,2,2,1,2,2,0,0,0,1,0,2,0,2,2,1,2,2,1,1,2,2,0,2,1,1,1,1,2,2,1,1,1,1,2,2,0,1,1,2,1,0,1,0,1,0,2,2,2,2,1,1,2,2,1,2,0,1,1,0,2,0,1,1,1,2,0,2,0,0,0,0,0,1,1,1,1,2,2,0,1,2,1,0,0,1,2,1,0,1,0,0,0,1,1,1,0,2,2,2,0,2,2,2,2,0,1,1,0,0,2,2,2,1,1,2,1,2,2,1,2,0,0,2,1,1,0,0,0,1,1,1,2,1,1,1,0,1,1,0,0,1,1,2,1,0,1,0,1,2,0,1,2,1,1,0,1,2,0,2,1,1,2,2,1,0,2,1,2,2,2,2,1,2,2,2,2,2,1,1,2,2,0,1,1,1,2,1,1,0,0,1,1,0,0,0,0,1,0,2,1,1,2,2,1,1,2,2,0,0,1,2,2,2,0,1,1,0,0,1,1,0,0,0,0,0,0,2,1,2,0,0,2,0,0,1,1,0,2,2,0,2,1,1,1,2,0,1,1,0,1,1,2,1,1,2,1,0,2,1,1,1,1,1,1,2,1,1,1,1,1,1,0,1,2,2,1,1,1,2,2,2,1,0,2,0,1,1,0,0,1,2,1,2,0,2,1,1,2,2,1,2,2,1,1,0,0,1,0,1,2,2,2,1,1,1,2,2,2,2,0,0,2,0,2,2,2,1,2,0,1,0,1,2,1,0,1,1,2,0,1,1,2,0,2,1,2,2,0,2,1,0,2,2,1,0,2,2,2,2,2,0,1,2,2,2,2,0,2,2,0,0,1,1,0,1,2,1,0,0,1,0,1,0,0,1,2,0,2,1,2,1,2,2,1,0,0,2,1,1,1,1,0,1,1,2,2,1,0,2,1,0,0,0,1,0,1,0,1,1,2,1,1,1,1,2,0,2,1,0,1,1,1,0,2,0,2,1,1,0,1,1,1,0,0,0,1,0,1,2,0,1,2,0,2,1,0,0,2,2,0,0,0,0,0,1,0,1,2,2,0,0,2,2,0,0,1,1,0,2,2,2,1,1,1,0,0,0,1,1,1,1,2,0,0,1,2,1,1,1,1,1,2,1,1,1,0,0,1,1,0,0,1,0,1,1,1,1,0,1,2,2,0,0,0,0,2,0,1,0,2,0,1,1,1,2,0,1,0,0,1,1,2,1,1,1,2,1,1,0,2,2,2,1,1,2,1,0,1,1,1,0,1,0,1,0,2,1,0,2,0,1,2,2,2,2,0,1,2,2,2,2,2,1,1,2,2,0,1,0,2,2,2,1,0,1,1,2,1,2,2,2,2,0,0,1,1,0,2,2,0,1,0,2,1,0,1,0,2,0,1,0,2,0,1,1,1,1,0,1,1,1,1,1,1,1,2,0,2,0,1,0,0,0,0,0,2,1,0,2,0,0,1,1,2,0,2,2,1,1,0,0,1,1,0,2,2,0,1,2,2,0,2,0,0,1,1,0,1,0,1,1,2,1,0,2,0,2,2,0,0,1,0,2,0,0,1,0,1,1,2,1,2,0,0,1,2,2,0,0,1,0,1,1,1,0,1,1,1,1,1,0,2,2,2,0,1,2,1,1,1,1,0,2,2,0,0,2,1,2,1,1,1,2,0,2,0,1,0,1,1,1,2,1,2,1,0,0,1,0,1,1,1,1,0,1,1,0,1,0,0,2,2,0,2,2,2,1,0,2,1,0,0,0,2,0,2,0,1,1,1,0,1,0,1,0,1,1,2,1,0,0,0,1,1,1,0,1,1,2,1,0,0,1,2,2,2,0,0,2,2,0,0,2,1,1,0,1,2,1,1,0,2,0,2,1,0,2,1,2,2,1,0,2,2,0,1,0,1,0,1,2,0,2,2,2,0,2,0,0,1,0,0,0,0,0,0,1,1,1,1,0,1,0,0,1,1,1,0,1,1,1,2,1,1,1,1,0,1,2,1,0,2,2,1,0,0,0,2,1,2,2,2,0,2,2,0,0,0,1,2,1,2,1,0,0,1,2,2,0,1,2,0,2,0,2,1,0,2,1,1,2,0,1,1,2,0,1,1,0,2,0,1,2,1,2,2,0,2,1,0,1,2,0,1,2,0,0,0,1,0,0,0,0,2,0,0,0,2,0,2,0,0,0,0,2,0,0,1,0,2,1,1,1,2,1,2,1,1,2,0,0,1,0,0,0,1,2,2,2,2,1,1,0,1,2,2,2,2,1,0,2,2,1,0,0,1,1,0,1,1,0,2,1,2,1,0,0,1,2,1,1,0,1,1,0,1,1,1,1,0,0,1,0,1,0,2,1,0,0,2,0,2,2,0,1,2,2,0,1,2,2,2,1,0,0,1,0,2,1,1,1,0,0,0,1,1,1,1,1,0,2,0,2,0,1,1,0,2,2,1,1,2,1,2,0,1,2,0,0,0,1,1,0,1,0,0,1,2,0,1,2,2,2,2,0,1,2,2,2,0,0,2,0,1,2,0,2,2,1,1,2,2,2,0,2,0,2,1,0,0,1,1,1,0,2,0,2,1,2,1,1,0,0,1,1,1,2,0,2,1,0,0,0,0,2,2,2,2,1,1,2,0,1,2,1,2,2,0,1,1,1,1,2,0,1,0,0,0,1,1,0,1,2,1,1,1,1,1,2,1,1,1,0,1,0,0,1,1,0,1,2,2,0,2,2,1,1,1,0,1,0,1,2,2,1,1,1,1,0,1,1,0,1,0,2,0,0,0,0,0,1,0,1,2,1,0,2,0,1,0,0,2,1,1,0,2,0,1,2,1,2,1,0,2,1,1,2,1,1,1,2,0,2,2,1,1,0,0,1,1,1,1,0,1,1,0,1,0,1,2,2,0,0,2,0,0,2,0,2,0,1,1,0,2,1,0,1,0,2,0,0,0,0,1,2,1,2,2,0,1,2,1,2,2,2,2,0,0,0,1],\"coloraxis\":\"coloraxis\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"\",\"showlegend\":false,\"text\":[\"Smoke\",\"drizzle\",\"jostling\",\"losing\",\"slipping\",\"sliding\",\"deposits\",\"sticking\",\"accumulating\",\"Fog\",\"fog\",\"flows\",\"rolls\",\"creeping\",\"hovering\",\"drooping\",\"wheezing\",\"pinching\",\"peeping\",\"Gas\",\"looming\",\"lighted\",\"gas\",\"addressed\",\"contemplation\",\"engaged\",\"tripping\",\"groping\",\"running\",\"mentioned\",\"yawning\",\"come\",\"plants\",\"keeps\",\"drones\",\"blew\",\"observed\",\"handled\",\"correcting\",\"tickled\",\"acquired\",\"infection\",\"see\",\"pretence\",\"lowering\",\"temper\",\"born\",\"married\",\"died\",\"issue\",\"still-born\",\"improved\",\"adding\",\"Married\",\"birth\",\"inserting\",\"lost\",\"settled\",\"death\",\"introduction\",\"forced\",\"excursions\",\"found\",\"confirmed\",\"invited\",\"seen\",\"encouraged\",\"expected\",\"tidings\",\"purchased\",\"uniting\",\"summoned\",\"retired\",\"entered\",\"brought\",\"welded\",\"resolved\",\"provisioned\",\"precautions\",\"provided\",\"entertained\",\"ball\",\"masquerade\",\"held\",\"illumined\",\"appearances\",\"fire\",\"projected\",\"streamed\",\"look\",\"f\\u00eate\",\"revel\",\"dreams\",\"glare\",\"glitter\",\"piquancy\",\"phantasm\",\"unsuited\",\"fancies\",\"stalked\",\"writhed\",\"taking\",\"causing\",\"strikes\",\"voice\",\"echoes\",\"chime\",\"die\",\"endured\",\"laughter\",\"depart\",\"floats\",\"swells\",\"writhe\",\"stream\",\"waning\",\"appals\",\"peal\",\"indulged\",\"gaieties\",\"reaches\",\"beat\",\"sounding\",\"ceased\",\"music\",\"evolutions\",\"quieted\",\"cessation\",\"revelled\",\"rumour\",\"presence\",\"buzz\",\"murmur\",\"disapprobation\",\"surprise\",\"terror\",\"horror\",\"disgust\",\"made\",\"assume\",\"looked\",\"smiled\",\"vows\",\"chiming\",\"disconcert\",\"tremulousness\",\"meditation\",\"told\",\"painted\",\"pause\",\"performance\",\"harken\",\"sound\",\"rang\",\"pale\",\"passed\",\"swung\",\"clang\",\"constrained\",\"stiff-frozen\",\"strokes\",\"aware\",\"pestilence\",\"walking\",\"nod\",\"inclined\",\"came\",\"met\",\"rode\",\"hummed\",\"said\",\"halted\",\"turned\",\"reply\",\"hesitation\",\"discovery\",\"hunting\",\"give\",\"concluded\",\"smacking\",\"explained\",\"tracing\",\"asked\",\"Concluding\",\"doubts\",\"retailing\",\"gone\",\"walked\",\"sat\",\"depositing\",\"appeared\",\"pursued\",\"seeing\",\"quickened\",\"frowned\",\"order\",\"call\",\"telling\",\"announcement\",\"declining\",\"stretched\",\"contemplated\",\"continued\",\"been\",\"put\",\"produced\",\"estimate\",\"investigations\",\"admit\",\"record\",\"add\",\"conferred\",\"abrogation\",\"received\",\"hint\",\"insisted\",\"fervid\",\"assured\",\"gesticulating\",\"thrust\",\"intimated\",\"submission\",\"appeal\",\"saw\",\"resolving\",\"presented\",\"declare\",\"act\",\"advent\",\"deemed\",\"favor\",\"sent\",\"handed\",\"awakened\",\"crosser\",\"stammered\",\"passion\",\"kicked\",\"repeated\",\"slunk\",\"hurried\",\"wandered\",\"play\",\"pretended\",\"stuck\",\"angry\",\"muttering\",\"grinding\",\"heard\",\"talking\",\"stared\",\"lifted\",\"say\",\"answered\",\"wrung\",\"cried\",\"party\",\"wailing\",\"clutched\",\"shivering\",\"grew\",\"gasped\",\"ran\",\"cholera\",\"ill\",\"wailed\",\"dead\",\"run\",\"dying\",\"hid\",\"panic\",\"forgotten\",\"slept\",\"sounds\",\"crept\",\"ate\",\"drank\",\"drowsy\",\"went\",\"shut\",\"frightened\",\"cries\",\"sleepy\",\"lay\",\"wails\",\"carried\",\"wondered\",\"noise\",\"hurrying\",\"waiting\",\"rustling\",\"gliding\",\"watching\",\"slipped\",\"watched\",\"talked\",\"opened\",\"frowning\",\"hungry\",\"startled\",\"drawing\",\"thought\",\"asleep\",\"wakened\",\"exclaimed\",\"turning\",\"stayed\",\"saying\",\"footsteps\",\"neglected\",\"struck\",\"sinking\",\"lined\",\"awaiting\",\"clung\",\"start\",\"turn\",\"noticed\",\"looking\",\"muttered\",\"flushed\",\"lowered\",\"proud\",\"took\",\"closed\",\"lain\",\"list\",\"answer\",\"command\",\"greeted\",\"blocked\",\"blocking\",\"unpopular\",\"assenting\",\"anxiety\",\"pounced\",\"declared\",\"persuade\",\"ordered\",\"chuckled\",\"sighed\",\"nodded\",\"hinted\",\"assure\",\"interrupted\",\"led\",\"catching\",\"conversation\",\"murmured\",\"rose\",\"collected\",\"ensconced\",\"added\",\"rousing\",\"poured\",\"bite\",\"helped\",\"left\",\"Promoted\",\"promoted\",\"check\",\"row\",\"meeting\",\"affecting\",\"Reproved\",\"visiting\",\"resumed\",\"success\",\"inquired\",\"confessed\",\"resume\",\"intervened\",\"combed\",\"wounded\",\"demobbed\",\"appealed\",\"launched\",\"chosen\",\"happened\",\"Armistice\",\"tired\",\"peeped\",\"reading\",\"considering\",\"hear\",\"occurred\",\"TOOK\",\"started\",\"flashed\",\"pop\",\"falling\",\"wonder\",\"tried\",\"disappointment\",\"fell\",\"fall\",\"fallen\",\"got\",\"began\",\"spoke\",\"hope\",\"wish\",\"dream\",\"jumped\",\"sight\",\"trying\",\"wondering\",\"fitted\",\"knelt\",\"longed\",\"think\",\"taste\",\"finding\",\"finished\",\"brightened\",\"waited\",\"Adverb\",\"dozing\",\"nervous\",\"Coming\",\"gossiping\",\"returned\",\"pulled\",\"sprang\",\"introduced\",\"drove\",\"followed\",\"measure\",\"glances\",\"disapproval\",\"inspection\",\"sifted\",\"caught\",\"drew\",\"leaned\",\"regarded\",\"smoked\",\"fanning\",\"hot\",\"bellowed\",\"glance\",\"examined\",\"lift\",\"irritating\",\"tickling\",\"sneered\",\"sprinkled\",\"slurred\",\"civility\",\"stare\",\"pronounced\",\"curl\",\"bent\",\"undid\",\"revealing\",\"straightened\",\"ignoring\",\"swore\",\"discovered\",\"scratched\",\"ripped\",\"shook\",\"mounted\",\"disturbed\",\"jingling\",\"discussed\",\"eyeing\",\"stated\",\"gamble\",\"tribute\",\"reminds\",\"remarked\",\"Ambling\",\"laughed\",\"fleered\",\"jerked\",\"agreed\",\"joke\",\"blazing\",\"offense\",\"take\",\"go\",\"remember\",\"plodding\",\"following\",\"whistling\",\"breaking\",\"rapped\",\"called\",\"lingering\",\"says\",\"threw\",\"set\",\"learn\",\"taken\",\"promised\",\"crossed\",\"dinner\",\"observing\",\"pipe\",\"supposed\",\"talk\",\"flapped\",\"stopped\",\"glared\",\"oath\",\"spoken\",\"described\",\"hearing\",\"speaking\",\"pouring\",\"leapt\",\"played\",\"flung\",\"amusement\",\"awkwardness\",\"increased\",\"mist\",\"walk\",\"thickened\",\"reached\",\"suspended\",\"joined\",\"leaning\",\"remarking\",\"broke\",\"cursed\",\"exchanged\",\"glanced\",\"replied\",\"stirred\",\"compressing\",\"danger\",\"reddened\",\"remark\",\"speculation\",\"hits\",\"lying\",\"depicted\",\"written\",\"read\",\"displaying\",\"decided\",\"built\",\"control\",\"attended\",\"checked\",\"dealing\",\"demanded\",\"interposed\",\"announced\",\"rejecting\",\"silenced\",\"detected\",\"controlling\",\"consolation\",\"increase\",\"wind\",\"wandering\",\"clouds\",\"rain\",\"dispensed\",\"possessed\",\"care\",\"gathering\",\"drawn\",\"studied\",\"cloud\",\"sweeping\",\"blast\",\"formed\",\"surveying\",\"paused\",\"calling\",\"wished\",\"trembled\",\"idea\",\"seating\",\"gesture\",\"thrusting\",\"mused\",\"tottered\",\"regaining\",\"impudence\",\"answering\",\"fetched\",\"did\",\"poise\",\"stand\",\"cry\",\"hit\",\"striking\",\"cutting\",\"bled\",\"pain\",\"climax\",\"feelings\",\"sharp\",\"glancing\",\"pinning\",\"newly-risen\",\"retreated\",\"defeat\",\"blows\",\"erection\",\"vanished\",\"emerged\",\"accepted\",\"promise\",\"shrunken\",\"listened\",\"verbiage\",\"resentment\",\"coined\",\"juggled\",\"poised\",\"regard\",\"wrapping\",\"disputed\",\"tossed\",\"raised\",\"disappeared\",\"gave\",\"crashing\",\"gulps\",\"outlined\",\"speech\",\"silent\",\"thoughtful\",\"sensation\",\"sickness\",\"off\",\"buttered\",\"smile\",\"folded\",\"drawled\",\"glint\",\"donned\",\"teased\",\"writing\",\"washed\",\"temptation\",\"shower\",\"enjoy\",\"answers\",\"completing\",\"recorded\",\"travelled\",\"effaced\",\"astonished\",\"Looking\",\"opening\",\"rush\",\"entering\",\"beginning\",\"coming\",\"vowed\",\"delight\",\"breath\",\"consented\",\"blossomed\",\"varnished\",\"burst\",\"flowered\",\"delighted\",\"let\",\"invitation\",\"flatter\",\"abuse\",\"visit\",\"disclosed\",\"Observing\",\"scolding\",\"coughing\",\"tear\",\"adjusting\",\"paid\",\"astonishment\",\"joy\",\"raptures\",\"trimming\",\"sorry\",\"afraid\",\"feeding\",\"growled\",\"seized\",\"pleaded\",\"staring\",\"pointed\",\"emptied\",\"licking\",\"shake\",\"expressed\",\"tilted\",\"question\",\"dip\",\"roll\",\"keeping\",\"faltered\",\"hugged\",\"clasping\",\"limped\",\"picking\",\"going\",\"hugging\",\"impression\",\"soaked\",\"smothered\",\"lamed\",\"cut\",\"stung\",\"torn\",\"gained\",\"sinister\",\"sullen\",\"arose\",\"wheeled\",\"melted\",\"murmuring\",\"dropped\",\"listening\",\"split\",\"whispered\",\"stepped\",\"closing\",\"hung\",\"harkened\",\"starting\",\"fearing\",\"fluttered\",\"danced\",\"hesitated\",\"impelled\",\"power\",\"shrinking\",\"song\",\"fear\",\"note\",\"smoke\",\"shouting\",\"peering\",\"flew\",\"dancing\",\"twined\",\"twirled\",\"flickered\",\"motion\",\"sang\",\"forgot\",\"dazzled\",\"bewildered\",\"fascinated\",\"whirl\",\"panting\",\"awoke\",\"peered\",\"blurred\",\"crowding\",\"fled\",\"stumbling\",\"feeling\",\"voices\",\"toiled\",\"haste\",\"struggling\",\"sank\",\"exhausted\",\"trembling\",\"drifted\",\"sleep\",\"sifting\",\"remembered\",\"bethought\",\"vision\",\"waving\",\"flying\",\"determination\",\"gazed\",\"dreaming\",\"abstracted\",\"alert\",\"quizzical\",\"laughing\",\"challenged\",\"bridled\",\"scoffed\",\"asserted\",\"scornful\",\"soft\",\"waking\",\"daydream\",\"noting\",\"glinting\",\"fumbling\",\"Untying\",\"clapped\",\"bounded\",\"flitted\",\"swaying\",\"dripping\",\"draught\",\"touched\",\"shock\",\"rained\",\"brightening\",\"still\",\"mischief\",\"hopped\",\"nestled\",\"pointing\",\"admitted\",\"faded\",\"crouched\",\"puzzled\",\"supper\",\"warmth\",\"cheer\",\"fared\",\"address\",\"refuse\",\"misinterpreted\",\"request\",\"difference\",\"storming\",\"concerned\",\"assault\",\"event\",\"believe\",\"treating\",\"wrangle\",\"sally\",\"separated\",\"forded\",\"planted\",\"fighting\",\"orders\",\"excesses\",\"loaded\",\"plunder\",\"confusion\",\"conquest\",\"frenzy\",\"slaughter\",\"riot\",\"disgraced\",\"jests\",\"catchwords\",\"plundering\",\"break\",\"yelling\",\"dread\",\"crowded\",\"rushed\",\"attendance\",\"throng\",\"proclamation\",\"words\",\"shouted\",\"stripped\",\"escaped\",\"Preserved\",\"removed\",\"transported\",\"worshipped\",\"completed\",\"breathed\",\"commanded\",\"bowed\",\"predicted\",\"polluted\",\"broken\",\"sacrilege\",\"perished\",\"possession\",\"placed\",\"won\",\"STORMING\",\"Extracted\",\"revert\",\"stored\",\"entrusted\",\"wedding\",\"sit\",\"recalled\",\"marriage\",\"chatted\",\"maintain\",\"exertions\",\"flow\",\"ideas\",\"hoped\",\"animated\",\"inquiries\",\"grief\",\"loss\",\"adverb\",\"repeating\",\"exclamation\",\"Seating\",\"quitting\",\"withdrew\",\"playing\",\"bustling\",\"directions\",\"crinkled\",\"mass\",\"croquet\",\"lit\",\"drag\",\"gaze\",\"advancing\",\"approach\",\"seated\",\"facing\",\"plunge\",\"surveyed\",\"reminded\",\"given\",\"understanding\",\"laugh\",\"sparkled\",\"adventure\",\"realized\",\"yawned\",\"proposed\",\"instructed\",\"prepared\",\"holding\",\"lifting\",\"descended\",\"shrugged\",\"felt\",\"understood\",\"nodding\",\"wanted\",\"kissed\",\"rolled\",\"saving\",\"gathered\",\"reflected\",\"fan\",\"puffs\",\"performing\",\"strolled\",\"amused\",\"entrance\",\"undressed\",\"piled\",\"utterances\",\"leaving\",\"assumed\",\"recommending\",\"submitted\",\"imitation\",\"watered\",\"tears\",\"catastrophe\",\"settling\",\"accident\",\"circumstance\",\"returning\",\"journey\",\"desire\",\"pass\",\"pacing\",\"work\",\"risen\",\"problem\",\"shown\",\"waved\",\"indicated\",\"fancy\",\"observe\",\"deduce\",\"changed\",\"fail\",\"rubbed\",\"tell\",\"scraped\",\"caused\",\"remove\",\"deduction\",\"lighting\",\"throwing\",\"wrote\",\"endeavouring\",\"comes\",\"out\",\"outburst\",\"plumes\",\"covered\",\"ended\",\"signalised\",\"differing\",\"connected\",\"thunders\",\"wagged\",\"sunset\",\"preferred\",\"sauntered\",\"leave\",\"gratified\",\"preparing\",\"attracted\",\"cavalcade\",\"blushed\",\"recollected\",\"blush\",\"recollection\",\"beholding\",\"emotion\",\"witnessed\",\"driving\",\"proceed\",\"determined\",\"designs\",\"Arriving\",\"dismounted\",\"sending\",\"proceeded\",\"illuminated\",\"composure\",\"intention\",\"open\",\"overtook\",\"compliments\",\"begged\",\"ingratiate\",\"parting\",\"marrying\",\"arrived\",\"enquired\",\"tapped\",\"bid\",\"rivetted\",\"fixed\",\"painting\",\"shaking\",\"offered\",\"wiped\",\"cast\",\"drop\",\"blotted\",\"discoloured\",\"addressing\",\"thinking\",\"hushed\",\"odour\",\"scent\",\"perfume\",\"smoking\",\"catch\",\"effect\",\"shouldering\",\"circling\",\"insistence\",\"roar\",\"disappearance\",\"tossing\",\"elevated\",\"whorls\",\"strolling\",\"insist\",\"expression\",\"speak\",\"abducted\",\"discharged\",\"approaching\",\"pressed\",\"shudder\",\"darkened\",\"showed\",\"protest\",\"accents\",\"rising\",\"feigning\",\"protested\",\"belied\",\"sigh\",\"supported\",\"assisted\",\"desirable\",\"leading\",\"declined\",\"proffered\",\"bit\",\"aloof\",\"making\",\"picked\",\"lurked\",\"cluttered\",\"clutching\",\"seizure\",\"listen\",\"moved\",\"filtering\",\"place\",\"chord\",\"roamed\",\"thrill\",\"shattering\",\"regarding\",\"offended\",\"resented\",\"eating\",\"irritability\",\"anticipated\",\"shot\",\"grunted\",\"celebrated\",\"victory\",\"putting\",\"sniffing\",\"Smoking\",\"helplessness\",\"recoiling\",\"researches\",\"draft\",\"infuriated\",\"cleared\",\"mount\",\"tapping\",\"approval\",\"arranged\",\"whirring\",\"rippling\",\"breeze\",\"working\",\"Reaching\",\"partial\",\"begun\",\"swayed\",\"pretend\",\"alluding\",\"learned\",\"waxing\",\"smelling\",\"knocking\",\"learning\",\"breakfast\",\"drinking\",\"served\",\"selected\",\"buried\",\"poked\",\"pronouncing\",\"perceived\",\"counted\",\"rejoined\",\"thanked\",\"compliment\",\"astride\",\"attacked\",\"straightening\",\"converted\",\"springing\",\"kicking\",\"jump\",\"scattering\",\"acquaintance\",\"standing\",\"observation\",\"bought\",\"responded\",\"write\",\"claim\",\"betook\",\"attention\",\"gratification\",\"escorted\",\"inherited\",\"sale\",\"lamentations\",\"sold\",\"step\",\"appearance\",\"commenced\",\"bidding\",\"hold\",\"breathless\",\"visitation\",\"shaken\",\"consequence\",\"scattered\",\"appearing\",\"present\",\"asking\",\"grimace\",\"faced\",\"groan\",\"reproach\",\"charmed\",\"adjured\",\"inference\",\"fix\",\"quitted\",\"handshook\",\"communicative\",\"had\",\"wonders\",\"committed\",\"departed\",\"professed\",\"apparition\",\"kick\",\"episode\",\"organizing\",\"imagined\",\"arrive\",\"chinked\",\"doubled\",\"affirmative\",\"tightened\",\"shutting\",\"rage\",\"grappled\",\"twist\",\"struggled\",\"fury\",\"lolling\",\"ebbed\",\"glazed\",\"flagged\",\"jolted\",\"shriek\",\"anger\",\"quick\",\"choked\",\"hiding\",\"struggle\",\"grumbled\",\"calculated\",\"attempted\",\"thrown\",\"filing\",\"nursing\",\"rattled\",\"expecting\",\"twisted\",\"growl\",\"stormed\",\"raged\",\"assailed\",\"allowed\",\"carted\",\"trucked\",\"deposited\",\"mauling\",\"headed\",\"deposition\",\"cheated\",\"fever\",\"trekked\",\"spending\",\"overcharged\",\"weighed\",\"transhipped\",\"excited\",\"suspected\",\"ascertained\",\"referring\",\"mistake\",\"anticipating\",\"trip\",\"excitement\",\"burned\",\"cleansed\",\"erected\",\"shrank\",\"contact\",\"wakes\",\"fair\",\"whine\",\"rushing\",\"stood\",\"tooting\",\"braying\",\"quivered\",\"wrong\",\"whining\",\"wizzening\",\"trudged\",\"pistol-shots\",\"cracks\",\"screeching\",\"shouts\",\"screeches\",\"gazing\",\"killed\",\"spent\",\"lamented\",\"smelled\",\"change\",\"rest\",\"drifting\",\"flood\",\"haze\",\"toying\",\"satisfied\",\"shone\",\"sombre\",\"appear\",\"stir\",\"boarded\",\"regret\",\"thoughts\",\"kiss\",\"touch\",\"clacked\",\"passing\",\"replaced\",\"conjectures\",\"termed\",\"pulling\",\"conscious\",\"fidgetting\",\"intuition\",\"prevailed\",\"agreeable\",\"sprung\",\"marked\",\"aroused\",\"clew\",\"ache\",\"affected\",\"realised\",\"smiling\",\"mingling\",\"interest\",\"growing\",\"amazement\",\"offer\",\"decline\",\"acknowledge\",\"planned\",\"dash\",\"ask\",\"flutter\",\"reassured\",\"studying\",\"witticism\",\"dragged\",\"whimpering\",\"tucked\",\"deploring\",\"escapade\",\"longing\",\"fought\",\"objections\",\"woke\",\"gong\",\"racket\",\"dressed\",\"explored\",\"sought\",\"plunged\",\"persuasive\",\"hypnotized\",\"aghas\",\"consider\",\"clashed\",\"intend\",\"snowy\",\"sunny\",\"sparkling\",\"grumbling\",\"tiptoed\",\"accept\",\"return\",\"receiving\",\"deliberate\",\"drowned\",\"shivered\",\"crying\",\"wringing\",\"mewing\",\"clinging\",\"chased\",\"stooped\",\"wipe\",\"projecting\",\"tease\",\"mewed\",\"unwound\",\"tied\",\"bared\",\"blowing\",\"severity\",\"predicament\",\"darted\",\"ascent\",\"colder\",\"tearing\",\"tearful\",\"tying\",\"stabbed\",\"saddened\",\"laying\",\"do\",\"meant\",\"generosity\",\"peevish\",\"wrangled\",\"guise\",\"leant\",\"repressing\",\"shocked\",\"sympathized\",\"repressed\",\"persisted\",\"thumped\",\"displeasure\",\"toy\",\"censured\",\"mumbled\",\"decision\",\"reversed\",\"parted\",\"revealed\",\"apologizing\",\"lateness\",\"exclaiming\",\"expect\",\"forward\",\"beckoned\",\"glad\",\"filling\",\"appointed\",\"enjoying\",\"informed\",\"advice\",\"bow\",\"whinnied\",\"box\",\"trotted\",\"chase\",\"frost\",\"pricked\",\"cantered\",\"galloping\",\"snorted\",\"away\",\"barking\",\"straight\",\"fright\",\"leaped\",\"dashing\",\"end\",\"whipped\",\"bleeding\",\"down\",\"groaning\",\"getting\",\"raise\",\"riding\",\"bang\",\"imprisonment\",\"swear\",\"deaths\",\"send\",\"hissing\",\"clenching\",\"drive\",\"spoilt\",\"puzzle\",\"reported\",\"INCIDENT\",\"bearing\",\"beheld\",\"standstill\",\"hopping\",\"untied\",\"survey\",\"glow\",\"lustre\",\"reflection\",\"deed\",\"smiles\",\"conjecture\",\"actions\",\"steps\",\"descending\",\"espial\",\"dispute\",\"reverie\",\"stepping\",\"handing\",\"gaining\",\"retreating\",\"indifference\",\"trail\",\"exhorted\",\"prayer\",\"weep\",\"cower\",\"sprawled\",\"lengthening\",\"spurring\",\"flogged\",\"emerge\",\"unbarring\",\"fastening\",\"wishing\",\"Locking\",\"refilled\",\"ridden\",\"flocking\",\"encountered\",\"tilt\",\"busy\",\"warranted\",\"testify\",\"grant\",\"rowing\",\"errand\",\"pace\",\"dissatisfaction\",\"exchange\",\"recount\",\"stroke\",\"ladling\",\"puff\",\"spat\",\"crammed\",\"desired\",\"confess\",\"approached\",\"persuaded\",\"annoyed\",\"discovering\",\"toil\",\"beckon\",\"sounded\",\"miss\",\"excel\",\"pushing\",\"pleasure\",\"apply\",\"laid\",\"carrying\",\"conclusions\",\"suggest\",\"supposing\",\"conclusion\",\"presentation\",\"vanishes\",\"emerges\",\"inferences\",\"paced\",\"ring\",\"beg\",\"occupation\",\"becomes\",\"mastered\",\"earned\",\"stretching\",\"practising\",\"sneezing\",\"stop\",\"clicketing\",\"exercises\",\"goes\",\"reappears\",\"inflict\",\"amuse\",\"watch\",\"keep\",\"setting\",\"knocked\",\"pushed\",\"hush\",\"expedition\",\"wasted\",\"stopping\",\"liked\",\"insisting\",\"ruined\",\"restoration\",\"prevented\",\"disengaged\",\"adapted\",\"excuse\",\"dropping\",\"interested\",\"blazed\",\"judged\",\"urged\",\"suggested\",\"breakfasted\",\"word\",\"marking\",\"skirted\",\"advanced\",\"hunted\",\"squatted\",\"investigate\",\"Look\",\"scrambled\",\"foamed\",\"boiled\",\"searched\",\"lunch\",\"pumped\",\"assembled\",\"settlement\",\"borne\",\"occupied\",\"fastened\",\"sentence\",\"scandal\",\"warrant\",\"shame\",\"Stretching\",\"repelled\",\"action\",\"bore\",\"winked\",\"judging\",\"glided\",\"disappearing\",\"blight\",\"pierced\",\"yelp\",\"twittering\",\"calls\",\"afterglow\",\"flooded\",\"flaring\",\"swept\",\"moan\",\"strode\",\"fading\",\"brewing\",\"trotting\",\"Peering\",\"moving\",\"deadened\",\"loom\",\"clinking\",\"jingled\",\"curses\",\"thuds\",\"strain\",\"heaves\",\"Rustlings\",\"footstep\",\"wave\",\"became\",\"crash\",\"attested\",\"click\",\"sputtering\",\"efforts\",\"crackling\",\"flame\",\"shuffled\",\"glittered\",\"queried\",\"heerd\",\"comin\",\"moaned\",\"raindrops\",\"pattered\",\"Silence\",\"cracked\",\"query\",\"showing\",\"backed\",\"Sent\",\"ejaculated\",\"hoof-beats\",\"Left\",\"glimpse\",\"visited\",\"search\",\"find\",\"howling\",\"drink\",\"refused\",\"mixed\",\"follow\",\"questions\",\"implored\",\"dried\",\"happy\",\"cuddled\",\"sob\",\"sobbed\",\"guessed\",\"pulse\",\"ready\",\"deprived\",\"anxious\",\"opportunity\",\"bade\",\"permission\",\"enjoyed\",\"funeral\",\"luncheon\",\"raced\",\"lurking\",\"landed\",\"agreement\",\"display\",\"affectionate\",\"expressions\",\"know\",\"fragrance\",\"pride\",\"confronted\",\"storm\",\"opposition\",\"outbreak\",\"palpitating\",\"invisible\",\"jet\",\"compared\",\"eruption\",\"vigil\",\"vibrating\",\"activity\",\"flash\",\"projection\",\"streamer\",\"swimming\",\"gleam\",\"blaze\",\"flaming\",\"jetting\",\"established\",\"dapple\",\"intoxicating\",\"whispering\",\"heightened\",\"heat-haze\",\"sun-sparkles\",\"transplanted\",\"plucked\",\"substitution\",\"suffering\",\"typing\",\"inserted\",\"altered\",\"appended\",\"snap\",\"whimperings\",\"tramped\",\"planked\",\"re-addressing\",\"recognised\",\"tore\",\"inspected\",\"tackled\",\"despatched\",\"scented\",\"sorted\",\"phrased\",\"absent\",\"sadness\",\"re-entered\",\"revolved\",\"enlightened\",\"situation\",\"resolutions\",\"outbreaks\",\"apparent\",\"sketched\",\"solaced\",\"rehearsed\",\"forgetting\",\"sped\",\"sorrow\",\"inspiration\",\"reversal\",\"hopes\",\"arrival\",\"weeping\",\"reposing\",\"troublesome\",\"lessening\",\"ceremony\",\"crisis\",\"heaviness\",\"indignation\",\"tea-service\",\"overthrow\",\"suffered\",\"dismissed\",\"wept\",\"mingle\",\"lessened\",\"eat\",\"grievance\",\"avenged\",\"swallowed\",\"draw\",\"embarrassment\",\"figurative usage\",\"recollect\",\"exploit\",\"bewitched\",\"battle\",\"mention\",\"swarming\",\"charge\",\"thud\",\"kept\",\"runs\",\"fluttering\",\"driven\",\"scrimmage\",\"flashing\",\"rubbing\",\"stamping\",\"dodged\",\"shoulder\",\"sitting\",\"smell\",\"taught\",\"cold\",\"chilled\",\"confused\",\"worked\",\"nap\",\"brood\",\"brooding\",\"dining\",\"objected\",\"contented\",\"mots\",\"staid\",\"overheard\",\"whisper\",\"presentiment\",\"engagement\",\"coaxed\",\"exertion\",\"bark\",\"pity\",\"exhibited\",\"bidden\",\"quietly\",\"newly-seen\",\"sleeping\",\"sobs\",\"familiar\",\"intoned\",\"Halted\",\"blessed\",\"crosses\",\"gurgling\",\"whistle\",\"whistles\",\"skipped\",\"propped\",\"dipped\",\"lathered\",\"Ceasing\",\"shave\",\"shaved\",\"raving\",\"moaning\",\"pull\",\"stirring\",\"Leaning\",\"clearing\",\"fretted\",\"giving\",\"kills\",\"Laughter\",\"asks\",\"pinched\",\"Laughing\",\"Drawing\",\"linked\",\"clacking\",\"Parried\",\"begging\",\"postpone\",\"prompted\",\"feel\",\"disembarked\",\"nearing\",\"stolen\",\"invoked\",\"consorted\",\"evasion\",\"winced\",\"touching\",\"delay\",\"Recognition\",\"enquiry\",\"wound\",\"rejoinder\",\"qualification\",\"shirked\",\"muffled\",\"forsaken\",\"protection\",\"appointment\",\"presenting\",\"preparation\",\"impulse\",\"sense\",\"response\",\"comparison\",\"recognition\",\"case\",\"deserted\",\"madness\",\"snapped\",\"compromised\",\"forged\",\"fragrant\",\"showered\",\"chirping\",\"down-spirited\",\"chirp\",\"scrape\",\"engaging\",\"rented\",\"pool\",\"tattoo\",\"Hearing\",\"echoing\",\"snapping\",\"coughed\",\"contracted\",\"threatened\",\"controlled\",\"shoveled\",\"whistled\",\"cough\",\"producing\",\"soothed\",\"cosseted\",\"breathing\",\"dusted\",\"breaths\",\"bending\",\"swinging\",\"brisker\",\"Surprised\",\"sowing\",\"spoiled\",\"knitting\",\"twitched\",\"jaunting\",\"headache\",\"dumb\",\"tea\",\"jolt\",\"distinguish\",\"registered\",\"traversed\",\"recognized\",\"rung\",\"sentenced\",\"pardoned\",\"serving\",\"misdemeanor\",\"Leaving\",\"paces\",\"Retracing\",\"varnishing\",\"task\",\"effusion\",\"air\",\"gravity\",\"quoted\",\"remarks\",\"retraced\",\"converged\",\"seeking\",\"light\",\"punishment\",\"hovered\",\"flickering\",\"campfire\",\"reflections\",\"fanned\",\"paling\",\"sparks\",\"echoed\",\"clink\",\"pads\",\"reach\",\"move\",\"halt\",\"vigi\",\"preparations\",\"movements\",\"grateful\",\"solitude\",\"traveled\",\"digging\",\"crack\",\"halting\",\"packed\",\"slaps\",\"march\",\"camped\",\"travel\",\"reserve\",\"respond\",\"realization\",\"dug\",\"bring\",\"grasped\",\"amazed\",\"pitying\",\"curious\",\"quiver\",\"astounded\",\"force\",\"seep\",\"complied\",\"filled\",\"moist\",\"dark\",\"sniff\",\"agree\",\"examining\",\"complaining\",\"advised\",\"softened\",\"pecking\",\"lecture\",\"stroking\",\"snow\",\"crackled\",\"lecturing\",\"counsel\",\"expostulated\",\"persuasions\",\"discourse\",\"resolution\",\"concern\",\"prompting\",\"rise\",\"sick\",\"terrified\",\"reflect\",\"abandoning\",\"entreaties\",\"reproached\",\"blow\",\"snowfall\",\"staggered\",\"stamped\",\"brisked\",\"surprised\",\"melting\",\"dripped\",\"advances\",\"whisked\",\"mixing\",\"stabs\",\"cooked\",\"delaying\",\"gaping\",\"rigid\",\"recover\",\"messing\",\"done\",\"unfolded\",\"extended\",\"Talkin '\",\"tangent\",\"smouldered\",\"eaten\",\"drunk\",\"warmed\",\"explanation\",\"acknowledgment\",\"snatching\",\"upsettled\",\"animation\",\"ARRIVAL\",\"fortune\",\"embarrassed\",\"INTRODUCED\",\"reposed\",\"dumped\",\"Getting\",\"flourishing\",\"directed\",\"hitching\",\"singing\",\"dined\",\"lingered\",\"dawdled\",\"scanned\",\"thrilled\",\"mantled\",\"suffused\",\"wooing\",\"flitting\",\"confusing\",\"guess\",\"leaping\",\"pictured\",\"blinking\",\"meditating\",\"bellied\",\"wistfulness\",\"flared\",\"fleeing\",\"arched\",\"love\",\"giggling\",\"Trailing\",\"indignant\",\"glowed\",\"rammed\",\"rid\",\"yawn\",\"patted\",\"stroked\",\"fidgeting\",\"bell\",\"apologize\",\"averred\",\"mole-catching\",\"fit\",\"cured\",\"uncurled\",\"up\",\"packing\",\"extracted\",\"inscription\",\"blushing\",\"tipsify\",\"imputes\",\"tells\",\"detained\",\"apprehend\",\"bequeathed\",\"abandon\",\"applying\",\"aims\",\"discontinued\",\"perplexity\",\"uneasiness\",\"remorse\",\"detaining\",\"complying\",\"infatuated\",\"conjuring\",\"recommended\",\"instigation\",\"endeavoured\",\"enraged\",\"unkindness\",\"Disappointed\",\"burnt\",\"denied\",\"anguish\",\"illness\",\"recovery\",\"remind\",\"foresee\",\"elopement\",\"surging\",\"seething\",\"carnage\",\"allowing\",\"slip\",\"escape\",\"aghast\",\"howl\",\"blunder\",\"located\",\"departure\",\"glimpses\",\"topped\",\"arming\",\"suspense\",\"strapped\",\"darkness\",\"await\",\"trot\",\"pursuing\",\"positive\",\"hoping\",\"report\",\"shots\",\"consternation\",\"dismay\",\"riveted\",\"narration\",\"charging\",\"shooting\",\"whooping\",\"convinced\",\"routing\",\"apprehension\",\"Riding\",\"grasping\",\"BIRTH\",\"condescended\",\"provision\",\"named\",\"adhered\",\"premised\",\"burying\",\"chuses\",\"Going\",\"dreary\",\"beamed\",\"placing\",\"BORN\",\"ushered\",\"gasping\",\"struggles\",\"sneezed\",\"advertise\",\"imposed\",\"proof\",\"rustled\",\"warm\",\"rub\",\"tasting\",\"perspective\",\"imprinted\",\"shuddered\",\"chafed\",\"pausing\",\"applied\",\"dress\",\"badged\",\"ticketed\",\"retiring\",\"tremble\",\"adopted\",\"arguing\",\"encouragement\",\"debate\",\"opposed\",\"advocated\",\"clamored\",\"bids\",\"bustled\",\"disdaining\",\"comments\",\"discussions\",\"crawled\",\"wreathed\",\"rebellion\",\"winds\",\"clangoring\",\"rejoicing\",\"shiver\",\"enlisted\",\"milking\",\"silence\",\"milk\",\"disappointed\",\"primed\",\"destroyed\",\"peeled\",\"plan\",\"fill\",\"beating\",\"ecstasy\",\"trails\",\"discouraged\",\"punching\",\"resurrected\",\"seize\",\"arrest\",\"whirled\",\"snatched\",\"through\",\"stealing\",\"Said\",\"scare\",\"suspicion\",\"flattered\",\"forestalled\",\"vexed\",\"overlooked\",\"missed\",\"forgive\",\"miscarried\",\"stumbled\",\"strike\",\"advertised\",\"withdrawn\",\"raffle\",\"pay\",\"separation\",\"affronted\",\"desponding\",\"welcomed\",\"foreboding\",\"glowing\",\"pressing\",\"frown\",\"begin\",\"withdraw\",\"sheltered\",\"express\",\"wincing\",\"uttered\",\"preceded\",\"soliloquised\",\"relieving\",\"conjectured\",\"ejaculation\",\"admire\",\"distinguished\",\"chatter\",\"clatter\",\"squealing\",\"haunted\",\"bestow\",\"decamp\",\"attempting\",\"sneaking\",\"watering\",\"caress\",\"gnarl\",\"provoked\",\"checking\",\"punch\",\"striding\",\"dived\",\"guardianship\",\"imagining\",\"winking\",\"faces\",\"irritated\",\"interpose\",\"proceeding\",\"issued\",\"parrying\",\"demand\",\"climbed\",\"yelping\",\"used\",\"subsided\",\"heaving\",\"treatment\",\"restoring\",\"doubt\",\"overwhelmed\",\"tempest\",\"a-bothering\",\"ease\",\"mad\",\"tiresome\",\"lonesome\",\"shivers\",\"down-hearted\",\"scared\",\"crawling\",\"flipped\",\"shriveled\",\"licks\",\"Says\",\"tiptoeing\",\"stooping\",\"scrouched\",\"itching\",\"itch\",\"itched\",\"reckoned\",\"breathe\",\"snore\",\"comfortable\",\"build\",\"executed\",\"prophecy\",\"grasp\",\"operations\",\"hard\",\"brackish\",\"execution\",\"wrested\",\"trodden\",\"crime\",\"blasted\",\"unearthed\",\"commissioned\",\"elated\",\"appalled\",\"sailed\",\"chartered\",\"scouring\",\"washing\",\"tripped\",\"overturning\",\"drenched\",\"oaths\",\"regained\",\"felled\",\"snarl\",\"crushing\",\"white\",\"mutiny\",\"firing\",\"lodged\",\"Words\",\"assisting\",\"limping\",\"wounding\",\"glowering\",\"breakdown\",\"bored\",\"consumption\",\"sampled\",\"pleased\",\"tipsy\",\"essayed\",\"exaltation\",\"reaction\",\"incident\",\"rejoiced\",\"hunched\",\"dreamy\",\"bound\",\"operation\",\"curled\",\"KISS\",\"gives\",\"shipwrecked\",\"swims\",\"sail\",\"voyage\",\"inform\",\"spied\",\"shift\",\"rowed\",\"computation\",\"trusted\",\"overset\",\"flurry\",\"swam\",\"tide\",\"abated\",\"awaked\",\"roared\",\"hurt\",\"falls\",\"ventured\",\"wrench\",\"bind\",\"loosened\",\"shout\",\"flight\",\"striving\",\"volley\",\"design\",\"knew\",\"oration\",\"support\",\"acted\",\"threatenings\",\"promises\",\"demands\",\"signify\",\"learnt\",\"supplied\",\"sign\",\"slung\",\"signs\",\"warning\",\"tempted\",\"remembrance\",\"interpreted\",\"imaginations\",\"considered\",\"treated\",\"posture\",\"show\",\"blisters\",\"tokens\",\"repetitions\",\"relaxing\",\"conjecturing\",\"torrent\",\"violence\",\"daubed\",\"smart\",\"disposed\",\"mingled\",\"consulted\",\"passage\",\"over\",\"marks\",\"Gets\",\"commencement\",\"dispelled\",\"agitation\",\"dedicated\",\"own\",\"entreated\",\"hired\",\"complaints\",\"acquainted\",\"secure\",\"confessing\",\"abandoned\",\"bestowed\",\"solicited\",\"undertaking\",\"enterprise\",\"enthusiasm\",\"stifles\",\"puffing\",\"detect\",\"dragging\",\"chirps\",\"drags\",\"notion\",\"float\",\"choose\",\"steadying\",\"inclining\",\"pacified\",\"groped\",\"kindled\",\"glimmer\",\"trod\",\"Placing\",\"skinning\",\"munching\",\"smells\",\"vapor\",\"gloom\",\"unnerved\",\"pondered\",\"acting\",\"reined\",\"thrilling\",\"experiment\",\"uplifted\",\"Shaking\",\"Noticing\",\"conducted\",\"heighten\",\"sentiments\",\"accosted\",\"doubted\",\"awed\",\"deepen\",\"speculating\",\"routed\",\"saved\",\"disaster\",\"admonishing\",\"diminished\",\"spread\",\"certainty\",\"objects\",\"shelved\",\"discussion\",\"premise\",\"duel\",\"smuggled\",\"chill\",\"ascended\",\"curiosity\",\"clinched\",\"declaring\",\"propitiated\",\"plodded\",\"cheering\",\"rebuking\",\"ascending\",\"collared\",\"whack\",\"directing\",\"ploughing\",\"account\",\"pretending\",\"summons\",\"bursting\",\"inhabited\",\"shrieked\",\"carving\",\"snorting\",\"choking\",\"screaming\",\"demolishing\",\"discuss\",\"misadventure\",\"explosion\",\"warned\",\"letting\",\"stampeded\",\"rolling\",\"obtained\",\"confession\",\"tales\",\"wrought\",\"charged\",\"attack\",\"christened\",\"sweating\",\"grunting\",\"purchasing\",\"graduated\",\"dallied\"],\"x\":[2.5226547718048096,-47.381103515625,-15.108099937438965,5.861919403076172,-9.331326484680176,5.20269775390625,-36.00600051879883,42.305458068847656,-32.14439010620117,18.29448699951172,18.29448699951172,24.511884689331055,-3.4363913536071777,34.572818756103516,36.507144927978516,6.88671875,-33.81891632080078,-11.620354652404785,5.109312534332275,27.55644416809082,-10.966551780700684,31.57855224609375,27.556377410888672,17.183137893676758,-18.44545555114746,16.382305145263672,-15.062105178833008,31.328857421875,34.561614990234375,-1.2362569570541382,-34.29875183105469,-32.167205810546875,27.776113510131836,-7.183347702026367,43.58900833129883,24.15077781677246,-5.356809616088867,11.840299606323242,-4.02882719039917,-25.76327896118164,9.798273086547852,1.4589240550994873,-33.282711029052734,12.549375534057617,37.43820571899414,-7.371753215789795,-12.601768493652344,9.503400802612305,15.413216590881348,-12.269024848937988,-50.66064453125,-27.40536880493164,42.391319274902344,9.503400802612305,20.71814727783203,29.929380416870117,-11.935989379882812,17.790647506713867,0.13866645097732544,-20.432857513427734,43.264278411865234,-7.87683629989624,27.1721134185791,-27.56205177307129,22.755292892456055,-5.255445957183838,-35.85227966308594,-4.368992805480957,-1.5907052755355835,-26.546415328979492,30.92462921142578,-23.8984317779541,15.36525821685791,21.52744483947754,20.617475509643555,15.69051456451416,11.442427635192871,-25.015520095825195,-28.5102481842041,15.614120483398438,22.350730895996094,-9.479394912719727,-49.7938232421875,0.2708151638507843,6.106795787811279,21.394826889038086,23.733835220336914,7.537425994873047,57.05852127075195,38.90782165527344,-54.43800354003906,13.162801742553711,-9.554762840270996,2.0749831199645996,-12.451679229736328,-54.7514762878418,-54.925819396972656,-37.238712310791016,-10.360079765319824,22.118396759033203,6.4541473388671875,30.098154067993164,-12.51103401184082,32.948299407958984,-2.4311304092407227,-11.385339736938477,-1.430907130241394,-26.236125946044922,40.35966873168945,6.388125896453857,-7.516721248626709,-35.15956115722656,-8.279034614562988,-47.90156173706055,33.221900939941406,36.8183479309082,-12.959397315979004,13.436075210571289,8.833342552185059,-42.4814453125,24.886932373046875,21.082700729370117,39.44467544555664,14.984026908874512,0.7955660223960876,-39.44459533691406,10.417197227478027,3.0243637561798096,29.764890670776367,-16.743093490600586,-1.1792349815368652,-57.39850997924805,-15.283966064453125,-24.242603302001953,-13.351493835449219,-6.534239292144775,-8.199358940124512,-16.35820960998535,21.407949447631836,-39.54713821411133,30.96366310119629,23.393587112426758,-35.023841857910156,32.58675765991211,-26.598529815673828,-34.264827728271484,1.6973240375518799,-17.8408203125,16.97710609436035,-10.304067611694336,1.7263108491897583,-17.49718475341797,30.13224220275879,31.101337432861328,31.50804328918457,31.79246711730957,-12.810676574707031,38.75053787231445,18.4207763671875,-52.5394401550293,28.63326644897461,-12.0115966796875,-30.36216926574707,37.07548141479492,-40.172664642333984,22.511699676513672,21.4114990234375,-23.835168838500977,28.219079971313477,19.25344467163086,-35.31488037109375,7.7513909339904785,23.138015747070312,-29.88346290588379,1.6947672367095947,-0.9891648292541504,7.627853870391846,-33.93568801879883,-15.334460258483887,-2.5218915939331055,8.57807731628418,32.9092903137207,-8.393939018249512,-19.36586570739746,-25.66167640686035,-14.005988121032715,-31.50940704345703,29.71756935119629,28.592031478881836,-14.9844331741333,16.564401626586914,25.790945053100586,-5.793014049530029,1.5293809175491333,22.926666259765625,1.3508236408233643,-33.60264587402344,-5.910501003265381,-0.6881531476974487,30.374420166015625,32.069732666015625,-14.402236938476562,-8.716338157653809,-17.400733947753906,-4.138420581817627,16.357872009277344,-31.336624145507812,-39.162906646728516,-24.947677612304688,-27.274627685546875,-8.68076229095459,10.703617095947266,-30.62845230102539,13.899970054626465,-12.49517822265625,5.40676736831665,-39.4289436340332,16.545175552368164,-13.254830360412598,25.22226333618164,32.03959655761719,-39.17854309082031,-2.0938475131988525,-13.975275039672852,-13.762160301208496,18.930744171142578,-24.44901466369629,-21.16754722595215,1.1570801734924316,8.274385452270508,3.3501789569854736,-19.85508918762207,10.741167068481445,20.006107330322266,-3.95636248588562,-23.281166076660156,25.100662231445312,-18.881593704223633,22.296104431152344,-50.75752639770508,12.494672775268555,19.588279724121094,4.200727462768555,-4.053369522094727,23.632062911987305,-20.80724334716797,31.184349060058594,33.06485366821289,-8.341795921325684,30.230073928833008,25.47833251953125,17.987396240234375,-35.1240234375,-31.402767181396484,-42.72793197631836,13.087129592895508,11.58979320526123,30.331480026245117,24.224285125732422,31.237627029418945,31.931318283081055,-1.4254440069198608,22.88726806640625,-16.40399932861328,-33.971492767333984,14.616201400756836,-25.62899398803711,2.567127227783203,7.0503010749816895,16.276935577392578,-42.68058776855469,-11.721724510192871,18.802902221679688,29.616506576538086,18.792680740356445,48.979454040527344,23.889623641967773,27.392759323120117,11.74593734741211,-3.8805091381073,19.970752716064453,35.97283172607422,-7.130222320556641,31.79802131652832,-33.05134582519531,12.917339324951172,-5.137050628662109,-24.607330322265625,35.0475959777832,30.98533821105957,-5.4951982498168945,-0.6320316791534424,29.902475357055664,22.044105529785156,30.711835861206055,16.096811294555664,26.90464973449707,38.22712707519531,-1.1907509565353394,22.629344940185547,35.7732048034668,-29.05480194091797,-11.271800994873047,42.62694549560547,8.159314155578613,8.473672866821289,16.58587074279785,-14.091225624084473,-9.030296325683594,23.23818588256836,26.03751564025879,4.830961227416992,53.32255554199219,-7.62325382232666,33.54146957397461,22.829233169555664,1.4877992868423462,10.70711898803711,37.46400451660156,9.168801307678223,0.7407781481742859,15.112462043762207,2.70097279548645,3.3730361461639404,32.05397415161133,44.429439544677734,19.127016067504883,-33.58549499511719,1.5588710308074951,28.97612762451172,28.690017700195312,43.59673309326172,-46.68988037109375,22.211166381835938,-47.33257293701172,47.216941833496094,15.417472839355469,-23.750350952148438,11.31205940246582,-45.088294982910156,9.483891487121582,-23.501131057739258,42.720027923583984,-27.75537109375,13.09144401550293,32.9483642578125,-8.122698783874512,-0.5165798664093018,10.738049507141113,24.749664306640625,16.398372650146484,-23.202363967895508,11.67158317565918,41.94613265991211,46.197574615478516,47.20799255371094,6.833338260650635,23.910619735717773,15.946590423583984,15.946590423583984,-10.61520004272461,34.84955596923828,-5.092776775360107,15.902300834655762,-28.883371353149414,23.577207565307617,14.067886352539062,-6.888288974761963,16.175289154052734,19.579591751098633,-38.0333251953125,21.551481246948242,6.182532787322998,15.900954246520996,-51.23767852783203,22.01005744934082,-3.7145421504974365,22.993783950805664,-3.924774408340454,-29.55230712890625,17.579753875732422,12.632551193237305,-3.8969156742095947,2.667492151260376,-32.66438674926758,11.794206619262695,3.373049259185791,25.683504104614258,12.093951225280762,-16.39408302307129,35.80738067626953,-30.783065795898438,13.084657669067383,-7.015334129333496,-16.28081512451172,-8.800497055053711,21.67669105529785,-33.288604736328125,-9.249237060546875,-17.685260772705078,-16.459163665771484,-31.64561653137207,24.622255325317383,17.864980697631836,23.024721145629883,1.3408918380737305,5.258438587188721,43.7419548034668,-48.31871032714844,16.453401565551758,-34.50790786743164,3.0012881755828857,35.10873794555664,13.504313468933105,16.233659744262695,11.728132247924805,-47.45850372314453,1.1262470483779907,3.3783745765686035,-5.150445938110352,-12.056235313415527,16.629060745239258,30.228145599365234,16.56983184814453,7.364748477935791,-5.338067054748535,23.028207778930664,-26.711875915527344,23.117595672607422,-33.178524017333984,-0.6367230415344238,8.358132362365723,-10.670845985412598,27.62320327758789,20.817720413208008,10.928032875061035,22.14531898498535,32.70518493652344,-4.6629743576049805,22.7391414642334,20.706228256225586,5.491276264190674,27.238737106323242,-20.201152801513672,-15.280462265014648,-24.927154541015625,-40.48057174682617,-26.326744079589844,2.3602285385131836,28.837203979492188,24.064563751220703,-9.160338401794434,10.767333030700684,-39.57766342163086,-12.293511390686035,22.47641372680664,-8.471237182617188,-44.79936218261719,20.278339385986328,12.927781105041504,20.319101333618164,3.0643136501312256,25.837684631347656,15.898153305053711,0.1852128505706787,-22.729873657226562,-12.155158042907715,56.363704681396484,-28.97832679748535,-44.70187759399414,-16.216217041015625,10.729781150817871,4.453708648681641,9.668466567993164,-0.7804824709892273,-23.11684799194336,19.33287239074707,-15.717044830322266,36.87002944946289,-46.008522033691406,-33.479183197021484,-34.705692291259766,-31.724409103393555,-32.230865478515625,33.119850158691406,-2.5268521308898926,6.384397506713867,52.669342041015625,8.780611991882324,41.564022064208984,-32.78435516357422,29.829174041748047,-1.9396268129348755,-24.666189193725586,-12.209175109863281,14.39940071105957,23.808168411254883,-26.8138427734375,-4.653480052947998,17.945472717285156,8.978533744812012,-12.292119026184082,17.804380416870117,28.432266235351562,50.04355239868164,3.2412760257720947,-6.071596145629883,11.519643783569336,30.93657112121582,-5.8160014152526855,31.41735076904297,-46.79940414428711,17.817432403564453,32.65087890625,-0.3458757698535919,-15.782022476196289,13.108186721801758,2.2205307483673096,37.25648498535156,0.3454391658306122,15.67220401763916,-28.361099243164062,18.982606887817383,35.54601287841797,-3.197871685028076,24.356136322021484,49.233726501464844,8.499360084533691,25.54397201538086,-25.92877960205078,2.659705400466919,-3.9981703758239746,0.7414579391479492,-23.04775047302246,-17.728981018066406,2.445038318634033,41.396934509277344,29.295690536499023,-26.671133041381836,-26.47185707092285,-31.31466293334961,-19.811847686767578,20.431779861450195,22.580923080444336,-36.498252868652344,10.019510269165039,-0.06810043007135391,-5.362333297729492,0.9261962175369263,12.460756301879883,28.95218276977539,-18.63096046447754,7.270000457763672,-17.11885643005371,-10.48231315612793,-17.73084259033203,-22.7816104888916,33.204559326171875,30.338611602783203,31.46966552734375,25.87234878540039,2.020371198654175,15.07153034210205,-32.25538635253906,16.694355010986328,24.015539169311523,9.503515243530273,40.30984878540039,33.763954162597656,27.70644187927246,18.478546142578125,-33.706966400146484,10.799206733703613,1.6478561162948608,17.50478172302246,20.066761016845703,-20.88508415222168,39.38962936401367,-17.049287796020508,31.789838790893555,50.432247161865234,20.72435188293457,-14.15806770324707,-9.356376647949219,-14.976030349731445,26.356962203979492,-34.41168975830078,-6.472993850708008,31.91065788269043,-16.046390533447266,16.300819396972656,36.369651794433594,30.941452026367188,54.08732223510742,24.309659957885742,-48.37836837768555,-1.6057289838790894,23.791946411132812,37.3868408203125,34.954010009765625,-54.59003448486328,3.587287664413452,-48.90378952026367,32.95283508300781,-18.40229034423828,16.96576499938965,6.283637046813965,12.225790977478027,-6.631871700286865,-32.70686721801758,40.79943084716797,-46.27470397949219,-2.079817771911621,41.52080154418945,-39.65935134887695,42.216285705566406,-12.931012153625488,-15.007458686828613,-21.092405319213867,45.722679138183594,14.589637756347656,10.995988845825195,-21.608985900878906,-10.558053016662598,-57.53510284423828,-22.001474380493164,0.0039564622566103935,19.12439727783203,-23.87675666809082,-17.117290496826172,0.2891004681587219,0.9469639658927917,13.855602264404297,23.71460723876953,21.550430297851562,16.881267547607422,-4.185968399047852,11.367392539978027,20.960439682006836,-9.555268287658691,14.814840316772461,-23.320505142211914,2.9113786220550537,-20.705732345581055,-30.98841094970703,22.02653694152832,7.584121227264404,19.56407356262207,-39.10648727416992,17.5814151763916,37.46400451660156,38.49199295043945,20.875938415527344,42.129722595214844,-3.407350778579712,-5.150445938110352,-2.292609214782715,25.699304580688477,30.280696868896484,2.626039981842041,47.340415954589844,-21.890424728393555,3.105476140975952,43.28402328491211,46.73991775512695,-33.62165069580078,2.01279616355896,-15.48073673248291,-42.38230895996094,-26.510133743286133,-4.235628128051758,-4.653465270996094,-9.849367141723633,-14.938295364379883,-28.046533584594727,-22.011201858520508,-12.665227890014648,-0.04782476648688316,-17.307737350463867,-3.6073453426361084,-21.503660202026367,-30.10117530822754,-30.441015243530273,35.51387405395508,23.223634719848633,10.424493789672852,15.741654396057129,37.487579345703125,21.215524673461914,-21.71232032775879,40.25138854980469,-12.578628540039062,12.27642822265625,11.033367156982422,-28.886924743652344,-7.9055609703063965,-12.387210845947266,30.320125579833984,5.5699286460876465,9.830353736877441,-5.325716018676758,50.19477844238281,39.6027946472168,-24.39443588256836,-11.938512802124023,-20.21735954284668,1.7346433401107788,11.690417289733887,54.43458557128906,-2.0209312438964844,33.44256591796875,-5.6442790031433105,18.263866424560547,-36.70075988769531,15.27263069152832,21.36762237548828,-21.407594680786133,0.31300681829452515,-7.535878658294678,22.31707191467285,32.258155822753906,-4.0368475914001465,17.342483520507812,28.284866333007812,33.49752426147461,33.550689697265625,8.425688743591309,35.534244537353516,35.37376403808594,5.797392845153809,17.98458480834961,7.224455833435059,-3.172640323638916,-8.981734275817871,-10.001496315002441,31.406347274780273,-27.179946899414062,-25.247901916503906,2.5226690769195557,7.573415756225586,33.52360153198242,-1.7986204624176025,7.502009868621826,47.696754455566406,-27.093006134033203,-27.84780502319336,3.1716949939727783,-12.044997215270996,-10.286087036132812,-46.96178436279297,8.500449180603027,15.8289155960083,-47.556427001953125,43.02720260620117,-47.59343338012695,11.483260154724121,-2.960493803024292,32.994327545166016,32.911251068115234,-2.0703985691070557,1.0212405920028687,18.736726760864258,-1.2127933502197266,16.562667846679688,33.45646286010742,-7.754125118255615,-39.55817794799805,32.04009246826172,14.9103422164917,2.899379014968872,-2.3756372928619385,-26.30648422241211,-28.58671760559082,2.177720069885254,8.029528617858887,5.8331499099731445,-17.441814422607422,29.047060012817383,7.380908012390137,-24.014751434326172,-8.163206100463867,-46.106842041015625,-6.742020606994629,7.067636489868164,-1.4932637214660645,-37.82706069946289,18.17910385131836,-38.213096618652344,25.70325469970703,37.33321762084961,-50.77010726928711,16.811126708984375,6.540876865386963,-11.077901840209961,-11.476613998413086,48.429588317871094,25.643836975097656,21.950300216674805,-10.529609680175781,-14.283509254455566,-1.5945394039154053,19.348112106323242,24.21576690673828,52.03575897216797,31.23102378845215,-5.441558837890625,-5.575438499450684,14.125240325927734,44.84077453613281,1.3392456769943237,11.825905799865723,20.389698028564453,22.701148986816406,-46.53489685058594,-8.875731468200684,22.04497718811035,48.89018630981445,48.882320404052734,2.458320379257202,-27.993562698364258,-36.30514144897461,-26.58262825012207,-21.36147117614746,34.72696304321289,15.333094596862793,-46.06215286254883,-3.727599859237671,-33.408607482910156,34.00387191772461,-39.48321533203125,16.2742977142334,15.809785842895508,46.98359680175781,10.849316596984863,34.55139923095703,21.63640785217285,10.417067527770996,-3.044628143310547,-37.042320251464844,-0.756009578704834,-38.46079635620117,-55.030128479003906,-21.980270385742188,-45.862850189208984,-23.644617080688477,-23.488557815551758,-50.91682052612305,-12.009101867675781,19.71444320678711,34.1758918762207,20.64459991455078,21.54453468322754,22.5913143157959,-20.020851135253906,33.44968795776367,-24.784677505493164,-16.518033981323242,9.640130996704102,27.611162185668945,15.615728378295898,-11.183844566345215,14.420235633850098,-26.783334732055664,-25.640127182006836,27.50535774230957,21.916044235229492,13.966201782226562,23.214534759521484,-27.72881507873535,-37.42375564575195,26.59178924560547,-52.90419006347656,46.999977111816406,0.5450419187545776,15.355428695678711,-3.889519453048706,34.72708511352539,28.9350643157959,12.951088905334473,54.335845947265625,-0.3103348910808563,-10.653099060058594,-16.0035343170166,8.788369178771973,-7.890377044677734,30.976356506347656,24.251253128051758,-16.656517028808594,52.41731643676758,-15.718594551086426,51.23945999145508,6.629776477813721,-33.49660110473633,1.8495320081710815,3.183755397796631,-47.4584846496582,-3.6652638912200928,-20.426921844482422,39.38962936401367,1.339694619178772,-50.29597091674805,-8.68242073059082,-7.807459354400635,-19.216617584228516,-39.16148376464844,22.61186408996582,0.8407983183860779,22.716279983520508,-29.597442626953125,23.980432510375977,34.43500900268555,1.9832158088684082,20.032258987426758,32.5339469909668,-25.227731704711914,-38.49958038330078,10.53118896484375,-23.008983612060547,-4.662433624267578,0.9264170527458191,-25.565134048461914,-23.364646911621094,-4.6987504959106445,-43.81828689575195,3.7025046348571777,-20.651466369628906,9.717832565307617,31.545408248901367,35.356178283691406,18.62407684326172,-28.686256408691406,0.23842313885688782,-20.226112365722656,-14.307682037353516,-14.445449829101562,16.01844596862793,29.554121017456055,24.764568328857422,12.563750267028809,14.236202239990234,16.156116485595703,-45.73958206176758,-5.700650215148926,28.792282104492188,26.272323608398438,25.110885620117188,10.688167572021484,26.152843475341797,2.032832384109497,30.865022659301758,8.55290699005127,19.949871063232422,-26.651226043701172,-19.11880111694336,24.80835723876953,14.458033561706543,-56.083160400390625,-10.054696083068848,-8.352070808410645,-1.7611656188964844,-4.894790172576904,-4.333423137664795,-22.689271926879883,31.23183822631836,36.34442138671875,-20.77546501159668,-23.954240798950195,-38.65901184082031,20.876056671142578,17.469335556030273,11.27904224395752,-6.3226704597473145,-30.026094436645508,-33.6041145324707,40.63591003417969,1.1214631795883179,-45.970611572265625,-35.08675765991211,1.317911982536316,14.7075834274292,-25.242454528808594,-24.662588119506836,35.794063568115234,33.316009521484375,-8.867955207824707,-3.6437509059906006,-24.598419189453125,-4.4862565994262695,-46.58194351196289,-9.932112693786621,19.1496524810791,46.72898483276367,-36.871700286865234,30.482948303222656,9.298150062561035,-9.439065933227539,50.049888610839844,27.725990295410156,8.791610717773438,-24.561206817626953,-25.374975204467773,-20.301382064819336,-3.1515541076660156,7.320340633392334,-54.61066436767578,25.316896438598633,-0.15092265605926514,22.12315559387207,-23.52668571472168,-3.8084311485290527,-19.256746292114258,10.468664169311523,35.07001876831055,19.139278411865234,8.532197952270508,-22.837751388549805,9.331290245056152,-18.998462677001953,28.200176239013672,26.659786224365234,9.09870719909668,-40.23610305786133,-0.63848477602005,32.6392822265625,-23.46675682067871,1.1216449737548828,18.962297439575195,-24.772043228149414,42.23554611206055,22.04476547241211,21.249500274658203,0.6879991292953491,48.386714935302734,-17.110729217529297,-21.705278396606445,20.996397018432617,34.73076629638672,29.97327995300293,9.365222930908203,22.361366271972656,3.6853902339935303,34.01929473876953,47.54146194458008,9.060863494873047,21.62356948852539,-14.989543914794922,47.90467071533203,-47.10770034790039,21.153770446777344,-37.82854461669922,-2.3055570125579834,-4.146686553955078,-15.288372993469238,30.726728439331055,2.3910911083221436,-19.224811553955078,-1.7328681945800781,2.664811134338379,24.789306640625,-22.213090896606445,-45.508056640625,33.00578689575195,-8.048355102539062,-16.66127586364746,-31.391738891601562,-43.29160690307617,21.63254165649414,-2.8565993309020996,23.67634391784668,-13.79084587097168,5.034783840179443,11.87973690032959,-9.593212127685547,-22.357559204101562,6.053867340087891,-5.832175254821777,11.114530563354492,16.12362289428711,18.030179977416992,18.933141708374023,16.247488021850586,-22.540634155273438,5.599094390869141,45.86679458618164,10.782280921936035,-17.646501541137695,-23.83677864074707,7.58608865737915,9.934731483459473,-22.731409072875977,10.975105285644531,-11.472540855407715,-53.23904037475586,10.130013465881348,14.998291969299316,-5.157876968383789,22.99827766418457,-13.9989013671875,19.66526985168457,-33.644126892089844,23.250741958618164,19.649208068847656,-27.25682830810547,17.83674430847168,8.657063484191895,-37.16025924682617,8.494234085083008,-6.126872539520264,-3.428485631942749,-37.428871154785156,-26.095518112182617,6.38251256942749,-34.36273956298828,-2.3055570125579834,-33.221824645996094,-4.167774677276611,-22.67789077758789,-40.35103988647461,-24.408592224121094,19.690702438354492,41.73408126831055,-9.513032913208008,-35.339759826660156,44.261505126953125,-15.321046829223633,-33.15497589111328,15.757047653198242,-3.926173686981201,-5.9654436111450195,-23.23480224609375,-7.980245590209961,-0.3471313714981079,-26.298404693603516,-15.693168640136719,15.912446975708008,-13.569881439208984,32.97324752807617,0.5055666565895081,-4.927975177764893,-6.673046112060547,6.0279316902160645,11.097220420837402,22.35857391357422,41.16388702392578,51.95552444458008,-9.721309661865234,14.091906547546387,46.592044830322266,-38.86557388305664,-15.11014461517334,1.0291506052017212,-16.07636833190918,42.959678649902344,32.432395935058594,26.996280670166016,0.9924350380897522,1.814835548400879,4.325693607330322,32.57845687866211,-14.147150993347168,37.963314056396484,-19.038429260253906,-10.93668270111084,14.751267433166504,-25.240703582763672,19.075754165649414,-47.48245620727539,-19.253082275390625,-24.93184471130371,5.508501052856445,11.194780349731445,18.94312286376953,1.935705304145813,5.200654029846191,31.927642822265625,4.177526473999023,27.744136810302734,-4.078791618347168,-6.409092426300049,-29.133235931396484,1.6666529178619385,0.7457602024078369,-17.05028533935547,17.059446334838867,27.235876083374023,-1.2828441858291626,30.976228713989258,-28.636327743530273,26.302780151367188,-3.0928096771240234,-20.282424926757812,47.253177642822266,-23.24404525756836,-4.121516227722168,-42.76096725463867,13.567118644714355,-35.80339050292969,-47.32169723510742,-3.9962637424468994,-21.359880447387695,17.233064651489258,21.4910888671875,12.686518669128418,0.351397305727005,-20.668378829956055,-58.455963134765625,-34.00559997558594,16.756441116333008,-7.283968448638916,19.487686157226562,18.5168399810791,-41.9920768737793,13.425368309020996,1.4951913356781006,37.07270431518555,7.755010604858398,-6.993891716003418,21.313968658447266,-7.40156888961792,-1.102512240409851,-47.29182434082031,21.039505004882812,-2.4303102493286133,-19.737680435180664,-53.69263458251953,-5.2922587394714355,-3.374208927154541,17.52561378479004,36.73107147216797,36.08599090576172,-24.76764678955078,19.183753967285156,19.727313995361328,30.74054527282715,42.633087158203125,-8.849723815917969,16.260961532592773,4.991809844970703,14.96679973602295,-13.522367477416992,21.216754913330078,52.91940689086914,-20.535554885864258,16.773929595947266,48.17693328857422,-21.21946907043457,0.9299964308738708,2.467515468597412,51.80510711669922,-22.677608489990234,54.65784454345703,-28.15836524963379,-41.2255744934082,31.32897186279297,26.38222885131836,8.052270889282227,-28.435043334960938,21.95128631591797,-17.503210067749023,-26.11650276184082,-18.557483673095703,-23.47602081298828,-4.943243503570557,-10.5891752243042,19.5574951171875,51.676937103271484,4.801087856292725,4.754332542419434,-36.5479850769043,-38.997413635253906,20.072288513183594,20.953170776367188,18.409774780273438,-5.686314582824707,32.2618522644043,34.751827239990234,5.959039688110352,9.342571258544922,24.375181198120117,33.58437728881836,-32.14823913574219,2.9938652515411377,-54.49998474121094,-5.525793552398682,-31.168636322021484,27.851573944091797,-29.351396560668945,0.2746610641479492,14.295166969299316,-5.444149971008301,45.33097839355469,45.715667724609375,-0.7209866642951965,-18.149396896362305,2.5598325729370117,21.531871795654297,-32.911041259765625,34.25663757324219,14.121672630310059,-15.6187744140625,-16.53518295288086,-19.260465621948242,25.338247299194336,20.67413330078125,-8.032941818237305,-25.042247772216797,-5.317296504974365,-6.479462623596191,23.286670684814453,30.72618293762207,-26.574382781982422,-10.601635932922363,25.850130081176758,36.77724075317383,-4.7408013343811035,-18.616252899169922,-22.360069274902344,-37.823707580566406,-0.34072721004486084,-33.493858337402344,19.34534454345703,48.3431396484375,28.12554931640625,45.24663162231445,9.684893608093262,-20.745119094848633,40.481449127197266,39.76540756225586,-17.851449966430664,33.46284484863281,-32.50837707519531,-20.313936233520508,3.432483434677124,-34.48502731323242,6.528611183166504,-22.099485397338867,-33.83974075317383,-27.44280242919922,-9.013702392578125,1.9970520734786987,-56.18918228149414,16.063236236572266,-3.583148717880249,-25.766969680786133,-21.946285247802734,-40.76165771484375,-0.607793927192688,-11.220075607299805,-17.064725875854492,-24.694997787475586,2.0070083141326904,34.156227111816406,17.885730743408203,-26.619956970214844,-8.500950813293457,6.350727081298828,-38.55686950683594,-53.9959716796875,-57.497802734375,-21.398117065429688,19.23187828063965,-9.574335098266602,-52.49562454223633,19.822824478149414,-14.515586853027344,-15.194132804870605,-49.78463363647461,-20.279949188232422,-21.94676971435547,-12.026737213134766,-30.346872329711914,51.905784606933594,18.534143447875977,8.020549774169922,-6.1226348876953125,2.8101186752319336,41.59135437011719,44.78556823730469,26.278268814086914,-32.69066619873047,-4.859764575958252,-27.06093978881836,5.108013153076172,-40.16423416137695,21.090238571166992,51.11839294433594,36.027259826660156,-2.9059598445892334,-22.89191246032715,18.835622787475586,-23.135059356689453,30.968250274658203,33.171661376953125,-2.179563522338867,36.92892837524414,-40.49614334106445,5.557997703552246,3.955254554748535,-35.450077056884766,-32.00590515136719,-29.630966186523438,-48.67585754394531,-39.71673583984375,14.335039138793945,14.79665470123291,-4.097773551940918,19.982152938842773,-16.99094581604004,11.567645072937012,12.517500877380371,-2.2629880905151367,-19.525253295898438,-18.333545684814453,-23.332698822021484,-1.9487661123275757,0.9773768186569214,12.129791259765625,19.83620262145996,12.343104362487793,-23.121034622192383,22.192338943481445,-17.836332321166992,-28.722013473510742,22.38140296936035,-26.90566635131836,-14.070500373840332,35.938602447509766,20.34469985961914,-6.217916488647461,9.71540641784668,-10.083769798278809,41.26546096801758,3.2772655487060547,22.81005859375,17.476964950561523,0.9414825439453125,17.27964210510254,12.889942169189453,11.913375854492188,-30.13951873779297,9.646305084228516,17.732290267944336,-0.6900489926338196,33.27470779418945,42.59272384643555,8.866350173950195,-7.457014560699463,17.346576690673828,13.234655380249023,32.573822021484375,34.50998306274414,32.735565185546875,-7.7738165855407715,-3.7292892932891846,9.268341064453125,32.753841400146484,-41.220306396484375,-27.763652801513672,-26.59976577758789,-24.80824851989746,2.0122649669647217,-3.9429376125335693,-7.217413902282715,-32.182151794433594,-53.4169807434082,18.571073532104492,20.483930587768555,-1.8966615200042725,-42.48548889160156,-33.75123596191406,-14.099448204040527,13.175090789794922,-48.03078842163086,34.38335418701172,1.7616074085235596,-17.25652503967285,50.516143798828125,19.195528030395508,-27.835302352905273,11.795299530029297,33.76332473754883,28.077497482299805,-36.09709167480469,-35.67512893676758,-38.36760711669922,0.4371955990791321,43.872764587402344,35.80723571777344,-12.358019828796387,-21.35457420349121,-2.443247079849243,0.9252780675888062,-40.89837646484375,18.844696044921875,2.002581834793091,-24.805471420288086,-9.622166633605957,-13.206124305725098,-23.71357536315918,-34.657440185546875,-32.556854248046875,34.13521957397461,-14.774964332580566,43.78158187866211,5.633388042449951,-1.189922571182251,36.26130294799805,-22.841259002685547,-29.557588577270508,-3.472683906555176,26.467451095581055,-55.572940826416016,51.44659423828125,36.68158721923828,-8.547313690185547,23.096393585205078,-24.72312355041504,-16.505046844482422,-8.040621757507324,20.152727127075195,-11.123878479003906,-17.732675552368164,21.285072326660156,-24.0010929107666,15.541658401489258,-5.350046157836914,19.365449905395508,9.286311149597168,18.039339065551758,35.236576080322266,-17.094348907470703,-36.913002014160156,22.0034236907959,-26.766529083251953,-12.607647895812988,-6.069893836975098,-10.733952522277832,13.948147773742676,-8.341797828674316,35.763877868652344,-23.218215942382812,-6.272315502166748,-15.005163192749023,-19.571414947509766,-17.150653839111328,-11.738699913024902,-24.13347816467285,0.8252732157707214,28.302446365356445,37.612937927246094,-27.775102615356445,0.8704483509063721,-24.319143295288086,40.14509582519531,30.85264778137207,34.05654525756836,-4.595178127288818,-43.127952575683594,37.91289138793945,-10.223104476928711,0.07748039066791534,-9.119906425476074,-38.287593841552734,-43.410484313964844,-19.625537872314453,33.238121032714844,-17.89822006225586,32.856327056884766,3.7543294429779053,22.027610778808594,-4.094693183898926,-24.670175552368164,45.209083557128906,-1.3922817707061768,18.39264678955078,30.910852432250977,15.743115425109863,-0.21906307339668274,18.671972274780273,-16.830995559692383,-22.496604919433594,-27.14176368713379,-1.4204890727996826,7.287629127502441,56.43922424316406,29.680227279663086,18.04461669921875,11.035120010375977,14.037322998046875,-31.965566635131836,34.30375289916992,46.36403274536133,44.39804458618164,17.112119674682617,0.38653260469436646,-47.588191986083984,38.90782165527344,-37.951847076416016,7.2524638175964355,-2.4671730995178223,19.805932998657227,2.9932479858398438,0.274149090051651,-21.823469161987305,2.3460121154785156,-8.967982292175293,18.651811599731445,41.851806640625,-17.398456573486328,-22.32370376586914,19.494611740112305,-8.31867504119873,34.056556701660156,4.620955467224121,1.5016798973083496,-2.1605210304260254,13.43821907043457,-12.68227767944336,24.610973358154297,-2.171854019165039,57.54779815673828,9.434233665466309,-56.225528717041016,29.47852325439453,-8.505606651306152,-45.48028564453125,20.461973190307617,40.55172348022461,0.24559122323989868,-43.26298522949219,32.09799575805664,33.3287467956543,-2.2620363235473633,1.0064078569412231,33.52357482910156,-0.449258029460907,18.801959991455078,34.34724807739258,30.95036506652832,-1.6631975173950195,44.531673431396484,-11.060547828674316,57.585479736328125,43.59046173095703,-12.81717300415039,-22.03651237487793,-29.51333999633789,-11.639204025268555,-22.827865600585938,19.411500930786133,34.43775177001953,-3.347444772720337,-36.49644470214844,-15.396141052246094,-8.842527389526367,-19.866788864135742,14.016706466674805,-24.70470428466797,-38.389461517333984,-25.87965965270996,-4.401832103729248,-48.68673324584961,30.619657516479492,-11.425665855407715,13.238189697265625,-27.19050407409668,32.54652786254883,52.94474411010742,-19.85508918762207,-25.741310119628906,-55.333168029785156,23.910614013671875,3.493472099304199,25.353275299072266,-0.3806849420070648,4.303548336029053,-12.999042510986328,-5.735816478729248,-3.5545406341552734,13.887932777404785,27.922574996948242,-28.844575881958008,-22.45708656311035,51.43135452270508,-28.651485443115234,-27.05818748474121,-37.34336853027344,-45.311161041259766,17.063430786132812,-41.00395584106445,-31.114131927490234,-22.410690307617188,-15.041330337524414,22.41851234436035,26.76784324645996,-23.99509048461914,18.13711166381836,-6.386950492858887,23.21441078186035,51.875526428222656,-14.828042984008789,56.327369689941406,19.645648956298828,-35.73921585083008,-20.229961395263672,-19.216014862060547,-35.32164764404297,-27.976051330566406,21.454683303833008,-21.086332321166992,21.899747848510742,3.7607972621917725,-16.571353912353516,-33.77806091308594,-10.516069412231445,15.318602561950684,12.30447769165039,-44.46076202392578,-46.02001190185547,-7.434565544128418,-1.229860544204712,-9.070368766784668,-16.81139373779297,40.970252990722656,-34.0858268737793,23.222747802734375,32.84550476074219,-4.4560017585754395,4.956040859222412,18.28349494934082,-23.078433990478516,-32.34224319458008,27.968042373657227,13.730949401855469,-54.76126480102539,-51.01642608642578,-40.4029541015625,-23.311330795288086,-38.525386810302734,-3.5818450450897217,-9.764365196228027,20.81816864013672,31.62430763244629,28.13883399963379,-58.37648391723633,-5.625263690948486,2.2181947231292725,0.28760817646980286,-37.4553337097168,-17.47285270690918,14.521737098693848,13.371970176696777,-39.470027923583984,5.0782318115234375,22.917278289794922,46.80018615722656,5.811488151550293,-4.482359886169434,-0.7191104888916016,-37.4691276550293,-38.657325744628906,-1.3916959762573242,2.416804075241089,-16.596790313720703,-17.942724227905273,-6.424424171447754,-2.322988748550415,-0.9548791646957397,-26.617807388305664,-3.7299609184265137,54.00847244262695,17.258087158203125,-23.25318145751953,-28.40850067138672,21.365201950073242,0.515209436416626,39.36139678955078,-13.596020698547363,-27.158111572265625,-2.9622294902801514,-52.99382400512695,-38.44313049316406,0.8783231377601624,1.690221905708313,-51.45355987548828,-15.453350067138672,9.983668327331543,-25.717323303222656,18.303007125854492,19.921142578125,16.84909439086914,1.2964462041854858,-28.200876235961914,20.881362915039062,-3.803260326385498,42.14995193481445,-45.92536163330078,-49.301368713378906,-30.66187286376953,-54.460323333740234,-2.128772497177124,-8.862533569335938,-18.625097274780273,-5.38884162902832,21.68453598022461,-31.168550491333008,-0.3993930220603943,3.626028537750244,-3.2581851482391357,10.679771423339844,-39.61436462402344,-5.547519207000732,-31.57063102722168,-9.652633666992188,12.62448501586914,3.9294769763946533,34.48169708251953,-15.995627403259277,-8.298211097717285,23.1379337310791,51.92810821533203,10.449601173400879,15.598955154418945,-58.232215881347656,31.988431930541992,-2.4949533939361572,50.7042350769043,21.31235122680664,7.404512405395508,28.696889877319336,1.5517547130584717,-9.60151481628418,1.166122555732727,-16.29349136352539,1.386274814605713,-19.954164505004883,-0.6673158407211304,-3.3733859062194824,2.5029079914093018,-38.695072174072266,16.097625732421875,-8.571479797363281,-56.047000885009766,33.85475540161133,-52.54475402832031,1.012975811958313,44.33137130737305,7.7513909339904785,46.511417388916016,29.388566970825195,-33.00524139404297,24.65289306640625,21.227535247802734,-2.207221746444702,44.9553108215332,19.932029724121094,28.762466430664062,-4.129175662994385,-29.007240295410156,29.1774959564209,41.18098068237305,-0.778407871723175,-10.436172485351562,-9.967202186584473,35.54602813720703,34.44044494628906,45.7413215637207,0.09197777509689331,-34.37737274169922,6.388134479522705,-28.321697235107422,22.130800247192383,-6.742088317871094,35.773197174072266,8.299651145935059,33.99573516845703,14.934163093566895,-10.918013572692871,-53.77108383178711,13.775579452514648,-26.741291046142578,-43.96946334838867,37.16290283203125,4.479360103607178,-21.27147102355957,8.72259521484375,-29.531400680541992,52.35176086425781,-8.450512886047363,-40.79264450073242,-22.03174591064453,-52.337318420410156,23.59383773803711,-47.840858459472656,-25.0692138671875,-23.70598602294922,12.764004707336426,-41.03651428222656,-19.22231674194336,-0.7145747542381287,6.744924068450928,0.616507351398468,-48.59056854248047,-28.282733917236328,-27.439048767089844,-26.90707015991211,-22.03174591064453,-28.443233489990234,25.73662757873535,1.9798513650894165,7.071803092956543,-28.00160789489746,51.51344680786133,-37.73281478881836,20.01827049255371,-14.452720642089844,-40.16536331176758,-47.17637634277344,-34.69444274902344,-12.063519477844238,49.81224060058594,15.67372989654541,-53.018680572509766,30.936582565307617,-32.74713134765625,-19.795391082763672,19.694509506225586,12.301197052001953,4.756867408752441,1.6922740936279297,0.415455162525177,16.701284408569336,-28.622360229492188,-2.494934320449829,11.383639335632324,-23.116893768310547,-10.35097599029541,17.635557174682617,-1.1041680574417114,39.26276779174805,31.439916610717773,-36.168006896972656,14.010102272033691,30.501293182373047,14.283299446105957,-6.413961410522461,22.800838470458984,-18.62130355834961,-34.71525955200195,-13.449548721313477,-27.3764591217041,-48.450809478759766,18.356319427490234,-17.61163902282715,15.123366355895996,7.360809803009033,40.040367126464844,11.839144706726074,6.541231155395508,31.102449417114258,-56.12596893310547,30.865022659301758,41.278263092041016,-13.391182899475098,-35.648677825927734,-28.229936599731445,-36.683170318603516,34.77900695800781,-5.559381008148193,-21.844228744506836,-16.934642791748047,0.7995077967643738,13.03902816772461,0.9665815830230713,31.80463409423828,-22.690149307250977,15.57498550415039,-16.1127872467041,-21.317895889282227,-19.987762451171875,10.474363327026367,43.08382034301758,-10.644577980041504,-19.167095184326172,-14.320171356201172,-15.411343574523926,-6.5590314865112305,-19.728076934814453,0.7641685009002686,-55.91969299316406,-19.86564064025879,21.90462875366211,20.00959587097168,24.678922653198242,17.41852378845215,-18.17647361755371,20.39293098449707,29.50584602355957,24.642396926879883,-42.871498107910156,-9.767589569091797,50.860450744628906,-11.022098541259766,-27.92344856262207,-12.503203392028809,-20.954442977905273,-48.22294235229492,35.20130157470703,1.7068628072738647,-24.22275733947754,-17.224842071533203,28.265277862548828,-9.946579933166504,5.611560344696045,3.1420414447784424,-21.031904220581055,15.728804588317871,28.933462142944336,-35.107479095458984,32.32756042480469,-49.181175231933594,-3.162405014038086,-6.447871208190918,-4.459627151489258,8.75135612487793,19.574066162109375,35.283958435058594,-6.127642631530762,34.34986877441406,-11.722698211669922,-26.4133243560791,-5.680405616760254,-0.16600443422794342,-37.12338638305664,-22.721023559570312,-24.138437271118164,-1.8291070461273193,-13.876934051513672,-9.984186172485352,-8.128009796142578,23.6201114654541,-29.57301902770996,-13.74795150756836,-33.56664276123047,-5.113966941833496,7.463833808898926,33.53545379638672,-55.23420715332031,-3.411684274673462,24.526447296142578,-0.6020020246505737,14.010102272033691,-10.931021690368652,16.732446670532227,20.228092193603516,-23.138505935668945,-10.63694953918457,-42.724571228027344,51.318084716796875,-19.580076217651367,0.22190316021442413,-56.15766525268555,6.0638556480407715,22.305706024169922,-33.97046661376953,-26.547609329223633,18.936540603637695,-56.406944274902344,-22.819244384765625,1.4027955532073975,8.81466007232666,4.992863655090332,53.060054779052734,-22.588167190551758,-37.635379791259766,-17.03154182434082,-29.112323760986328,0.41178226470947266,0.5151770710945129,3.351712703704834,-41.507110595703125,7.364743709564209,14.671271324157715,15.302902221679688,-7.7738165855407715,-18.414548873901367,11.302486419677734,-5.290203094482422,2.836768865585327,52.088993072509766,48.21437454223633,-44.53719711303711,1.7118761539459229,12.543087005615234,44.99250030517578,-26.03595542907715,-17.1191349029541,-0.8070436716079712,-4.312267780303955,-16.761932373046875,-7.039193630218506,27.05537223815918,31.643564224243164,-3.1318116188049316,16.911823272705078,-40.315826416015625,44.34906005859375,-15.25257396697998,56.71405029296875,-31.145448684692383,-30.925674438476562,34.206329345703125,-16.21034049987793,24.668527603149414,-0.5570628046989441,-17.379398345947266,-56.7750244140625,48.72388458251953,29.596513748168945,-19.335330963134766,-32.77412796020508,-29.919618606567383,17.121074676513672,-49.9622688293457,-5.7200212478637695,52.841209411621094,-23.923049926757812,34.701560974121094,36.5769157409668,28.935136795043945,-30.784006118774414,30.918346405029297,-58.93147277832031,-22.940942764282227,-30.33860206604004,4.475409507751465,-49.988014221191406,-28.29576301574707,-35.558719635009766,-13.870952606201172,-50.141136169433594,-20.40816879272461,0.5700567960739136,-23.160442352294922,-26.072555541992188,-6.636399269104004,-5.711512088775635,-26.425247192382812,-20.62030792236328,12.973885536193848,-25.01862907409668,12.380948066711426,-23.72675132751465,-8.342488288879395,11.245223045349121,-13.085785865783691,46.44223403930664,-26.32721710205078,0.08873581141233444,-1.3963910341262817,-26.761627197265625,-35.147674560546875,1.6672430038452148,24.46773910522461,-33.042667388916016,-28.7548828125,40.05184555053711,26.894636154174805,-1.240676760673523,-49.56059265136719,-48.59114074707031,-24.27216339111328,1.4372985363006592,-9.136497497558594,-2.7668137550354004,25.419591903686523,41.883602142333984,-41.32808303833008,54.845458984375,16.3395938873291,-39.90317153930664,-39.03693771362305,-17.465978622436523,-22.066078186035156,0.9007872343063354,-27.368324279785156,35.905799865722656,1.9566084146499634,-46.76047134399414,-24.39539337158203,4.969039440155029,42.18765640258789,29.441465377807617,-18.054052352905273,18.368255615234375,36.94322204589844,-40.135677337646484,9.268403053283691,-9.677215576171875,20.717472076416016,-14.465960502624512,-30.070608139038086,11.209965705871582,5.1148457527160645,9.379694938659668,35.48246383666992,-1.319316029548645,-24.39443588256836,1.0384222269058228,43.73124313354492,-0.6907248497009277,-12.60178279876709,24.212982177734375,-12.018871307373047,24.554088592529297,-40.778568267822266,-16.298559188842773,28.476688385009766,19.88373374938965,13.449703216552734,33.47037887573242,-47.50099563598633,35.244319915771484,-20.047456741333008,-26.519235610961914,9.138392448425293,0.6833401322364807,-6.688852310180664,12.586864471435547,20.520479202270508,5.468990802764893,-3.7779862880706787,-15.08168888092041,16.66158676147461,-22.584421157836914,-15.251867294311523,-32.62151336669922,-38.50886154174805,45.477542877197266,-38.104881286621094,-22.085176467895508,-12.741202354431152,8.944640159606934,-15.821855545043945,17.87065887451172,-35.72637176513672,-24.931020736694336,-2.089123487472534,-38.61954879760742,31.899272918701172,-29.86257553100586,-43.170654296875,4.6742262840271,-22.365379333496094,30.28449821472168,-11.425665855407715,-1.3882590532302856,11.245223045349121,45.243431091308594,7.866797924041748,-2.0512125492095947,23.708688735961914,-9.989386558532715,35.28125762939453,-57.77186584472656,-21.599506378173828,-15.844986915588379,-17.490373611450195,-25.891523361206055,-38.29737091064453,-9.784445762634277,-5.3074049949646,-3.2466981410980225,19.57221031188965,4.79644775390625,-35.31488037109375,-16.130889892578125,-44.495357513427734,22.875511169433594,3.2786121368408203,-4.138510227203369,22.593555450439453,42.77079391479492,-26.56827735900879,-23.875083923339844,2.2344532012939453,33.62260437011719,11.180593490600586,-49.8050422668457,-36.48676300048828,-10.568394660949707,3.670954704284668,-24.55508041381836,-3.0056240558624268,17.496267318725586,-16.642728805541992,33.212318420410156,21.305805206298828,34.2351188659668,-15.650961875915527,-51.045467376708984,-24.010177612304688,-20.437076568603516,41.063594818115234,25.98813247680664,7.079184532165527,-42.015525817871094,-17.18134307861328,-16.447927474975586,-30.473438262939453,-26.46962547302246,27.250812530517578,-17.146648406982422,-5.539724826812744,-31.485454559326172,0.17486432194709778,-16.115585327148438,-35.74406051635742,-4.662749767303467,-31.12095832824707,42.8058967590332,2.6342549324035645,-57.232784271240234,-27.6839656829834,-0.5017135739326477,-23.823883056640625,38.60023880004883,47.69462966918945,-35.07682800292969,-4.509348392486572,36.40830993652344,28.732894897460938,-21.663965225219727,-7.7057366371154785,30.50933265686035,29.195053100585938,36.15862274169922,-14.957082748413086,-19.872539520263672,-30.506179809570312,15.423470497131348,-18.9752254486084,41.617462158203125,-10.72913646697998,-3.327176570892334,-27.844070434570312,-44.437652587890625,-37.68168640136719,-32.12028503417969,-20.306921005249023,-9.697199821472168,-8.056025505065918,-37.622779846191406,23.101280212402344,-39.608970642089844,17.150304794311523,-31.362390518188477,9.845952033996582,-23.27532196044922,-15.08655834197998,-32.78435516357422,-52.265235900878906,-2.286989450454712,-1.0623449087142944,-6.995604515075684,25.832942962646484,31.325611114501953,-15.56149959564209,-15.072772026062012,-37.7593994140625,-7.689339637756348,22.347368240356445,-22.82390785217285,-55.69785690307617,-49.01255798339844,3.146111488342285,0.10770019143819809,-36.224998474121094,-24.44020652770996,-21.623308181762695,-16.28521728515625,-3.737816095352173,24.192407608032227,-2.239042043685913,-11.250397682189941,45.962379455566406,7.575706958770752,41.959285736083984,43.16063690185547,-5.9729323387146,-6.149670124053955,14.788517951965332,-0.4420676827430725,20.097944259643555,-37.53472900390625,-21.748409271240234,50.80283737182617,-53.719234466552734,-11.4998779296875,30.078964233398438,-48.37776184082031,35.46717834472656,9.727251052856445,-16.51801109313965,22.854360580444336,32.33113098144531,37.54494094848633,30.941917419433594,-40.822147369384766,54.295989990234375,-38.88675308227539,6.108879089355469,16.119606018066406,-57.25117874145508,-23.740739822387695,-24.32716941833496,-18.94424819946289,20.483943939208984,-41.069801330566406,25.200300216674805,-22.290531158447266,32.74966812133789,5.069324493408203,53.436859130859375,-5.317314147949219,-8.465868949890137,-44.234344482421875,-47.15149688720703,-17.2894229888916,3.125967264175415,-9.834013938903809,51.55105209350586,-23.65994644165039,31.49289321899414,-22.78274917602539,-16.696678161621094,-16.181230545043945,-45.54620361328125,-32.80864334106445,27.689531326293945,1.706483006477356,42.02405548095703,14.1419038772583,-8.311393737792969,-1.6902427673339844,29.016948699951172,-36.699119567871094,18.547380447387695,5.763565540313721,17.4185791015625,32.30392837524414,38.76163864135742,-32.37968063354492,-26.381929397583008,-30.99150276184082,5.856300354003906,-25.868736267089844,21.274166107177734,-3.9790878295898438,-0.5547096729278564,-8.637763023376465,-54.57056427001953,-37.25847625732422,-19.06954002380371,-7.388131141662598,-14.839029312133789,-9.33302116394043,39.493045806884766,25.921424865722656,-23.54718589782715,9.704313278198242,-21.346229553222656,7.384677886962891,19.382705688476562,-19.107030868530273,-11.500330924987793,23.31199836730957,-29.73211669921875,-27.90900230407715,-43.39714813232422,-25.91157341003418,-9.59456729888916,-8.760031700134277,-44.819759368896484,-2.200627088546753,27.642377853393555,15.283994674682617,-18.916160583496094,41.41550827026367,16.499509811401367,1.8667402267456055,-10.75786304473877,-0.6378984451293945,5.285008430480957,-22.984416961669922,6.368780612945557,1.5960618257522583,12.097284317016602,53.083858489990234,-12.311482429504395,8.913768768310547,-7.336628437042236,4.768637180328369,-27.759111404418945,16.435413360595703,10.542028427124023,-19.156246185302734,-22.011056900024414,-57.23173522949219,15.977879524230957,-14.855764389038086,-37.667449951171875,-9.383149147033691,-48.47990417480469,-40.51518249511719,-19.443620681762695,-40.70669174194336,-2.2088706493377686,1.641554832458496,-2.4582574367523193,-41.996543884277344,54.220489501953125,4.352824687957764,-16.59501838684082,-32.71683120727539,-0.6907248497009277,6.491772651672363,-12.81232738494873,-14.931803703308105,-50.092872619628906,30.534692764282227,-26.89369773864746,18.697776794433594,30.501117706298828,16.389162063598633,6.950131416320801,-6.429584503173828,-38.9744758605957,29.97327995300293,-9.55978775024414,19.32306480407715,-2.012007236480713,-6.150624752044678,-15.619180679321289,11.478517532348633,53.6673698425293,-16.029659271240234,19.414560317993164,25.148475646972656,20.48354148864746,-8.456730842590332,-36.00814437866211,-24.02509307861328,19.406932830810547,-21.54044532775879,-0.2708747386932373,11.625190734863281,-30.30780029296875,-23.705480575561523,-9.886221885681152,-43.056983947753906,14.969138145446777,-22.647510528564453,22.741987228393555,22.2280216217041,30.347755432128906,-39.02273941040039,-41.281394958496094,43.101993560791016,-35.736114501953125,-15.638991355895996,41.289424896240234,-32.9168815612793,9.419889450073242,-34.089317321777344,-21.05422019958496,6.047464370727539,-36.380950927734375,24.745046615600586,22.469261169433594,-40.439090728759766,-5.235108852386475,-6.6998515129089355,0.10889622569084167,-15.293086051940918,-35.819889068603516,-42.50587844848633,-40.58437728881836,-23.47722053527832,50.59648513793945,1.5835015773773193,-5.481541633605957,34.80414581298828,6.0496673583984375,-0.10085934400558472,19.103015899658203,-15.90929889678955,51.665042877197266,34.25905227661133,46.167991638183594,3.936270236968994,-8.086978912353516,-19.429773330688477,-38.47647476196289,11.509926795959473],\"xaxis\":\"x\",\"y\":[8.304418563842773,-18.88983917236328,-19.122488021850586,-19.656667709350586,-19.172988891601562,-21.9480037689209,-9.269472122192383,-19.758995056152344,-7.551247596740723,-18.992046356201172,-18.992046356201172,-24.533424377441406,-13.648789405822754,-22.095293045043945,-16.106517791748047,-20.034215927124023,-18.717212677001953,10.266557693481445,-21.685420989990234,5.310532569885254,-23.293649673461914,-46.94618225097656,5.310515880584717,-5.002431869506836,32.489688873291016,-4.407418727874756,-8.513262748718262,-21.868104934692383,-23.56509017944336,41.373069763183594,-23.885385513305664,56.43672180175781,7.036074638366699,30.479148864746094,9.977996826171875,-17.230499267578125,35.996700286865234,-42.786800384521484,5.665203094482422,-34.08086013793945,15.84483814239502,48.4845085144043,57.38121032714844,45.010223388671875,-28.26239013671875,36.649662017822266,16.173410415649414,11.240975379943848,0.45355117321014404,43.11996078491211,-20.377971649169922,-25.755769729614258,11.36369514465332,11.240975379943848,-18.52949333190918,0.28483137488365173,42.428955078125,-2.7031233310699463,17.858684539794922,28.269866943359375,-1.2333515882492065,36.513336181640625,-27.792522430419922,-9.12047004699707,-6.829368591308594,39.23959732055664,-14.868969917297363,37.355098724365234,0.808498740196228,-27.496353149414062,-2.8488059043884277,-10.420190811157227,10.825958251953125,-8.3916015625,-0.22311681509017944,-34.43587112426758,29.552040100097656,-1.5501337051391602,31.116260528564453,-4.40634298324585,12.405741691589355,27.523517608642578,-18.756357192993164,10.053235054016113,-3.084306240081787,7.385961532592773,-22.116548538208008,-1.7618436813354492,2.82795786857605,-23.559471130371094,-22.69072723388672,45.15406036376953,0.44760334491729736,34.075748443603516,23.07166290283203,-21.602886199951172,-20.02625274658203,-23.1265811920166,3.9615159034729004,-49.153785705566406,-2.5317955017089844,-9.475708961486816,9.61860466003418,-19.396648406982422,17.171476364135742,1.8462828397750854,28.544607162475586,51.13222122192383,3.490122079849243,23.491600036621094,40.02345275878906,-21.925735473632812,30.559059143066406,-10.685809135437012,-43.46498107910156,-10.22248363494873,17.954072952270508,43.84387969970703,0.9583931565284729,-20.77364158630371,4.732726097106934,30.239593505859375,-28.445125579833984,12.727606773376465,16.628847122192383,-0.2974480390548706,-1.5844632387161255,48.30278396606445,17.292831420898438,19.218524932861328,45.6561279296875,-23.659923553466797,20.505321502685547,25.326189041137695,43.5390739440918,20.751134872436523,4.377787113189697,23.827266693115234,4.869430065155029,1.2682729959487915,-47.61779022216797,-41.028926849365234,-21.962743759155273,-10.549139022827148,29.597890853881836,-7.443942070007324,48.100406646728516,50.11854553222656,-34.72017288208008,33.433998107910156,23.465225219726562,14.057985305786133,-34.41997528076172,-30.69464111328125,-38.647743225097656,-47.06404495239258,-21.490285873413086,-16.834501266479492,3.628755807876587,-24.14067840576172,-31.614524841308594,41.40080261230469,4.638166904449463,-37.02542495727539,5.146024703979492,-1.5357532501220703,-1.585962176322937,48.59956741333008,-31.38991928100586,-6.678531646728516,57.88795471191406,-35.116241455078125,-42.67632293701172,52.912025451660156,42.98961639404297,20.142614364624023,-17.70138168334961,58.39842987060547,4.729061126708984,-24.233684539794922,23.215654373168945,-13.201974868774414,25.703678131103516,-4.898943901062012,33.60187911987305,-6.782009124755859,53.64122009277344,-45.53403854370117,-18.929643630981445,-6.6442551612854,5.996047019958496,10.271138191223145,9.848404884338379,-6.341557502746582,-46.75844955444336,19.83478355407715,53.20089340209961,13.157194137573242,28.791217803955078,-0.7409720420837402,-47.795074462890625,5.193486213684082,32.37620544433594,38.91706085205078,13.866941452026367,-3.636107921600342,2.3434298038482666,-2.517270803451538,47.943153381347656,34.67060089111328,49.6239013671875,25.163707733154297,6.285019397735596,11.645002365112305,17.374635696411133,-5.078943252563477,2.252187490463257,14.726747512817383,14.830303192138672,4.235511779785156,16.388551712036133,-2.548523426055908,39.82149887084961,50.209564208984375,12.714066505432129,20.060487747192383,46.28569412231445,35.59736633300781,39.76089859008789,-1.3237519264221191,29.41206169128418,39.863468170166016,-42.407737731933594,-0.9050245881080627,-8.073975563049316,-34.016998291015625,-5.324136257171631,-34.21870040893555,11.624237060546875,-18.77322006225586,-30.50934410095215,17.04631233215332,35.210105895996094,36.660438537597656,-23.789234161376953,49.49126434326172,-0.13122175633907318,-24.72180938720703,27.273609161376953,-22.56477165222168,-46.0927734375,-43.324771881103516,58.68008804321289,50.890987396240234,-22.248695373535156,-12.346009254455566,4.16981315612793,-20.261423110961914,-50.0875244140625,-14.229734420776367,-40.245426177978516,-42.05803298950195,-26.04184913635254,14.642219543457031,54.697593688964844,-43.50226593017578,50.465457916259766,32.61069107055664,-16.931873321533203,31.788013458251953,-20.11867332458496,43.62083435058594,31.64272689819336,-36.444889068603516,33.56496810913086,11.428200721740723,-16.190166473388672,3.8944740295410156,9.310259819030762,13.923565864562988,-0.4010498821735382,0.5377359986305237,27.30797004699707,-30.307415008544922,-7.048408031463623,-29.336320877075195,36.143310546875,44.92790222167969,-6.644895076751709,-9.66354751586914,-16.2028751373291,-21.568283081054688,-9.230056762695312,-40.99357604980469,-48.18959426879883,-30.67937469482422,-47.23621368408203,-28.38263702392578,17.33622169494629,-40.47053909301758,-35.85795974731445,51.127716064453125,48.82680130004883,4.860538005828857,15.630463600158691,-19.154449462890625,-40.41771697998047,51.434635162353516,-0.6100654602050781,13.128192901611328,-25.541460037231445,-19.27249526977539,9.337200164794922,-15.660139083862305,-21.440866470336914,-19.52501106262207,32.56300735473633,27.39301109313965,-37.59011459350586,-0.36341071128845215,-40.48811340332031,-37.455909729003906,36.275272369384766,6.684500217437744,-48.273162841796875,9.288241386413574,40.074684143066406,56.7847900390625,28.144983291625977,-47.5238037109375,-43.28252410888672,-21.68230438232422,-11.819756507873535,31.602052688598633,-11.57710075378418,6.001593112945557,16.210803985595703,33.153133392333984,-7.899626731872559,-26.412796020507812,-41.324886322021484,-27.46308708190918,3.4702231884002686,50.064849853515625,16.25836181640625,-48.35228729248047,9.379277229309082,45.069915771484375,0.59904944896698,-23.929615020751953,-4.712867736816406,-11.666010856628418,-32.85301971435547,11.118650436401367,6.886582374572754,12.204644203186035,-8.876771926879883,-15.657851219177246,2.9992120265960693,2.9992120265960693,-25.22418975830078,-42.88093185424805,0.04782000556588173,29.01637077331543,-9.542476654052734,33.740211486816406,15.506263732910156,41.69588088989258,11.519976615905762,19.752695083618164,5.249312400817871,16.310426712036133,-1.6515976190567017,-35.661041259765625,-24.191890716552734,-7.28561544418335,-43.1839714050293,7.449931621551514,36.55428695678711,2.136411666870117,-33.66657638549805,-38.429039001464844,10.945310592651367,26.64596176147461,53.990447998046875,-1.4734808206558228,6.684498310089111,-46.73480224609375,-42.13179397583008,13.860801696777344,-22.84119415283203,49.9233283996582,-12.328543663024902,44.83660125732422,51.39407730102539,27.546588897705078,-23.68693733215332,53.73066329956055,31.068811416625977,50.305885314941406,47.67148971557617,54.462345123291016,3.896587610244751,-35.98211669921875,-22.00731658935547,4.1626811027526855,32.78231430053711,5.239943504333496,-8.629623413085938,-31.646947860717773,58.6067008972168,30.212726593017578,-18.409835815429688,-1.6654534339904785,-39.673397064208984,-32.734981536865234,-12.157448768615723,2.9047906398773193,29.187829971313477,12.160888671875,9.529363632202148,10.465017318725586,-46.71669387817383,-19.706714630126953,-5.404877185821533,21.498031616210938,-46.33634567260742,45.54363250732422,-27.500240325927734,0.6019368767738342,45.04960632324219,-39.309608459472656,31.62004852294922,-31.31730842590332,-47.42719650268555,-4.329124450683594,-49.158935546875,-15.395658493041992,30.587085723876953,-3.6395456790924072,-26.76882553100586,-6.578972339630127,4.864611625671387,-18.696666717529297,-19.96642303466797,-30.39314842224121,-27.219297409057617,-32.85334396362305,16.861879348754883,1.4515215158462524,12.361937522888184,-1.3723065853118896,9.268436431884766,2.9903929233551025,9.358979225158691,-39.850826263427734,-14.43013858795166,-10.355021476745605,-5.229050159454346,-36.825469970703125,-49.26703643798828,7.980165958404541,-44.502830505371094,-3.047856092453003,-20.2935733795166,-26.76704216003418,-24.102882385253906,2.936967372894287,-6.8470563888549805,-9.968123435974121,21.952207565307617,23.730464935302734,-17.83020782470703,-35.79670715332031,-36.432315826416016,-32.01708984375,-2.004915475845337,49.59328842163086,-28.792680740356445,-9.717292785644531,56.1895751953125,57.763431549072266,55.74235916137695,-21.634658813476562,-26.840065002441406,-20.1748104095459,-19.06858253479004,9.881650924682617,-32.7015380859375,-19.051578521728516,54.831424713134766,-37.18609619140625,11.768855094909668,47.43046569824219,44.18232727050781,15.343528747558594,-46.71574401855469,51.13416290283203,24.236513137817383,29.134441375732422,23.82037925720215,48.40946960449219,-32.89015197753906,-47.693355560302734,3.218517303466797,-40.327762603759766,22.314571380615234,24.571029663085938,-8.631038665771484,25.64755630493164,-15.901410102844238,-13.791849136352539,-2.9052505493164062,-21.384140014648438,37.98472213745117,0.8730218410491943,15.529559135437012,37.13886642456055,-22.82964515686035,-4.82187557220459,-40.88233184814453,-9.404454231262207,-3.5788629055023193,-21.265390396118164,10.939508438110352,-20.25636100769043,10.31553840637207,1.6540439128875732,-47.83950424194336,52.13953399658203,-40.99085235595703,22.311676025390625,24.076324462890625,-30.725656509399414,44.10795211791992,48.031917572021484,-31.62327003479004,-23.432464599609375,-25.20319175720215,42.382774353027344,53.09894561767578,-3.6844425201416016,-4.97476863861084,-17.53879737854004,2.766876697540283,17.67552375793457,-41.46272659301758,12.873618125915527,-6.651978015899658,9.406694412231445,17.837055206298828,-4.195220947265625,-36.344478607177734,2.9632463455200195,-3.4390997886657715,34.66908264160156,35.00041961669922,-44.00410461425781,0.40421897172927856,-37.55083465576172,-22.591203689575195,-2.317408800125122,4.507974147796631,52.72490692138672,-21.31743812561035,-26.726993560791016,-39.3341064453125,-29.991073608398438,-14.201279640197754,-27.100969314575195,-45.44001007080078,-15.24107837677002,-34.011783599853516,4.902751445770264,12.010129928588867,-6.155581951141357,44.94898223876953,-16.04621124267578,45.92565155029297,-14.040014266967773,11.603723526000977,16.166955947875977,-8.652239799499512,19.4016170501709,50.87614059448242,-48.47895050048828,58.00776672363281,47.050048828125,-26.946197509765625,24.547103881835938,33.28645324707031,-35.32146453857422,-21.813222885131836,11.08676528930664,-27.57135772705078,-18.650005340576172,16.151765823364258,30.56370735168457,-26.61699867248535,-14.996893882751465,-21.350473403930664,-8.290634155273438,-18.198143005371094,-39.23639678955078,34.419654846191406,-2.6399075984954834,-7.517304420471191,27.604793548583984,49.90110778808594,-5.746654987335205,3.49786639213562,-17.418365478515625,44.520751953125,3.4066150188446045,-27.814430236816406,-0.6156499981880188,43.12510681152344,-7.3890485763549805,-13.165985107421875,-0.8439838886260986,-37.83922576904297,16.877117156982422,42.53828811645508,10.812434196472168,-23.457571029663086,-32.94676971435547,35.276710510253906,22.199403762817383,43.46584701538086,33.95882034301758,19.255535125732422,15.249187469482422,13.780159950256348,3.4210915565490723,-45.883609771728516,-42.769657135009766,-7.6393656730651855,1.1536495685577393,16.779775619506836,10.755951881408691,-43.136592864990234,25.357160568237305,28.09876823425293,48.98948669433594,52.17833709716797,31.865020751953125,-0.8467574119567871,0.2842710614204407,-27.824390411376953,12.935017585754395,-37.59011459350586,-27.02235221862793,32.04718017578125,9.003348350524902,9.56402587890625,12.160888671875,-42.29128646850586,-17.645347595214844,-35.067745208740234,46.715030670166016,11.24728775024414,-24.796417236328125,35.28257751464844,0.23529160022735596,-1.162990689277649,53.704071044921875,43.2403678894043,41.72590637207031,-0.925216555595398,50.37598419189453,-40.796485900878906,24.236541748046875,-23.66403579711914,52.352962493896484,53.62449264526367,-18.00275421142578,48.8530158996582,28.688005447387695,48.208824157714844,-8.963577270507812,-18.795913696289062,49.938533782958984,50.89506149291992,-6.819794178009033,-40.16343688964844,-33.00035095214844,-32.5201530456543,-34.218849182128906,-5.158023834228516,-28.666292190551758,-17.267044067382812,50.438941955566406,10.960295677185059,-31.46687126159668,47.813682556152344,48.275596618652344,-4.296561241149902,-8.278751373291016,-6.02678918838501,-37.44976043701172,-18.301164627075195,4.354833126068115,-17.83944320678711,54.794254302978516,-19.42075538635254,28.599323272705078,-8.666120529174805,13.012861251831055,4.516665458679199,4.966230392456055,-35.065425872802734,21.897701263427734,-43.50802993774414,3.531766176223755,-20.53267478942871,33.408355712890625,-28.12136459350586,-8.016839981079102,-15.105697631835938,-43.18193817138672,-8.251324653625488,20.737991333007812,3.6266674995422363,-43.450496673583984,-32.792179107666016,-44.10456848144531,-4.238795757293701,-21.98578643798828,-13.215657234191895,-2.603148937225342,-44.799076080322266,-7.0730671882629395,-35.05223846435547,20.343441009521484,-19.062240600585938,-13.930088996887207,52.2781982421875,43.65424728393555,8.30441665649414,-19.2286376953125,-23.921993255615234,1.981370210647583,-20.340560913085938,-1.1961864233016968,-34.45851135253906,-26.99887466430664,48.5675163269043,-5.827858924865723,32.584991455078125,-26.009523391723633,-3.1382999420166016,-3.265373468399048,-18.26181411743164,11.730281829833984,-11.447587966918945,-37.766883850097656,-42.02387619018555,-25.47291374206543,-46.916561126708984,-21.547557830810547,6.054732322692871,-17.767480850219727,-41.52657699584961,38.35561752319336,-24.5791015625,26.896957397460938,-18.46750259399414,-8.911567687988281,-38.31285858154297,33.168304443359375,-18.989866256713867,48.14612579345703,41.10438919067383,42.95038986206055,-18.127307891845703,-22.656272888183594,36.23151397705078,-46.830055236816406,-20.09239387512207,-26.215351104736328,43.91677474975586,-11.330972671508789,11.959330558776855,-33.2149543762207,-43.16415786743164,-22.770015716552734,-3.9246835708618164,-9.774038314819336,-22.847352981567383,-21.193479537963867,-19.531349182128906,41.09733200073242,-18.28771209716797,-18.805700302124023,-6.3875837326049805,4.303421974182129,-44.91706848144531,-3.39272403717041,-5.584852695465088,-19.716297149658203,15.938698768615723,-42.29682159423828,-20.500404357910156,3.001176357269287,-29.472232818603516,14.641597747802734,31.472450256347656,-42.81413650512695,3.6634209156036377,5.451008319854736,28.56801414489746,-44.844608306884766,-47.46065139770508,-26.164020538330078,35.101531982421875,-22.573453903198242,10.971305847167969,9.279111862182617,19.1674747467041,49.811866760253906,-6.231741905212402,44.83500289916992,40.702205657958984,-16.40196418762207,14.327276229858398,-11.709986686706543,49.55494689941406,57.31745147705078,-17.6628360748291,-10.588020324707031,44.78580856323242,-4.204216003417969,-0.8247134685516357,-3.2034709453582764,-24.046205520629883,4.89699649810791,4.074860572814941,-41.429683685302734,-5.291299343109131,43.98721694946289,5.563357830047607,-22.857059478759766,20.52644157409668,-13.396995544433594,-26.609025955200195,-2.7715225219726562,-18.927780151367188,-9.414005279541016,33.788997650146484,-11.619062423706055,30.58525276184082,-41.25947570800781,-41.41375732421875,30.59039306640625,-37.973243713378906,23.880693435668945,44.76762008666992,-34.41500473022461,-43.062461853027344,-31.991287231445312,20.553735733032227,10.765751838684082,-25.45323944091797,-9.43399429321289,17.32649803161621,-44.50986099243164,9.0040864944458,-47.88782501220703,-9.448758125305176,-24.57826805114746,-35.05088424682617,-20.141626358032227,11.403685569763184,44.197425842285156,-33.64529037475586,49.30988693237305,-16.40187644958496,16.19310760498047,45.27433776855469,2.480564832687378,-5.043476581573486,-19.014049530029297,48.751007080078125,-4.4549560546875,38.45975112915039,16.325428009033203,5.474422454833984,40.51838302612305,8.395769119262695,45.07691192626953,7.415783405303955,-4.880232810974121,-5.749216079711914,12.011882781982422,31.459840774536133,-12.157417297363281,11.181129455566406,29.753131866455078,-16.04621124267578,-16.305099487304688,-24.38380241394043,8.973411560058594,-15.312061309814453,31.62832260131836,-27.488882064819336,-26.232309341430664,11.509801864624023,29.46842384338379,-24.96877098083496,-17.210079193115234,-5.31326961517334,16.619001388549805,-45.50172424316406,-29.219375610351562,-6.419857501983643,-18.58342170715332,22.859609603881836,42.84916305541992,25.94127655029297,32.71375274658203,-34.67633819580078,34.84822463989258,36.15972900390625,-26.81256103515625,-5.035282611846924,-13.44275951385498,15.914886474609375,-24.001190185546875,-16.8026123046875,3.604243278503418,-28.432092666625977,33.14802551269531,49.8895263671875,-8.331806182861328,47.44438552856445,-33.435672760009766,-47.27314376831055,33.35078430175781,-5.916738986968994,15.2445650100708,44.30038070678711,-16.072526931762695,4.283857822418213,-48.19241714477539,12.753564834594727,-5.293420791625977,-8.028176307678223,-47.96282196044922,19.80214500427246,-7.718616962432861,-0.012701386585831642,28.73200035095215,-2.982940196990967,28.719493865966797,-40.66580581665039,20.709901809692383,-23.522315979003906,-7.323879241943359,20.520177841186523,39.19935989379883,7.983335971832275,30.941564559936523,43.95336151123047,-42.73583221435547,-28.47104263305664,47.18235397338867,-2.599984884262085,2.148456335067749,30.98067283630371,-31.34982681274414,-8.16637897491455,33.50065231323242,51.98395538330078,2.1486172676086426,1.9348098039627075,28.012266159057617,-25.959348678588867,58.38518142700195,-34.919883728027344,13.391559600830078,43.060977935791016,25.583206176757812,-36.32488250732422,-33.43977737426758,31.47933006286621,7.350643634796143,42.94021987915039,14.151161193847656,-13.488419532775879,2.191094398498535,-44.38064193725586,1.127495527267456,-14.262185096740723,1.2478059530258179,20.51679039001465,21.63614273071289,3.8555281162261963,3.3678622245788574,22.42752456665039,-11.486140251159668,49.52315902709961,-27.599557876586914,4.446680545806885,-2.6916890144348145,-22.368303298950195,-45.40868377685547,-4.247875213623047,-20.50274658203125,26.32990074157715,-19.424551010131836,28.56965446472168,-8.758171081542969,-15.348420143127441,14.88221263885498,24.735132217407227,35.855552673339844,-16.52194595336914,-32.90010070800781,-1.6573125123977661,11.357852935791016,1.0469932556152344,4.705150604248047,46.37196731567383,-40.24357223510742,-19.620792388916016,37.06671905517578,-0.5109038949012756,-6.076818466186523,10.571249008178711,31.965320587158203,-7.5885910987854,-5.507086753845215,4.070278644561768,49.69086456298828,-25.36839485168457,-43.791175842285156,-13.391284942626953,-18.897388458251953,-0.544456422328949,-42.97472381591797,34.547706604003906,-0.7306903600692749,4.935927867889404,0.7571441531181335,29.93296241760254,51.813602447509766,-1.8566937446594238,-10.693925857543945,22.165416717529297,-0.46159180998802185,-26.08064842224121,18.032392501831055,44.33692169189453,-0.6012671589851379,-17.436634063720703,30.5806827545166,1.6844532489776611,23.424007415771484,32.23106384277344,-27.760963439941406,-10.919732093811035,-32.547142028808594,20.264404296875,35.19987106323242,53.93653869628906,-23.266664505004883,-1.2560056447982788,11.007086753845215,-6.534489631652832,16.499431610107422,-39.239837646484375,-36.257362365722656,35.7975959777832,22.82052993774414,-17.109901428222656,-18.252878189086914,15.237509727478027,44.43080520629883,29.007896423339844,18.279081344604492,12.350528717041016,36.40294647216797,-21.45342254638672,11.891714096069336,16.36319351196289,20.808900833129883,-18.84187889099121,-15.97826862335205,-38.87582778930664,-32.30484390258789,-2.026726245880127,-5.4435577392578125,-16.539478302001953,24.03310775756836,-35.57991409301758,-19.682580947875977,7.968213081359863,16.30447769165039,-43.30678176879883,3.6044840812683105,33.79193115234375,27.67376708984375,-26.144084930419922,37.69535827636719,-20.45844841003418,-8.77869701385498,16.380386352539062,2.406743049621582,-39.30078125,-23.223148345947266,42.571231842041016,-22.202091217041016,-20.652931213378906,-26.08064842224121,-1.9466365575790405,4.8117170333862305,-5.035299777984619,-22.91197967529297,-26.735868453979492,-4.37136697769165,-30.706867218017578,-4.502933025360107,-6.206461429595947,3.2750091552734375,-24.343385696411133,-21.775531768798828,-21.419559478759766,0.7483195662498474,25.721296310424805,-0.09248015284538269,48.315948486328125,-35.69890213012695,48.95536422729492,-19.703948974609375,15.691191673278809,-22.847152709960938,-9.860452651977539,6.6041364669799805,7.334529399871826,4.075111389160156,-20.109479904174805,18.502229690551758,12.012899398803711,1.267126202583313,4.16571569442749,-3.025224447250366,11.446558952331543,6.280145645141602,-18.09900665283203,3.463329315185547,39.82530975341797,-16.73182487487793,5.693089962005615,-29.726173400878906,18.128822326660156,-23.895709991455078,-21.039400100708008,33.664833068847656,-0.3732869029045105,39.46818161010742,-33.55696105957031,33.7686653137207,31.662080764770508,14.970353126525879,44.8387336730957,32.04804611206055,-14.04185676574707,35.431312561035156,19.897296905517578,-6.86177396774292,-5.152255058288574,38.73627471923828,23.488079071044922,30.08460235595703,-33.18489074707031,21.10374641418457,17.950456619262695,-18.73011589050293,29.198890686035156,-25.492637634277344,47.77531051635742,12.813043594360352,36.31873321533203,2.573029041290283,-1.6877164840698242,46.94412612915039,-4.172884941101074,-24.15458106994629,-43.6578369140625,-10.64653491973877,21.499404907226562,4.7072343826293945,-28.015727996826172,46.36484146118164,-26.266559600830078,26.518556594848633,-21.49820327758789,-7.652654647827148,35.33300018310547,47.370361328125,38.53232955932617,-4.5566558837890625,6.4112749099731445,44.30192947387695,-19.55390739440918,-25.28905487060547,-19.206552505493164,13.770574569702148,46.172603607177734,0.5530799031257629,13.308917999267578,-0.5659560561180115,-30.860746383666992,-20.037809371948242,0.308831125497818,-40.09164810180664,21.661218643188477,-40.532920837402344,46.92906951904297,-22.602258682250977,-26.03742218017578,-49.27507400512695,-38.09846496582031,-31.679941177368164,-21.449487686157227,47.82307434082031,13.938526153564453,-40.7913818359375,-25.175411224365234,-0.17100366950035095,-34.63478088378906,15.279111862182617,16.465072631835938,-37.46726608276367,-20.723417282104492,-16.760156631469727,-36.976505279541016,35.86819839477539,-3.417862892150879,-21.658344268798828,-41.30731201171875,2.964276075363159,-32.437347412109375,-36.20125961303711,4.493539810180664,-24.27655029296875,-6.920289039611816,-17.3680419921875,4.752066612243652,27.32181739807129,4.235631942749023,48.58085632324219,-27.60111427307129,0.12956756353378296,-16.70229148864746,-40.69781494140625,-27.37718391418457,-6.643835544586182,3.750826120376587,-8.87144947052002,-4.389624118804932,45.665931701660156,23.213685989379883,48.48256301879883,22.420148849487305,5.982535362243652,-7.441751003265381,-5.031923770904541,-12.056021690368652,0.7090356349945068,29.957658767700195,-22.994585037231445,40.21393585205078,13.273454666137695,-40.18920135498047,-7.216569900512695,-18.072397232055664,-2.3998007774353027,31.985109329223633,-14.71288013458252,-21.852405548095703,-1.1050196886062622,-23.17951774597168,3.001079797744751,-6.448947906494141,2.962587594985962,-24.613662719726562,-25.287656784057617,-37.209556579589844,39.82872009277344,3.5919604301452637,-2.868055820465088,27.0406436920166,38.14213180541992,-20.90165138244629,-20.72568130493164,1.7508257627487183,-7.306128978729248,26.05390739440918,19.82952117919922,23.274234771728516,37.209190368652344,-25.27425193786621,-48.25233840942383,47.447601318359375,42.06236267089844,20.4000301361084,31.679622650146484,-40.78126525878906,-30.204599380493164,-2.4438908100128174,17.60360336303711,12.588431358337402,-23.98135757446289,48.880252838134766,-19.356189727783203,26.26632308959961,-23.287397384643555,30.58818817138672,-22.25176429748535,-7.576565742492676,5.124238967895508,4.267802715301514,11.194507598876953,15.063989639282227,20.440195083618164,-16.692920684814453,-15.503947257995605,37.421836853027344,-28.920766830444336,-0.9026109576225281,42.69703674316406,35.54901885986328,3.5246191024780273,-36.42859649658203,-0.6041679978370667,56.93740463256836,31.1475772857666,33.6340446472168,-20.688573837280273,-24.33769989013672,-39.079994201660156,-11.447824478149414,-35.06196594238281,-18.453277587890625,-17.655517578125,-19.560344696044922,32.111106872558594,41.340858459472656,48.86830520629883,5.329766273498535,-2.688873767852783,-45.02672576904297,-9.329440116882324,45.25053787231445,-35.75542068481445,5.946974754333496,-24.06842803955078,-22.696548461914062,39.22743606567383,-40.16666793823242,35.37278366088867,-17.50414276123047,30.45178985595703,-21.118032455444336,-23.163171768188477,-24.882375717163086,49.58788299560547,46.08979797363281,8.398127555847168,2.9303689002990723,2.5603363513946533,-6.8394880294799805,-18.208768844604492,-17.258922576904297,-19.123056411743164,-18.339757919311523,1.4603211879730225,-48.18204879760742,-7.264303684234619,5.038784027099609,49.34550094604492,-9.770142555236816,-23.84356689453125,-50.08533477783203,3.233035087585449,-36.347171783447266,39.22661209106445,19.290363311767578,-43.77162551879883,20.890403747558594,17.41446876525879,-8.785528182983398,31.305143356323242,-10.493409156799316,-29.508298873901367,-8.65272331237793,-16.675241470336914,59.628414154052734,55.80705261230469,2.9376943111419678,-12.012398719787598,-27.508975982666016,43.4687614440918,42.89879608154297,22.679100036621094,-38.02798080444336,14.955141067504883,9.746221542358398,17.127758026123047,-36.46718215942383,22.024642944335938,21.006175994873047,-24.32984161376953,-35.65286636352539,23.229265213012695,30.1315975189209,-45.33137512207031,-2.417095899581909,-18.4974308013916,7.698764324188232,-4.601524829864502,48.81273651123047,-24.179195404052734,-2.415799617767334,50.043582916259766,-22.448894500732422,17.13861656188965,10.095965385437012,-8.716520309448242,43.71715545654297,-16.1762752532959,-0.9793863892555237,-23.254179000854492,14.335214614868164,38.55327606201172,-19.626976013183594,-31.061986923217773,28.870145797729492,-21.904991149902344,-33.40189743041992,28.483402252197266,-24.10191535949707,-42.66082763671875,9.596004486083984,-41.70920181274414,11.378321647644043,-20.52297592163086,-41.49943923950195,-13.698075294494629,-28.220197677612305,-25.671987533569336,8.534929275512695,27.67873191833496,-20.256059646606445,-14.441275596618652,-10.225802421569824,41.0659065246582,31.161300659179688,44.527442932128906,-18.638164520263672,8.777129173278809,47.654293060302734,1.5761446952819824,-22.588224411010742,18.20922088623047,22.43430519104004,-19.873720169067383,-11.927345275878906,-13.47628402709961,-18.951017379760742,-0.5228186249732971,-9.092930793762207,-37.980445861816406,10.991841316223145,37.51906967163086,12.067903518676758,-18.675535202026367,31.714128494262695,45.07865905761719,-38.14236831665039,-0.7353660464286804,-4.52286434173584,-5.565367698669434,6.457066059112549,-24.16533660888672,-20.162572860717773,-24.976688385009766,-8.33464241027832,34.3156852722168,2.2819018363952637,-6.8429460525512695,-0.8194048404693604,29.838842391967773,38.727149963378906,-33.75352096557617,-17.516830444335938,-20.552976608276367,-33.39631652832031,-6.562745094299316,-21.11788558959961,-13.644293785095215,52.07944107055664,-20.233915328979492,-8.07028865814209,-33.13900375366211,-24.613649368286133,-13.028363227844238,0.7382904291152954,27.339881896972656,10.773512840270996,-20.13920783996582,12.329233169555664,-36.07518768310547,-0.29983335733413696,-27.684722900390625,22.788631439208984,21.876665115356445,39.64043045043945,-21.796546936035156,-22.943832397460938,15.136734008789062,34.121376037597656,-31.1574649810791,12.821854591369629,49.33391571044922,-36.46885299682617,20.652101516723633,14.673619270324707,-0.37149837613105774,19.418302536010742,3.9883944988250732,-46.11326599121094,42.946292877197266,43.377445220947266,12.301462173461914,36.859344482421875,43.588157653808594,27.889047622680664,-6.332644462585449,32.794090270996094,32.09607696533203,18.792686462402344,30.369735717773438,33.913597106933594,1.7318165302276611,-5.544380187988281,37.2016487121582,-44.18303298950195,-37.65193557739258,52.665771484375,43.75482940673828,33.74209213256836,2.572730302810669,15.798315048217773,-26.582763671875,23.276187896728516,-22.173519134521484,-29.24209213256836,-18.578582763671875,17.415267944335938,47.783180236816406,-0.2943418323993683,-11.19221305847168,48.19194412231445,-19.306671142578125,50.98801040649414,-12.70031452178955,0.01353371236473322,-42.53963088989258,20.479183197021484,27.406536102294922,0.10813494771718979,-25.162229537963867,11.459183692932129,0.14373426139354706,-33.52912139892578,45.71010208129883,19.81829833984375,1.8524165153503418,-25.462514877319336,32.542396545410156,-19.6721248626709,20.518428802490234,3.54231333732605,17.723215103149414,-35.74802017211914,15.431638717651367,-2.877831220626831,54.867950439453125,-13.16404914855957,-1.8393305540084839,5.039461135864258,-34.566593170166016,-5.47118616104126,-17.69563865661621,-23.559471130371094,-22.636226654052734,-36.88084030151367,-43.33439254760742,-39.92177200317383,28.074811935424805,-41.352413177490234,-25.892446517944336,39.28178024291992,1.7807073593139648,1.912150502204895,1.1323763132095337,37.871334075927734,-1.225540280342102,38.49262619018555,4.001224517822266,-26.582752227783203,-1.2942883968353271,49.2645263671875,18.33785057067871,-43.66763687133789,-6.4641828536987305,-40.66720199584961,7.966819763183594,3.148123264312744,-32.41227722167969,-19.65349006652832,-0.3654962480068207,30.332378387451172,-17.573007583618164,-42.02695083618164,-18.51382827758789,15.636268615722656,-9.535765647888184,-38.001834869384766,-23.534900665283203,-26.253034591674805,-16.16816520690918,-23.921995162963867,-19.966842651367188,12.790079116821289,-38.569950103759766,-21.247682571411133,-41.017093658447266,10.897863388061523,-0.05571009963750839,3.285430908203125,10.159223556518555,-9.967507362365723,-4.811972618103027,52.75831604003906,34.7784538269043,-2.203193187713623,16.36781120300293,-2.733717441558838,3.105586528778076,-9.227310180664062,-21.496187210083008,3.644649028778076,-33.3857536315918,-3.8132569789886475,-11.570138931274414,-11.09827709197998,33.3770751953125,-41.20987319946289,-19.335050582885742,15.313179969787598,5.269412994384766,-36.86057662963867,31.52362823486328,-11.842082977294922,4.092309951782227,39.863468170166016,-25.984573364257812,-23.835695266723633,-15.657840728759766,29.170265197753906,12.126171112060547,18.8973445892334,30.901596069335938,-7.29263973236084,28.793672561645508,36.31354522705078,-30.812702178955078,-26.77671241760254,48.442893981933594,-25.982505798339844,7.008821964263916,51.96983337402344,-35.169673919677734,2.7364108562469482,-25.65479850769043,3.2660162448883057,-0.5371538400650024,53.33299255371094,-12.078726768493652,21.92537498474121,5.452030658721924,3.077819347381592,26.875530242919922,38.522361755371094,46.5811767578125,3.2735486030578613,3.5815978050231934,-23.19856071472168,2.8580453395843506,23.331680297851562,-4.207146644592285,31.9758243560791,37.321109771728516,59.409542083740234,-7.46372127532959,-15.608078956604004,-12.496339797973633,-19.531103134155273,21.574024200439453,19.94118309020996,-21.36240577697754,35.468048095703125,42.30309295654297,30.184219360351562,-12.309867858886719,-12.70030403137207,-14.381277084350586,37.966983795166016,0.6452748775482178,34.55601119995117,-30.33447265625,-21.00016212463379,30.111722946166992,-19.402929306030273,-18.31588363647461,-17.712921142578125,2.648979902267456,-0.9198141694068909,-19.936447143554688,-0.7906316518783569,-31.630416870117188,-23.532392501831055,-23.116634368896484,-18.869970321655273,-32.26841735839844,-1.387034296989441,8.459578514099121,-19.917951583862305,-4.861443519592285,15.740408897399902,16.960887908935547,-24.43950653076172,-5.896897792816162,-39.48384094238281,-38.11913299560547,-10.733360290527344,2.600255250930786,43.18478775024414,9.877163887023926,-28.955503463745117,-39.624900817871094,-6.839276313781738,10.89405345916748,-2.476982593536377,48.60321044921875,28.975780487060547,-14.342707633972168,-12.722557067871094,-36.818241119384766,42.771697998046875,40.310733795166016,14.983474731445312,38.919498443603516,-43.960418701171875,-35.975990295410156,-8.791427612304688,5.01595401763916,11.151369094848633,33.21819305419922,24.45170783996582,32.10700988769531,28.246997833251953,18.651334762573242,-16.669010162353516,11.912227630615234,35.602134704589844,0.8315943479537964,-17.425308227539062,-5.724920272827148,18.55290412902832,43.19298553466797,-15.636017799377441,-25.7838191986084,24.93830680847168,-26.581371307373047,33.121421813964844,38.0040283203125,-0.971977174282074,10.876542091369629,-7.335826396942139,0.7089251279830933,-41.611934661865234,8.041337013244629,-16.621084213256836,-19.971649169921875,2.6045308113098145,-17.193748474121094,-41.369258880615234,0.14279557764530182,39.338130950927734,-17.462867736816406,-17.288490295410156,1.750353455543518,15.587552070617676,30.755163192749023,1.1455327272415161,3.215240478515625,-10.513940811157227,11.609532356262207,-20.864805221557617,-24.17822265625,-0.7229098677635193,33.459171295166016,-29.590396881103516,50.607078552246094,45.1678352355957,2.416245698928833,7.265347957611084,25.959056854248047,11.196141242980957,-24.88807487487793,-37.94304275512695,-26.86216163635254,9.880695343017578,17.081180572509766,20.72255516052246,7.764247894287109,8.812111854553223,-1.995271921157837,25.17401885986328,36.731117248535156,22.148345947265625,-32.9465217590332,46.12668991088867,-13.744619369506836,19.69365119934082,-21.70383644104004,-20.543231964111328,31.268728256225586,-23.04844856262207,-12.161100387573242,-16.819917678833008,25.18981170654297,2.53251051902771,-35.116241455078125,-0.6953332424163818,-36.57570266723633,-22.316699981689453,5.21982479095459,-17.118661880493164,-41.2217903137207,-0.8653870224952698,-39.17356491088867,11.883748054504395,4.4511566162109375,54.558406829833984,17.198728561401367,10.02214241027832,-21.15911865234375,9.53013801574707,-23.969449996948242,-21.265363693237305,-15.084684371948242,12.467290878295898,6.903999328613281,3.481139898300171,23.491575241088867,40.464359283447266,-48.51007080078125,11.959421157836914,-35.85779571533203,-37.081382751464844,-27.83095359802246,-34.60056686401367,-5.280329704284668,-16.757036209106445,9.237750053405762,51.530853271484375,-22.12819480895996,-18.728322982788086,-40.57317352294922,-33.78581237792969,20.844327926635742,7.105764389038086,6.628110885620117,11.432312965393066,0.09856972843408585,28.594526290893555,-15.95827579498291,-16.528303146362305,-8.20468807220459,23.473737716674805,-32.97932052612305,-31.012500762939453,-11.232187271118164,34.53453826904297,38.745201110839844,33.824825286865234,44.67741012573242,-10.288021087646484,47.43070983886719,33.4732666015625,29.884153366088867,28.594526290893555,50.00012969970703,12.11205005645752,24.141098022460938,-35.325923919677734,-1.7389417886734009,11.347983360290527,-8.752251625061035,1.3608872890472412,-20.23721694946289,-19.346012115478516,-18.546098709106445,-4.578766822814941,-8.762935638427734,12.729849815368652,38.33854293823242,-17.04706382751465,-8.631033897399902,-15.035340309143066,-18.07213592529297,17.384485244750977,15.118947982788086,-37.46139144897461,-7.808475494384766,-40.00091552734375,-37.54788589477539,54.392608642578125,1.2124013900756836,-9.018342971801758,-26.843446731567383,27.909040451049805,-4.069321155548096,-32.595157623291016,-14.961532592773438,-23.487316131591797,-12.944900512695312,16.462345123291016,-22.917856216430664,-32.01433181762695,-16.34038734436035,-48.37449645996094,-17.597551345825195,-19.26862144470215,17.462244033813477,44.10540008544922,-12.430249214172363,31.835603713989258,2.376225233078003,9.997479438781738,2.2069387435913086,-18.145341873168945,18.1319637298584,-7.5815324783325195,1.6836785078048706,-20.300397872924805,-7.718616962432861,-31.335002899169922,-6.233850479125977,-2.5446763038635254,40.38036346435547,-7.469277858734131,-42.98423385620117,-5.898279190063477,-31.134418487548828,45.638153076171875,-3.9341542720794678,15.467757225036621,4.581671714782715,-48.68720626831055,19.77720832824707,-36.0150260925293,-21.289051055908203,-1.55000638961792,38.14813995361328,-33.01093292236328,-20.278343200683594,0.4838912785053253,-31.852827072143555,14.614055633544922,-26.70415687561035,46.15842056274414,44.47205352783203,34.101402282714844,-22.50885581970215,31.950407028198242,5.1675214767456055,31.84844970703125,7.489771366119385,-2.2764482498168945,-19.21693229675293,-21.34192657470703,-8.460411071777344,-42.957183837890625,-12.372078895568848,16.818729400634766,2.256251096725464,43.66849136352539,48.59312057495117,22.37577247619629,38.23738098144531,-16.32204246520996,-34.23145294189453,-41.32340621948242,-33.49693298339844,-19.8554744720459,7.397304058074951,34.910770416259766,-37.42411422729492,20.08841323852539,50.1229133605957,39.466758728027344,-45.831172943115234,-7.665419578552246,-38.71373748779297,-16.036222457885742,18.628076553344727,-14.367680549621582,7.060255527496338,-1.6473429203033447,-3.536776542663574,-14.699461936950684,47.80885696411133,-34.172401428222656,49.64604568481445,-34.422882080078125,-14.183609962463379,38.64961624145508,-13.957372665405273,33.181251525878906,44.222320556640625,45.57969284057617,39.418025970458984,-3.674509286880493,41.48136520385742,30.993938446044922,-9.567221641540527,43.26759719848633,-21.844043731689453,-6.460285186767578,-33.50693893432617,-43.74989318847656,-19.83466911315918,-39.95587158203125,-50.130619049072266,-40.14791488647461,16.462345123291016,-6.01324462890625,-40.601318359375,38.31227111816406,-33.50822830200195,-18.097257614135742,-27.181522369384766,3.152596950531006,-3.5751469135284424,-24.46446418762207,-21.460556030273438,23.54416847229004,34.82231903076172,57.40034866333008,-32.96944046020508,16.01397705078125,-23.77997398376465,20.9984073638916,-6.497159004211426,3.117460012435913,34.07157897949219,5.019174098968506,27.699647903442383,0.28613513708114624,-5.997731685638428,-28.239585876464844,45.020912170410156,18.6513614654541,20.30498504638672,-25.474885940551758,-5.404863357543945,13.971789360046387,-38.732730865478516,8.534929275512695,-3.0322399139404297,-7.821567058563232,0.0679946094751358,-19.50583267211914,10.92764663696289,2.7123138904571533,-26.65472984313965,-7.699069976806641,-41.71391677856445,6.315147399902344,-2.1336984634399414,-3.5624313354492188,-23.962247848510742,23.759828567504883,45.10848617553711,-17.973142623901367,11.75261402130127,-24.370214462280273,10.358747482299805,39.87154006958008,-10.454708099365234,0.094272680580616,-21.872072219848633,3.4340932369232178,51.724117279052734,-21.299842834472656,-16.690519332885742,21.819875717163086,-50.30099868774414,-37.41367721557617,51.07495880126953,-25.214231491088867,9.725319862365723,-45.8955078125,-20.101318359375,52.45136260986328,1.2393426895141602,0.20837214589118958,-20.25752067565918,30.36373519897461,10.755756378173828,-32.25223159790039,-28.1513671875,-28.290695190429688,16.193078994750977,5.882427215576172,-3.4454715251922607,-25.256919860839844,-4.917654991149902,50.557071685791016,-6.822436809539795,-16.999792098999023,-27.355215072631836,-13.959031105041504,-8.805432319641113,-17.723560333251953,20.38005256652832,27.121328353881836,21.322465896606445,42.93831253051758,-14.06138801574707,7.279338359832764,-26.476146697998047,-18.286334991455078,24.861875534057617,20.022769927978516,25.135467529296875,-30.676538467407227,20.305828094482422,25.834762573242188,16.855398178100586,10.592941284179688,31.9204044342041,20.212263107299805,20.81618881225586,34.303016662597656,-7.9089226722717285,25.910356521606445,33.4984016418457,51.74231719970703,2.0681686401367188,-16.6192569732666,3.4620473384857178,16.96876335144043,-19.091909408569336,-8.611756324768066,-16.933242797851562,-39.2037239074707,36.12095642089844,-10.079704284667969,-42.84919357299805,11.12519645690918,-0.14700376987457275,5.401970386505127,-6.759235382080078,1.6450892686843872,3.145031213760376,-17.921754837036133,42.2642822265625,3.0264832973480225,34.77787399291992,-0.7511036992073059,47.89064025878906,-10.732109069824219,-35.29230880737305,48.44889831542969,9.827807426452637,-8.661477088928223,-17.936147689819336,23.869550704956055,-25.93526268005371,-9.523270606994629,-20.25602149963379,-22.126998901367188,-18.530160903930664,5.355012893676758,6.261960983276367,-33.72267532348633,-2.8961269855499268,15.567870140075684,-7.060490131378174,-33.02549743652344,54.794254302978516,27.589929580688477,4.085659980773926,-20.575132369995117,16.173437118530273,11.114643096923828,-22.521839141845703,-17.55887222290039,-25.27858543395996,46.30134201049805,16.619903564453125,22.760787963867188,-32.45067596435547,-20.142166137695312,-15.976584434509277,-6.769525527954102,47.48252487182617,-26.455535888671875,-3.209284543991089,-38.644248962402344,-17.476551055908203,29.27703094482422,-26.682565689086914,-9.994956016540527,-34.8450813293457,-6.962553977966309,37.27235412597656,-28.437458038330078,-20.69200325012207,-1.0746551752090454,-9.094941139221191,-0.6674339175224304,-23.431827545166016,-32.71855545043945,-10.53436279296875,-33.221771240234375,-18.45417594909668,43.290985107421875,2.432452917098999,-34.817222595214844,-36.96257781982422,0.19329644739627838,-41.16487121582031,-22.17848777770996,-18.35200309753418,29.053956985473633,-27.319547653198242,-22.05719757080078,5.269412994384766,31.91026496887207,25.834762573242188,6.81142520904541,1.7293932437896729,-40.38938903808594,8.155566215515137,27.676546096801758,-11.924749374389648,-24.069778442382812,-4.768787384033203,3.9556639194488525,-3.689704656600952,-2.9888765811920166,-7.013772964477539,17.94212532043457,-41.51634979248047,-43.66360855102539,-17.19221305847168,-22.126283645629883,57.88795471191406,47.123451232910156,-11.90272331237793,11.422118186950684,-38.833473205566406,-40.46598815917969,-40.325992584228516,4.047267436981201,44.98569869995117,-12.652746200561523,-40.40530776977539,-3.343437671661377,24.345996856689453,-24.545602798461914,-12.377178192138672,41.957889556884766,48.28748321533203,-32.50046157836914,3.0761337280273438,39.18478012084961,-18.85297203063965,-32.51860046386719,35.385414123535156,-39.40410232543945,46.30892562866211,-23.989500045776367,-12.104052543640137,31.27568817138672,-19.064199447631836,13.09848403930664,-3.003432512283325,-19.15146255493164,20.079875946044922,4.489730358123779,6.543020248413086,31.585447311401367,10.167313575744629,42.80071258544922,46.71780014038086,-19.78110694885254,-6.61459493637085,20.493061065673828,-5.14976692199707,6.075296401977539,-21.344480514526367,-21.288557052612305,39.181278228759766,-24.567535400390625,-27.968338012695312,-24.729158401489258,-3.7648675441741943,-27.986892700195312,10.374185562133789,-4.1669769287109375,4.069156169891357,-27.385278701782227,-36.585262298583984,-27.575191497802734,40.49335861206055,1.3879327774047852,17.617115020751953,-12.48455810546875,14.567832946777344,-33.03319549560547,-20.917163848876953,0.5149955153465271,-32.86145782470703,10.533914566040039,4.288780212402344,7.148905277252197,51.89084243774414,-23.590913772583008,-4.216664791107178,-19.131484985351562,47.55226516723633,38.16166305541992,49.6875,6.448065280914307,4.701462268829346,-23.006711959838867,-31.942262649536133,-22.183982849121094,-41.91964340209961,-33.03713607788086,13.714300155639648,54.831424713134766,-23.835636138916016,-25.34164047241211,-41.07411575317383,8.539908409118652,5.656552314758301,-46.85462951660156,16.843170166015625,-27.31366539001465,-4.802820205688477,41.911380767822266,-24.726654052734375,-25.3344783782959,-21.075056076049805,-24.946773529052734,47.85647201538086,32.71259307861328,-12.520191192626953,24.1285457611084,-11.73039722442627,-16.2791805267334,29.83036994934082,-41.386966705322266,-34.805755615234375,20.61833381652832,0.1918834000825882,-38.98648452758789,5.203535556793213,3.9461252689361572,-14.030013084411621,12.520535469055176,-31.71849822998047,-18.01852798461914,-43.09168243408203,-11.375378608703613,-12.257877349853516,4.2499260902404785,-21.979740142822266,10.481876373291016,-35.890933990478516,-14.160894393920898,-29.410871505737305,-35.06866455078125,44.76762008666992,34.326416015625,-10.071048736572266,-18.05352020263672,-29.833330154418945,-23.69013214111328,4.981128692626953,-0.21454361081123352,-37.376548767089844,13.366193771362305,-24.208208084106445,-11.236949920654297,24.560312271118164,39.37489318847656,22.43427085876465,-19.208486557006836,-45.6531982421875,-3.760497570037842,-38.26728057861328,48.466705322265625,1.7535020112991333,20.400035858154297,41.869140625,-22.951961517333984,-17.30795669555664,51.238826751708984,35.5621337890625,16.613969802856445,11.419898986816406,-18.529935836791992,-46.16954040527344,27.87896728515625,3.5574631690979004,-16.589784622192383,-10.36279582977295,-5.333701133728027,-26.826091766357422,-38.56117248535156,5.006563186645508,-43.29379653930664,48.48968505859375,2.6811752319335938,16.40497589111328,-4.199682235717773,29.904895782470703,-36.56106185913086,29.607839584350586,-39.19697189331055,-27.789222717285156,-6.317228317260742,33.95125198364258,54.86626052856445,48.438880920410156,34.184967041015625,-8.862228393554688,-9.23665714263916,20.795297622680664,0.014203989878296852,-20.075313568115234,5.0146098136901855,20.43016242980957,49.4739990234375,-24.331317901611328,44.31020736694336,-17.61763572692871,11.387008666992188,41.12834548950195,16.921356201171875,32.22669219970703,21.195621490478516,-44.73491668701172,42.41100311279297,49.89888381958008,5.34976863861084,1.672807216644287,33.481929779052734,-18.09433364868164,-6.636972427368164,34.030635833740234,18.64208221435547,-26.421939849853516,20.122541427612305,12.16340446472168,-39.43381881713867,3.876877784729004,-31.620773315429688,-38.402748107910156,36.782257080078125,42.31338119506836,39.441890716552734,-7.5750732421875,25.329641342163086,-6.106878757476807,17.230722427368164,-39.32550048828125,2.540997266769409,20.219741821289062,21.708690643310547,43.15544509887695,35.104488372802734,-26.26024055480957,-2.3070201873779297,-3.6767003536224365,-3.5984909534454346,22.405576705932617,-23.854040145874023,-21.18207359313965,-19.84317970275879,4.4116902351379395,-19.963659286499023,-18.796104431152344,-23.281997680664062,40.08901596069336,-23.073543548583984,19.677135467529297,-17.66274642944336,0.417804479598999,-25.608509063720703,4.732577800750732,-9.627158164978027,14.19607162475586,3.169062376022339,-20.575132369995117,-19.8569278717041,11.057154655456543,15.09573745727539,-19.613330841064453,-34.45117950439453,-2.0847315788269043,39.599029541015625,-7.761519432067871,-0.5209321975708008,-20.544950485229492,39.97208023071289,-28.051116943359375,-18.897388458251953,11.214325904846191,19.345800399780273,-10.584660530090332,38.61085891723633,4.782830715179443,23.53680419921875,8.33580207824707,0.38802120089530945,27.88322639465332,-41.60388946533203,15.371498107910156,34.76115417480469,-2.356050968170166,-10.458959579467773,32.52461242675781,21.933246612548828,30.185453414916992,-0.7798293232917786,5.604739665985107,43.55326461791992,31.857349395751953,-26.44618034362793,42.9826545715332,-25.078676223754883,5.176232814788818,-47.05950927734375,0.31434768438339233,-19.963401794433594,-26.146459579467773,-20.663639068603516,-20.1300106048584,-18.644956588745117,1.220604658126831,-7.530839920043945,-15.718677520751953,4.15322208404541,44.30930709838867,34.10586166381836,2.897214651107788,32.98689270019531,-5.3276896476745605,-24.861278533935547,-0.1253981590270996,-14.903879165649414,-24.91352081298828,-7.688129425048828,-2.440488338470459,-11.41202449798584,-0.12896963953971863,22.974578857421875,3.353614091873169,4.410778522491455,-42.80348205566406,-27.058427810668945,-7.011229515075684,48.454959869384766,-17.81185531616211,25.429014205932617,7.479752540588379,-0.9765710830688477,11.710119247436523,-17.74318504333496,-18.04832649230957,-18.99686622619629,-28.551191329956055,0.6814442873001099],\"yaxis\":\"y\",\"type\":\"scattergl\",\"textfont\":{\"size\":2}}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"color\"}},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b5760feb-f65c-4ed1-8267-6651e8fa49a4');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig = px.scatter(x=embed_tsne[:,0], y=embed_tsne[:,1], text=triggers, color=clustering.labels_)\n",
        "fig.update_traces(textfont_size=2)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmzzvA7lL7rp"
      },
      "source": [
        "Судя по всему, в красном кластере оказались эмоции, в желтом - физические действия, а в синем состояния."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0sN7KFPpAs1"
      },
      "source": [
        "## Часть 2. [3 балла] Извлечение именованных сущностей\n",
        "1. Обучите стандартную модель для извлечения именованных сущностей, CNN-BiLSTM-CRF, для извлечения именованных *низкоуровневых именованных сущностей*, т.е. для самых коротких из вложенных сущностей.\n",
        "Модель устроена так: сверточная сеть на символах + эмбеддинги слов + двунаправленная LSTM сеть (модель последовательности) + CRF (глобальная нормализация).\n",
        "2. Замените часть модели на символах и словах (CNN + эмбеддинги словах) на ELMo и / или BERT. Должна получиться модель ELMo / BERT + BiLSTM + CRF.\n",
        "3. Замените модель последовательности (BiLSTM) на другой слой, например, на Transformer. Должна получиться модель CNN  + Transformer + CRF.\n",
        "\n",
        "[бонус] Дообучите BERT для извлечения именованных сущностей.\n",
        "\n",
        "[бонус] Используйте модель для извлечения вложенных именованных сущностей [Ju et al., 2018]\n",
        "\n",
        "[бонус] Модифицируйте модель для извлечения вложенных именованных сущностей [Ju et al., 2018]: вместо эмбеддингов слов используйте ELMo и/или BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhBodnhN7XHy"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c78tHehLJMx"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from typing import Union, Tuple, Dict\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import f1_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GF_206vaFobW",
        "outputId": "c1073a8a-dacc-4c31-9df7-a99803e20b29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'litbank' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/dbamman/litbank.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kmoiN_yF6XH"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcImG6X2pBLH"
      },
      "outputs": [],
      "source": [
        "root = '/content/litbank/entities/tsv'\n",
        "files = sorted(list(os.walk(root))[0][-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaMh4SPdGDkr"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def train_dev_test_split(data, train_percent = 80, dev_percent = 10, test_percent = 10):\n",
        "  random.shuffle(data)\n",
        "  train_size = int(len(data) * train_percent / 100)\n",
        "  train_data = data[:train_size]\n",
        "  dev_size = int(len(data) * dev_percent / 100)\n",
        "  dev_data = data[train_size:train_size+dev_size]\n",
        "  test_size = int(len(data) * test_percent / 100)\n",
        "  test_data = data[train_size+dev_size:]\n",
        "  return train_data, dev_data, test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60xAIUSeFlms"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from string import punctuation\n",
        "\n",
        "def prepare_sents(root, files, verbose=True):\n",
        "    sents, labels = [], []\n",
        "    for f in tqdm(files, disable=not verbose):\n",
        "        path = os.path.join(root, f)\n",
        "        with open(path) as file:\n",
        "            cont = file.readlines()\n",
        "        sent_temp = []\n",
        "        labels_temp = []\n",
        "        for s in cont:\n",
        "            if s == '\\n':\n",
        "                if sent_temp:\n",
        "                    sents.append(sent_temp)\n",
        "                    labels.append(labels_temp)\n",
        "                sent_temp = []\n",
        "                labels_temp = []\n",
        "            else:\n",
        "                s = s.split('\\t')\n",
        "                sent_temp.append(s[0])\n",
        "                labels_temp.append(s[1])\n",
        "        if sent_temp:\n",
        "            sents.append(sent_temp)\n",
        "            labels.append(labels_temp)\n",
        "    return sents, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYhPtSMSHBNV",
        "outputId": "a4470b6a-052d-4b1a-acb6-ea0c8e7970fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 80/80 [00:00<00:00, 179.11it/s]\n",
            "100%|██████████| 10/10 [00:00<00:00, 521.41it/s]\n",
            "100%|██████████| 10/10 [00:00<00:00, 454.03it/s]\n"
          ]
        }
      ],
      "source": [
        "root = '/content/litbank/entities/tsv'\n",
        "files = sorted(list(os.walk(root))[0][-1])\n",
        "train_data, dev_data, test_data = train_dev_test_split(files, 80, 10, 10)\n",
        "train_books = [os.path.join(root, file) for file in train_data]\n",
        "dev_books = [os.path.join(root, file) for file in dev_data]\n",
        "test_books = [os.path.join(root, file) for file in test_data]\n",
        "\n",
        "train_sents, train_labels = prepare_sents(root, train_books)\n",
        "dev_sents, dev_labels = prepare_sents(root, dev_books)\n",
        "test_sents, test_labels = prepare_sents(root, test_books)\n",
        "\n",
        "df_train = pd.DataFrame(zip(train_sents, train_labels), columns=['sentence', 'labels'])\n",
        "df_dev = pd.DataFrame(zip(dev_sents, dev_labels), columns=['sentence', 'labels'])\n",
        "df_test = pd.DataFrame(zip(test_sents, test_labels), columns=['sentence', 'labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-X4ejWGoHOrC"
      },
      "outputs": [],
      "source": [
        "df_train['chars'] = df_train['sentence'].apply(lambda x: [list(y) for y in x])\n",
        "df_dev['chars'] = df_dev['sentence'].apply(lambda x: [list(y) for y in x])\n",
        "df_test['chars'] = df_test['sentence'].apply(lambda x: [list(y) for y in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jya8-zm8J8pt"
      },
      "outputs": [],
      "source": [
        "def build_token_dict(tokens: List, special_tokens: List) -> Tuple[Dict]:\n",
        "    \"\"\"\n",
        "    Build a dictionary for tokens.\n",
        "\n",
        "    Args:\n",
        "    - tokens: A list of lists of tokens.\n",
        "    - special_tokens: A list of special tokens.\n",
        "\n",
        "    Returns:\n",
        "    - token2idx: A dictionary mapping tokens to indices.\n",
        "    - idx2token: A list of tokens sorted by indices.\n",
        "    \"\"\"\n",
        "    token2idx = defaultdict(lambda: 0)\n",
        "    idx2token = []\n",
        "\n",
        "    for index, stoken in enumerate(special_tokens):\n",
        "        token2idx[stoken] = index\n",
        "\n",
        "    unique_tokens = set()\n",
        "    for item in tokens:\n",
        "        for token in item:\n",
        "            if token not in special_tokens:\n",
        "                unique_tokens.add(token)\n",
        "\n",
        "    for index, utoken in enumerate(unique_tokens, len(special_tokens)):\n",
        "        token2idx[utoken] = index\n",
        "\n",
        "    sorted_dict = sorted(token2idx.items(), key=lambda x: x[1])\n",
        "\n",
        "    for elem in sorted_dict:\n",
        "        idx2token.append(elem[0])\n",
        "\n",
        "    return token2idx, idx2token\n",
        "\n",
        "\n",
        "def create_embedding_matrix(word_vecs, token2idx: Dict, emb_size: int = 300, special_ids: Tuple[int] = (0, 1)) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Create an embedding matrix from word vectors.\n",
        "\n",
        "    Args:\n",
        "    - word_vecs: Word vectors.\n",
        "    - token2idx: Token to index mapping.\n",
        "    - emb_size: Size of the embeddings.\n",
        "    - special_ids: Special token ids.\n",
        "\n",
        "    Returns:\n",
        "    - emb_matrix: Embeddings matrix.\n",
        "    \"\"\"\n",
        "    emb_matrix = np.zeros((len(token2idx), emb_size), dtype=\"float32\")\n",
        "\n",
        "    emb_matrix[special_ids[0]] = np.zeros(emb_size, dtype='float32')\n",
        "    emb_matrix[special_ids[1]] = np.random.uniform(-0.25, 0.25, emb_size)\n",
        "\n",
        "    for token, id in token2idx.items():\n",
        "        if id not in special_ids:\n",
        "            if token in word_vecs:\n",
        "                emb_matrix[id] = word_vecs[token]\n",
        "            else:\n",
        "                emb_matrix[id] = np.random.uniform(-0.25, 0.25, emb_size)\n",
        "    return emb_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgIOeRzcL9V5"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f09fgFkJLzIV"
      },
      "outputs": [],
      "source": [
        "def label_prediction(model, iterator, TAG_PAD_IDX=0, exclude_pad=True, device='cpu'):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    tokens, labels, pred_labels = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            predictions, _ = model(batch['tokens'].to(device), batch['chars'].to(device), batch['mask'].to(device), batch['labels'].to(device))\n",
        "\n",
        "            for sent_texts, sent_tags_true, preds in zip(batch['tokens'], batch['labels'], predictions):\n",
        "                sent_tags_true = sent_tags_true.cpu().numpy()\n",
        "                sent_texts = sent_texts.cpu().numpy()\n",
        "                preds = preds.cpu().numpy()\n",
        "                if exclude_pad:\n",
        "                    args = np.where(sent_texts != TAG_PAD_IDX)[0]\n",
        "\n",
        "                    tokens.append(list(sent_texts[args]))\n",
        "                    labels.append(list(sent_tags_true[args]))\n",
        "                    pred_labels.append(list(preds[args]))\n",
        "\n",
        "    return tokens, labels, pred_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15vmth4FN_ag"
      },
      "outputs": [],
      "source": [
        "def trainer(model, train_dataloader, valid_dataloader, optimizer, idx2tag, epochs=5, pad_idx=0, clip=1):\n",
        "\n",
        "    best_acc = 0\n",
        "    best_loss = 1000\n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "\n",
        "        classes = []\n",
        "        predicted_classes = []\n",
        "        epoch_acc_train = 0\n",
        "        epoch_loss_train = 0\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for batch in tqdm(train_dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            predictions, loss = model(batch['tokens'].to(device), batch['chars'].to(device), batch['mask'].to(device), batch['labels'].to(device))\n",
        "\n",
        "            train_label = batch['labels'].to(device).view(-1).cpu()\n",
        "            predictions = predictions.view(-1).cpu()\n",
        "            texts = batch['tokens'].to(device).view(-1).cpu()\n",
        "\n",
        "            true_pred = ((train_label == predictions) & (texts != pad_idx)).sum()\n",
        "            num_pred = (texts != pad_idx).sum()\n",
        "            acc = true_pred / num_pred\n",
        "            epoch_acc_train += acc\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)\n",
        "            optimizer.step()\n",
        "            epoch_loss_train += loss.item()\n",
        "\n",
        "        train_accuracy = epoch_acc_train / len(train_dataloader)\n",
        "        train_loss = epoch_loss_train / len(train_dataloader)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        epoch_acc_valid = 0\n",
        "        epoch_loss_valid = 0\n",
        "\n",
        "        for batch in valid_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            predictions, loss = model(batch['tokens'].to(device), batch['chars'].to(device), batch['mask'].to(device), batch['labels'].to(device))\n",
        "\n",
        "            epoch_loss_valid += loss.item()\n",
        "\n",
        "            valid_label = batch['labels'].to(device).view(-1).cpu()\n",
        "            predictions = predictions.view(-1).cpu()\n",
        "            texts = batch['tokens'].to(device).view(-1).cpu()\n",
        "\n",
        "            true_pred = ((valid_label == predictions) & (texts != pad_idx)).sum()\n",
        "            num_pred = (texts != pad_idx).sum()\n",
        "            acc = true_pred / num_pred\n",
        "            epoch_acc_valid += acc\n",
        "\n",
        "            predicted_classes.extend(predictions.cpu().numpy())\n",
        "            classes.extend(valid_label.cpu().numpy())\n",
        "\n",
        "        valid_accuracy = epoch_acc_valid / len(valid_dataloader)\n",
        "        valid_loss = epoch_loss_valid / len(valid_dataloader)\n",
        "\n",
        "        print(\n",
        "            f'Epochs: {epoch_num + 1} | Accuracy: {valid_accuracy: .3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "563vsRJtPfTS"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kKGB3W6jVS8"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self,\n",
        "               df: pd.DataFrame,\n",
        "               token2id: Dict,\n",
        "               label2id: Dict,\n",
        "               char2id: Dict,\n",
        "               text_col: str='context_processed',\n",
        "               char_col: str='characters',\n",
        "               label_col: str ='spell',\n",
        "               pad_to: int = 124,\n",
        "               pad_to_char: int = 30,\n",
        "               pad_value: int = 0,\n",
        "               unk_value: int = 1,\n",
        "               pad_left: bool = False,\n",
        "               pad_left_char: bool = False,\n",
        "               is_char=True\n",
        "               ) -> None:\n",
        "\n",
        "    self.tokens = df[text_col].tolist()\n",
        "    self.chars = df[char_col].tolist()\n",
        "    self.labels = df[label_col].tolist()\n",
        "\n",
        "    self.label2id = label2id\n",
        "    self.token2id = token2id\n",
        "    self.char2id = char2id\n",
        "\n",
        "    self.pad_to = pad_to\n",
        "    self.pad_to_char = pad_to_char\n",
        "    self.pad_left_char = pad_left_char\n",
        "    self.pad_value = pad_value\n",
        "    self.pad_left = pad_left\n",
        "    self.unk_value = unk_value\n",
        "    self.is_char = is_char\n",
        "\n",
        "  def __getitem__(self, ix: int) -> Dict:\n",
        "\n",
        "    \"\"\"\n",
        "    :param ix: index of object\n",
        "    :return object prepared for training\n",
        "    \"\"\"\n",
        "    if self.is_char:\n",
        "      tokens, chars, tags = self.tokens[ix], self.chars[ix], self.labels[ix]\n",
        "      tokens, chars, tags = self.transform(data=tokens, kind='token'), self.transform(data=chars, kind='char', is_char=True), self.transform(data=tags, kind='tag')\n",
        "      mask = [0 if token == self.pad_value else 1 for token in tokens]\n",
        "      mask = torch.tensor(mask, dtype=torch.bool)\n",
        "      res = {'tokens': tokens, 'chars': chars, 'labels': tags, 'mask': mask}\n",
        "    else:\n",
        "\n",
        "      tokens, tags = self.tokens[ix], self.labels[ix]\n",
        "      tokens, tags = self.transform(data=tokens), self.transform(data=tags)\n",
        "      res = {'tokens': tokens, 'labels': tags}\n",
        "    return res\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.tokens)\n",
        "\n",
        "\n",
        "  def transform(self,  data: List, kind: str = 'token', is_char: bool = False):\n",
        "\n",
        "    \"\"\"\n",
        "    add padding to data\n",
        "    :param data: data to process\n",
        "    :param is_char: whether add chars\n",
        "    :return tensor prepared for training\n",
        "    \"\"\"\n",
        "    if kind == 'token':\n",
        "      mapping = self.token2id\n",
        "    elif kind == 'char':\n",
        "      mapping = self.char2id\n",
        "    else:\n",
        "      mapping = self.label2id\n",
        "\n",
        "    if not is_char:\n",
        "      if self.pad_to is not None and len(data) != self.pad_to:\n",
        "        if len(data) > self.pad_to:\n",
        "          data = [mapping[i] if i in mapping else self.unk_value for i in data[:self.pad_to]]\n",
        "        else:\n",
        "          n_pads = self.pad_to - len(data)\n",
        "          if kind == 'token':\n",
        "            data = [self.pad_value]*n_pads*self.pad_left + [mapping[i] if i in mapping else self.unk_value for i in data] + [self.pad_value]*n_pads*(not self.pad_left)\n",
        "          else:\n",
        "            data = [mapping['O']]*n_pads*self.pad_left + [mapping[i] if i in mapping else self.unk_value for i in data] + [mapping['O']]*n_pads*(not self.pad_left)\n",
        "\n",
        "\n",
        "      try:\n",
        "        return torch.tensor(data, dtype=torch.int)\n",
        "      except:\n",
        "        print(data)\n",
        "\n",
        "    else:\n",
        "      chars_embeddings = []\n",
        "      for word in data:\n",
        "        if self.pad_to_char is not None and len(word) != self.pad_to_char:\n",
        "          if len(word) > self.pad_to_char:\n",
        "            word = [mapping[i] if i in mapping else self.unk_value for i in word[:self.pad_to_char]]\n",
        "          else:\n",
        "            n_pads = self.pad_to_char - len(word)\n",
        "            word = [self.pad_value]*n_pads*self.pad_left + [mapping[i] if i in mapping else self.unk_value for i in word] + [self.pad_value]*n_pads*(not self.pad_left_char)\n",
        "        else:\n",
        "          word = [mapping[i] if i in mapping else self.unk_value for i in word]\n",
        "        chars_embeddings.append(word)\n",
        "      if self.pad_to is not None and len(chars_embeddings) != self.pad_to:\n",
        "        if len(chars_embeddings) > self.pad_to:\n",
        "          chars_embeddings = chars_embeddings[:self.pad_to]\n",
        "        else:\n",
        "          n_pads = self.pad_to - len(chars_embeddings)\n",
        "          chars_embeddings = [[self.pad_value]*self.pad_to_char]*n_pads*self.pad_left + chars_embeddings + [[self.pad_value]*self.pad_to_char]*n_pads*(not self.pad_left)\n",
        "\n",
        "      try:\n",
        "        return torch.tensor(chars_embeddings, dtype=torch.int)\n",
        "      except:\n",
        "        print(chars_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXooWy5sQDM0"
      },
      "outputs": [],
      "source": [
        "from collections import Counter, defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGuoifpLh5A0"
      },
      "outputs": [],
      "source": [
        "token2idx, idx2token = build_token_dict(df_train['sentence'].tolist() + df_dev['sentence'].tolist(), [ '<PAD>', '<UNK>'])\n",
        "tag2idx, idx2tag = build_token_dict(df_train[\"labels\"], [])\n",
        "char2idx, idx2char = build_token_dict(df_train['sentence'].tolist() + df_dev['sentence'].tolist(), [ '<PAD>', '<UNK>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evnIUw_8EwQy"
      },
      "outputs": [],
      "source": [
        "dataset_train = CustomDataset(df=df_train, token2id=token2idx, label2id=tag2idx, char2id=char2idx, text_col='sentence', label_col='labels', char_col='chars')\n",
        "dataset_dev = CustomDataset(df=df_dev, token2id=token2idx, label2id=tag2idx, char2id=char2idx, text_col='sentence', label_col='labels', char_col='chars')\n",
        "dataset_test = CustomDataset(df=df_test, token2id=token2idx, label2id=tag2idx, char2id=char2idx, text_col='sentence', label_col='labels', char_col='chars')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnc1-AkzVln8"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "train_datloader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dev_dataloader = DataLoader(dataset_dev, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_dataloader = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppcUmJ6x-w_u",
        "outputId": "8cd77442-a592-489f-f5f8-7d5475c1d937"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-11-19 19:48:10--  https://www.dropbox.com/s/699kgut7hdb5tg9/GoogleNews-vectors-negative300.bin.gz?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/dl/699kgut7hdb5tg9/GoogleNews-vectors-negative300.bin.gz [following]\n",
            "--2023-11-19 19:48:10--  https://www.dropbox.com/s/dl/699kgut7hdb5tg9/GoogleNews-vectors-negative300.bin.gz\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucf34ee5355752cda12c4539d8d8.dl.dropboxusercontent.com/cd/0/get/CH2XSWywfn1RTKOje6S9rvI2RqIM56YR-cJTtnqqXxa5UIJ8fCSwMEok5-uM2l5yeoGFgp-gGmaG7WLeHpo6DPJS2zcROBsXrmiJ3MpZntZQ-oJTInszStonoHh8KVTlq6w/file?dl=1# [following]\n",
            "--2023-11-19 19:48:10--  https://ucf34ee5355752cda12c4539d8d8.dl.dropboxusercontent.com/cd/0/get/CH2XSWywfn1RTKOje6S9rvI2RqIM56YR-cJTtnqqXxa5UIJ8fCSwMEok5-uM2l5yeoGFgp-gGmaG7WLeHpo6DPJS2zcROBsXrmiJ3MpZntZQ-oJTInszStonoHh8KVTlq6w/file?dl=1\n",
            "Resolving ucf34ee5355752cda12c4539d8d8.dl.dropboxusercontent.com (ucf34ee5355752cda12c4539d8d8.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to ucf34ee5355752cda12c4539d8d8.dl.dropboxusercontent.com (ucf34ee5355752cda12c4539d8d8.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1743563840 (1.6G) [application/binary]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz?dl=1’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.62G  82.0MB/s    in 25s     \n",
            "\n",
            "2023-11-19 19:48:36 (66.2 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz?dl=1’ saved [1743563840/1743563840]\n",
            "\n",
            "gzip: GoogleNews-vectors-negative300.bin already exists; do you wish to overwrite (y or n)? n\n",
            "\tnot overwritten\n"
          ]
        }
      ],
      "source": [
        "! wget https://www.dropbox.com/s/699kgut7hdb5tg9/GoogleNews-vectors-negative300.bin.gz?dl=1\n",
        "! mv 'GoogleNews-vectors-negative300.bin.gz?dl=1' GoogleNews-vectors-negative300.bin.gz\n",
        "! gunzip GoogleNews-vectors-negative300.bin.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COy7KSiuQVbc"
      },
      "outputs": [],
      "source": [
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vkb6pcnqXS1-"
      },
      "outputs": [],
      "source": [
        "w2v = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
        "w2v_embeddings = create_embedding_matrix(w2v, token2idx, emb_size = 300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87y1cJHpQ_BA",
        "outputId": "f9d56519-58f8-4c35-b986-b15319990324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-crf in /usr/local/lib/python3.10/dist-packages (0.7.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install pytorch-crf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzk45UveQ_-S"
      },
      "outputs": [],
      "source": [
        "from torchcrf import CRF\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2o9F2LS3h4vz"
      },
      "outputs": [],
      "source": [
        "class CNNBiLSTMCRF(nn.Module):\n",
        "    def __init__(self, word2id, char2id, idx2tag, num_classes, device,\n",
        "                 word_embedding_dim=300, char_embedding_dim=20, num_filters=100,\n",
        "                 hidden_dim=200, num_layers=2, filter_size=3, drop_out=0.5,\n",
        "                 pad_idx=0, pretrained_embedding=None, freeze_embedding=False):\n",
        "\n",
        "        super(CNNBiLSTMCRF, self).__init__()\n",
        "\n",
        "        self.device = device\n",
        "        self.pad_idx = pad_idx\n",
        "        self.idx2tag = idx2tag\n",
        "\n",
        "        self.word_embedding = nn.Embedding(len(word2id), word_embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        if type(pretrained_embedding) == np.ndarray:\n",
        "            self.word_embedding.weight.data.copy_(torch.from_numpy(pretrained_embedding))\n",
        "            self.word_embedding.weight.requires_grad = True\n",
        "\n",
        "        self.embedding_dim = char_embedding_dim\n",
        "        self.char_embedding = nn.Embedding(len(char2id), char_embedding_dim)\n",
        "        self.char_embedding.weight.requires_grad = True\n",
        "        self.cnn = nn.Conv3d(in_channels=1, out_channels=num_filters, kernel_size=(1, filter_size, char_embedding_dim))\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn_input_dim = word_embedding_dim + num_filters\n",
        "        self.lstm = nn.LSTM(self.rnn_input_dim, hidden_dim // 2, num_layers,\n",
        "                            bidirectional=True, batch_first=True, dropout=drop_out)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "        self.crf = CRF(num_classes, batch_first=True)\n",
        "\n",
        "    def forward(self, words_to_id, chars_to_id, mask, label=None):\n",
        "        word_embedding = self.word_embedding(words_to_id.to(self.device))\n",
        "\n",
        "        max_len, max_len_char = chars_to_id.size(1), chars_to_id.size(2)\n",
        "        inputs = chars_to_id.view(-1, max_len * max_len_char)\n",
        "        input_embed = self.char_embedding(inputs)\n",
        "        input_embed = input_embed.view(-1, 1, max_len, max_len_char, self.embedding_dim)\n",
        "        conv_output = self.cnn(input_embed)\n",
        "        pool_output = torch.squeeze(torch.max(conv_output, -2)[0])\n",
        "        char_embedding = pool_output.transpose(-2, -1).contiguous()\n",
        "\n",
        "        embedding = torch.cat([word_embedding, char_embedding], 2)\n",
        "\n",
        "        self.lstm.flatten_parameters()\n",
        "        out, (h, c) = self.lstm(embedding)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        if label is not None:\n",
        "            loss = -self.crf(out, label, mask, reduction='mean')\n",
        "        else:\n",
        "            loss = None\n",
        "\n",
        "        predicted_id = self.crf.decode(out, mask)\n",
        "        seq_len = label.shape[-1]\n",
        "        for i, j in enumerate(predicted_id):\n",
        "            predicted_id[i] += [0] * (seq_len - mask[i, :].sum())\n",
        "        predicted_id = torch.tensor(predicted_id, dtype=torch.int)\n",
        "\n",
        "        return predicted_id, loss\n",
        "\n",
        "    def compute_all(self, batch):\n",
        "        words_to_id = batch['tokens']\n",
        "        chars_to_id = batch['chars']\n",
        "        label = batch['labels']\n",
        "        mask = batch['mask']\n",
        "        out = self.forward(words_to_id, chars_to_id)\n",
        "        loss = -self.crf(out, label, mask, reduction='mean')\n",
        "\n",
        "        seq_len = label.shape[-1]\n",
        "        predicted_id = self.crf.decode(out, mask)\n",
        "\n",
        "        for i, j in enumerate(predicted_id):\n",
        "            predicted_id[i] += [0] * (seq_len - mask[i, :].sum())\n",
        "        predicted_id = torch.tensor(predicted_id, dtype=torch.int).to(self.device)\n",
        "\n",
        "        true_pred = ((label == predicted_id) & (label != self.pad_idx)).sum()\n",
        "        num_pred = (label != self.pad_idx).sum()\n",
        "        acc = (true_pred / num_pred).cpu()\n",
        "        metrics = dict(acc=acc)\n",
        "\n",
        "        return loss, metrics\n",
        "\n",
        "    def decode(self, words_to_id, chars_to_id, mask):\n",
        "        out = self.forward(words_to_id, chars_to_id)\n",
        "        predicted_id = self.crf.decode(out, mask)\n",
        "        return predicted_id\n",
        "\n",
        "    def _eval(self, valid_dataloader):\n",
        "        self.eval()\n",
        "\n",
        "        true_labels = []\n",
        "        predict_labels = []\n",
        "\n",
        "        for batch in valid_dataloader:\n",
        "            batch = {k: v.to(self.device) for k, v in batch.items()}\n",
        "            words_to_id = batch['tokens'].to(self.device)\n",
        "            chars_to_id = batch['chars'].to(self.device)\n",
        "            label = batch['labels'].to(self.device)\n",
        "            mask = batch['mask'].to(self.device)\n",
        "\n",
        "            out = self.decode(words_to_id, chars_to_id, mask)\n",
        "\n",
        "            for out_sentence, label_sentence in zip(out, label.tolist()):\n",
        "                for predict_label, true_label in zip(out_sentence, label_sentence):\n",
        "                    true_labels.append(self.idx2tag[true_label])\n",
        "                    predict_labels.append(self.idx2tag[predict_label])\n",
        "        report = classification_report(true_labels, predict_labels, output_dict=True)\n",
        "        return report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxdEzNh3s_qa",
        "outputId": "569d88ef-2ab7-4601-ae57-5c9f502502b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CNNBiLSTMCRF(\n",
              "  (word_embedding): Embedding(16634, 300, padding_idx=0)\n",
              "  (char_embedding): Embedding(16634, 20)\n",
              "  (cnn): Conv3d(1, 100, kernel_size=(1, 3, 20), stride=(1, 1, 1))\n",
              "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=13, bias=True)\n",
              "  (crf): CRF(num_tags=13)\n",
              ")"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "N_CLASSES = len(tag2idx)\n",
        "model = CNNBiLSTMCRF(token2idx,\n",
        "                                  char2idx,\n",
        "                                  idx2tag,\n",
        "                                  N_CLASSES,\n",
        "                                  device=device,\n",
        "                                  hidden_dim=512,\n",
        "                                  num_layers=2,\n",
        "                                  drop_out=0.2,\n",
        "                                  pretrained_embedding=w2v_embeddings\n",
        "                                  )\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean = 0, std = 0.1)\n",
        "\n",
        "model.apply(init_weights)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-mBJ-FBSEaJ"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCKty9gx5xtO"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV9qbQcZ5xv1",
        "outputId": "11c762b0-435d-456d-f0b0-af42a7d1d673"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 218/218 [12:12<00:00,  3.36s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 1 | Accuracy:  0.899\n"
          ]
        }
      ],
      "source": [
        "trainer(model,  train_datloader, dev_dataloader, optimizer, idx2tag, epochs=10, pad_idx=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftZ5Db0CGqsl"
      },
      "outputs": [],
      "source": [
        "tokens, labels, pred_labels = label_prediction(model, test_dataloader, TAG_PAD_IDX=0, exclude_pad=True)\n",
        "flat_labels = [el for subset in labels for el in subset]\n",
        "flat_preds = [el for subset in pred_labels for el in subset]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjxHO164GqyI",
        "outputId": "bb7e5275-bba6-4efe-d4d6-8bbdfe3fd429"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           O       0.91      0.97      0.94     17577\n",
            "       B-VEH       0.00      0.00      0.00        21\n",
            "       I-FAC       0.60      0.25      0.36       362\n",
            "       B-FAC       0.48      0.34      0.40       221\n",
            "       I-VEH       0.00      0.00      0.00        25\n",
            "       B-ORG       0.00      0.00      0.00        15\n",
            "       I-PER       0.60      0.39      0.47      1189\n",
            "       I-ORG       0.00      0.00      0.00        28\n",
            "       I-GPE       0.75      0.06      0.11        53\n",
            "       B-LOC       0.92      0.13      0.22        94\n",
            "       B-PER       0.62      0.57      0.59       866\n",
            "       B-GPE       0.80      0.05      0.10        79\n",
            "       I-LOC       0.82      0.12      0.20       120\n",
            "\n",
            "    accuracy                           0.89     20650\n",
            "   macro avg       0.50      0.22      0.26     20650\n",
            "weighted avg       0.87      0.89      0.87     20650\n",
            "\n"
          ]
        }
      ],
      "source": [
        "true_labels = [idx2tag[l] for l in flat_labels]\n",
        "predict_labels = [idx2tag[l] for l in flat_preds]\n",
        "labels = [i for i in idx2tag if i not in ('<PAD>', '<UNK>')]\n",
        "print(classification_report(true_labels, predict_labels, labels=labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P7z-clNjXCR"
      },
      "source": [
        "**Bert + BiLSTMCRF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5r2EbW4jfcD"
      },
      "outputs": [],
      "source": [
        "class CustomBertDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, label2id: Dict, text_col: str='context_processed', label_col: str='spell', max_seq_len=124):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label2id = label2id\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.processed_data = self._process_data(df, text_col, label_col)\n",
        "\n",
        "    def _process_data(self, df, text_col, label_col):\n",
        "        processed_data = []\n",
        "        for text, label in zip(df[text_col].tolist(), df[label_col].tolist()):\n",
        "            processed_text, processed_label = self._process_sample(text, label)\n",
        "            processed_data.append((processed_text, processed_label))\n",
        "        return processed_data\n",
        "\n",
        "    def _process_sample(self, text, labels):\n",
        "        tmp_input_ids = self.tokenizer.convert_tokens_to_ids([\"[CLS]\"] + text + [\"[SEP]\"])[:self.max_seq_len]\n",
        "        attention_mask = [1] * len(tmp_input_ids)\n",
        "        input_ids = tmp_input_ids + [0] * (self.max_seq_len - len(tmp_input_ids))\n",
        "        attention_mask = attention_mask + [0] * (self.max_seq_len - len(tmp_input_ids))\n",
        "        labels = [self.label2id[label] for label in labels]\n",
        "        labels = [0] + labels + [0] + [0] * (self.max_seq_len - len(tmp_input_ids))\n",
        "        labels = labels[:self.max_seq_len]\n",
        "\n",
        "        processed_text = {\n",
        "            \"input_ids\": torch.tensor(input_ids),\n",
        "            \"attention_mask\": torch.tensor(attention_mask),\n",
        "        }\n",
        "        processed_label = torch.tensor(labels)\n",
        "        return processed_text, processed_label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.processed_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.processed_data[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gbnVQoC2v1R"
      },
      "outputs": [],
      "source": [
        "tag2idx = {tag: idx for idx, tag in enumerate(set(tag for sublist in df_train.labels.tolist() for tag in sublist), 1)}\n",
        "tag2idx['<PAD>'] = 0\n",
        "idx2tag = [tag for _, tag in sorted(tag2idx.items(), key=lambda x: x[1])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMSLPSvrk2G-"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizer\n",
        "from transformers import DistilBertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ziAXjgkHAbx"
      },
      "outputs": [],
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "bert = DistilBertModel.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNkB4dRhUOE-"
      },
      "outputs": [],
      "source": [
        "dataset_train = CustomBertDataset(df=df_train, tokenizer=tokenizer,  label2id=tag2idx, text_col='sentence', label_col='labels')\n",
        "dataset_dev = CustomBertDataset(df=df_dev, tokenizer=tokenizer,  label2id=tag2idx, text_col='sentence', label_col='labels')\n",
        "dataset_test = CustomBertDataset(df=df_test, tokenizer=tokenizer,  label2id=tag2idx, text_col='sentence', label_col='labels')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8Pe-pW5UOHr"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "train_datloader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dev_dataloader = DataLoader(dataset_dev, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_dataloader = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGu21PPnWtPU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchcrf import CRF\n",
        "from transformers import BertModel, BertConfig\n",
        "\n",
        "class BertBiLSTMCRF(nn.Module):\n",
        "  def __init__(self, bert, num_labels, max_seq_len=124):\n",
        "    super(BertBiLSTMCRF, self).__init__()\n",
        "    self.bert = bert\n",
        "    hidden_size = self.bert.config.hidden_size\n",
        "    self.lstm_hiden = 128\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.bilstm = nn.LSTM(hidden_size, self.lstm_hiden, 1, bidirectional=True, batch_first=True, dropout=0.1)\n",
        "    self.linear = nn.Linear(self.lstm_hiden * 2, num_labels)\n",
        "    self.crf = CRF(num_labels, batch_first=True)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, labels=None):\n",
        "    bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    seq_out = bert_output[0]\n",
        "    batch_size = seq_out.size(0)\n",
        "    seq_out, _ = self.bilstm(seq_out)\n",
        "    seq_out = seq_out.contiguous().view(-1, self.lstm_hiden * 2)\n",
        "    seq_out = seq_out.contiguous().view(batch_size, self.max_seq_len, -1)\n",
        "    seq_out = self.linear(seq_out)\n",
        "\n",
        "    predicted_id = self.crf.decode(seq_out, mask=attention_mask.bool())\n",
        "    seq_len = labels.shape[-1]\n",
        "    for i, j in enumerate(predicted_id):\n",
        "        predicted_id[i] += [0]*(seq_len - attention_mask[i, :].sum())\n",
        "    predicted_id = torch.tensor(predicted_id, dtype=torch.int)\n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "      loss = -self.crf(seq_out, labels, mask=attention_mask.bool(), reduction='mean')\n",
        "    return predicted_id, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aCgV0kiWtSE"
      },
      "outputs": [],
      "source": [
        "model = BertBiLSTMCRF(bert, len(tag2idx), 124)\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLetQuTiWtUc"
      },
      "outputs": [],
      "source": [
        "weight_decay_finetune = 1e-5\n",
        "learning_rate = 5e-5\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay_finetune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygnfM0LXWtXB"
      },
      "outputs": [],
      "source": [
        "def label_prediction(model, iterator, TAG_PAD_IDX=0, exclude_pad=True):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  tokens, labels, pred_labels = [], [], []\n",
        "  with torch.no_grad():\n",
        "        for i, (batch, tags) in enumerate(iterator):\n",
        "\n",
        "            texts = batch['input_ids'].to(device)\n",
        "            tags = tags.to(device)\n",
        "            mask = batch['attention_mask'].to(device)\n",
        "            predictions, loss = model(texts, mask, tags)\n",
        "\n",
        "            for i, j in enumerate(predictions):\n",
        "              sent_tags_true = tags[i, :].cpu().numpy()\n",
        "              sent_texts = texts[i, :].cpu().numpy()\n",
        "              preds = predictions[i, :].cpu().numpy()\n",
        "              if exclude_pad:\n",
        "                args = np.where(sent_texts!=TAG_PAD_IDX)[0]\n",
        "\n",
        "                tokens.append(list(sent_texts[args]))\n",
        "                labels.append(list(sent_tags_true[args]))\n",
        "                pred_labels.append(list(preds[args]))\n",
        "\n",
        "  return tokens, labels, pred_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OUmwqAR1Yd8"
      },
      "outputs": [],
      "source": [
        "def trainer(model, train_dataloader, valid_dataloader, optimizer, idx2tag, epochs=5, pad_idx=0, clip=1):\n",
        "\n",
        "    best_acc = 0\n",
        "    best_loss = 1000\n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "\n",
        "        classes = []\n",
        "        predicted_classes = []\n",
        "        epoch_acc_train = 0\n",
        "        epoch_loss_train = 0\n",
        "\n",
        "        model.train()\n",
        "\n",
        "\n",
        "\n",
        "        for batch, train_label in tqdm(train_dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            predictions, loss = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), train_label.to(device))\n",
        "\n",
        "            train_label = train_label.view(-1).cpu()\n",
        "            predictions = predictions.view(-1).cpu()\n",
        "            texts = batch['input_ids'].to(device).view(-1).cpu()\n",
        "\n",
        "            true_pred = ((train_label == predictions) & (texts != pad_idx)).sum()\n",
        "            num_pred = (texts != pad_idx).sum()\n",
        "            acc = true_pred / num_pred\n",
        "            epoch_acc_train += acc\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)\n",
        "            optimizer.step()\n",
        "            epoch_loss_train += loss.item()\n",
        "\n",
        "        train_accuracy = epoch_acc_train / len(train_dataloader)\n",
        "        train_loss = epoch_loss_train / len(train_dataloader)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        epoch_acc_valid = 0\n",
        "        epoch_loss_valid = 0\n",
        "\n",
        "        for batch, valid_label in valid_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            predictions, loss = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), valid_label.to(device))\n",
        "\n",
        "            epoch_loss_valid += loss.item()\n",
        "\n",
        "            valid_label = valid_label.view(-1).cpu()\n",
        "            predictions = predictions.view(-1).cpu()\n",
        "            texts = batch['input_ids'].to(device).view(-1).cpu()\n",
        "\n",
        "            true_pred = ((valid_label == predictions) & (texts != pad_idx)).sum()\n",
        "            num_pred = (texts != pad_idx).sum()\n",
        "            acc = true_pred / num_pred\n",
        "            epoch_acc_valid += acc\n",
        "\n",
        "            predicted_classes.extend(predictions.cpu().numpy())\n",
        "            classes.extend(valid_label.cpu().numpy())\n",
        "\n",
        "        valid_accuracy = epoch_acc_valid / len(valid_dataloader)\n",
        "        valid_loss = epoch_loss_valid / len(valid_dataloader)\n",
        "\n",
        "        print(\n",
        "            f'Epochs: {epoch_num + 1} | Accuracy: {valid_accuracy: .3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "THC8j0YkgH6X",
        "outputId": "e7a79f8b-5726-4ce4-9f5a-252da9fa7694"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 34/218 [17:00<1:32:01, 30.01s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-ea95fbb3d2fc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtrain_datloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx2tag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-49-b17457668b53>\u001b[0m in \u001b[0;36mtrainer\u001b[0;34m(model, train_dataloader, valid_dataloader, optimizer, idx2tag, epochs, pad_idx, clip)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mtrain_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-4c0fe338a02d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mbert_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mseq_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         return self.transformer(\n\u001b[0m\u001b[1;32m    600\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    367\u001b[0m                 )\n\u001b[1;32m    368\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    370\u001b[0m                     \u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                     \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \"\"\"\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Self-Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         sa_output = self.attention(\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_lin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, q_length, dim_per_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_lin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, k_length, dim_per_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_lin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, k_length, dim_per_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim_per_head\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, q_length, dim_per_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer(model,  train_datloader, dev_dataloader, optimizer, idx2tag, epochs=10, pad_idx=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmnToz358Tm6"
      },
      "outputs": [],
      "source": [
        "tokens, labels, pred_labels = label_prediction(model, test_dataloader, TAG_PAD_IDX=0, exclude_pad=True)\n",
        "flat_labels = [el for subset in labels for el in subset]\n",
        "flat_preds = [el for subset in pred_labels for el in subset]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY_P7uAD8hA-",
        "outputId": "cc062140-4967-4e75-bb2d-221011cb0a4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      1570\n",
            "           1       0.85      1.00      0.92     17573\n",
            "           2       0.00      0.00      0.00        21\n",
            "           3       0.00      0.00      0.00       362\n",
            "           4       0.00      0.00      0.00       221\n",
            "           5       0.00      0.00      0.00        25\n",
            "           6       0.00      0.00      0.00        15\n",
            "           7       0.00      0.00      0.00      1189\n",
            "           8       0.00      0.00      0.00        28\n",
            "           9       0.00      0.00      0.00        53\n",
            "          10       0.00      0.00      0.00        94\n",
            "          11       0.00      0.00      0.00       866\n",
            "          12       0.00      0.00      0.00        79\n",
            "          13       0.00      0.00      0.00       120\n",
            "\n",
            "    accuracy                           0.86     22216\n",
            "   macro avg       0.13      0.14      0.14     22216\n",
            "weighted avg       0.74      0.86      0.80     22216\n",
            "\n"
          ]
        }
      ],
      "source": [
        "true_labels = [idx2tag[l] for l in flat_labels]\n",
        "predict_labels = [idx2tag[l]  for l in flat_preds]\n",
        "labels = [i for i in idx2tag if i not in ('<PAD>', '<UNK>')]\n",
        "print(classification_report(true_labels, predict_labels, labels=labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Пункт 3 (CNN + Transformer + CRF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNN_TRANSFORMER_CRF(nn.Module):\n",
        "    def __init__(self,\n",
        "                word2id,\n",
        "                char2id,\n",
        "                idx2tag,\n",
        "                num_classes,\n",
        "                device,\n",
        "                word_embedding_dim=300,\n",
        "                char_embedding_dim=20,\n",
        "                num_filters=100,\n",
        "                filter_size=3,\n",
        "                drop_out=0.5,\n",
        "                pad_idx=0,\n",
        "                pretrained_embedding=None,\n",
        "                ):\n",
        "        super(CNN_TRANSFORMER_CRF, self).__init__()\n",
        "\n",
        "        self.device = device\n",
        "        self.pad_idx = pad_idx\n",
        "        self.idx2tag = idx2tag\n",
        "\n",
        "        # embeddings\n",
        "        self.word_embedding = nn.Embedding(len(word2id), word_embedding_dim, padding_idx=pad_idx)\n",
        "        # init embedding, if pretrained provided - add weights\n",
        "        if type(pretrained_embedding) == np.ndarray:\n",
        "            print('add pretrained embeddings')\n",
        "            self.word_embedding.weight.data.copy_(torch.from_numpy(pretrained_embedding))\n",
        "            self.word_embedding.weight.requires_grad = True\n",
        "\n",
        "        # cnn\n",
        "        self.embedding_dim = char_embedding_dim\n",
        "        self.char_embedding = nn.Embedding(len(char2id), char_embedding_dim)\n",
        "        self.char_embedding.weight.requires_grad = True\n",
        "        self.cnn = nn.Conv3d(in_channels=1, out_channels=num_filters, kernel_size=(1, filter_size, char_embedding_dim))\n",
        "\n",
        "        # transformer\n",
        "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "\n",
        "        # crf\n",
        "        self.crf = CRF(num_classes, idx2tag)\n",
        "\n",
        "    def forward(self, words_to_id, chars_to_id, mask, label=None):\n",
        "        word_embedding = self.word_embedding(words_to_id)\n",
        "        char_embedding = self.char_embedding(chars_to_id)\n",
        "        char_embedding = char_embedding.unsqueeze(1)\n",
        "        char_embedding = self.cnn(char_embedding).squeeze(-1)\n",
        "\n",
        "        embedding = torch.cat([word_embedding, char_embedding], 2)\n",
        "\n",
        "        bert_out = self.bert(embedding)[0]\n",
        "        out = self.dropout(bert_out)\n",
        "\n",
        "        # crf\n",
        "        out = out.contiguous()\n",
        "        if label is not None:\n",
        "            loss = -self.crf(out, label, mask, reduction='mean')\n",
        "        else:\n",
        "            loss = None\n",
        "\n",
        "        predicted_id = self.crf.decode(out, mask)\n",
        "        seq_len = label.shape[-1]\n",
        "        for i, j in enumerate(predicted_id):\n",
        "            predicted_id[i] += [0]*(seq_len - mask[i, :].sum())\n",
        "        predicted_id = torch.tensor(predicted_id, dtype=torch.int)\n",
        "        return predicted_id, loss\n",
        "\n",
        "    def compute_all(self, batch):\n",
        "        words_to_id = batch['tokens']\n",
        "        chars_to_id = batch['chars']\n",
        "        label = batch['labels']\n",
        "        mask = batch['mask']\n",
        "        out = self.forward(words_to_id, chars_to_id)\n",
        "        loss = -self.crf(out, label, mask, reduction='mean')\n",
        "\n",
        "        seq_len = label.shape[-1]\n",
        "        predicted_id = self.crf.decode(out, mask)\n",
        "\n",
        "        for i, j in enumerate(predicted_id):\n",
        "          predicted_id[i] += [0]*(seq_len - mask[i, :].sum())\n",
        "        predicted_id = torch.tensor(predicted_id, dtype=torch.int).to(self.device)\n",
        "\n",
        "        true_pred = ((label == predicted_id) & (label != self.pad_idx)).sum()\n",
        "        num_pred = (label != self.pad_idx).sum()\n",
        "        acc = (true_pred / num_pred).cpu()\n",
        "        metrics = dict(acc=acc)\n",
        "\n",
        "        return loss, metrics\n",
        "\n",
        "    def compute_all(self, batch):\n",
        "        words_to_id = batch['tokens']\n",
        "        chars_to_id = batch['chars']\n",
        "        label = batch['labels']\n",
        "        mask = batch['mask']\n",
        "        out = self.forward(words_to_id, chars_to_id)\n",
        "        loss = -self.crf(out, label, mask, reduction='mean')\n",
        "\n",
        "        seq_len = label.shape[-1]\n",
        "        predicted_id = self.crf.decode(out, mask)\n",
        "\n",
        "        for i, j in enumerate(predicted_id):\n",
        "          predicted_id[i] += [0]*(seq_len - mask[i, :].sum())\n",
        "        predicted_id = torch.tensor(predicted_id, dtype=torch.int).to(self.device)\n",
        "\n",
        "        true_pred = ((label == predicted_id) & (label != self.pad_idx)).sum()\n",
        "        num_pred = (label != self.pad_idx).sum()\n",
        "        acc = (true_pred / num_pred).cpu()\n",
        "        metrics = dict(acc=acc)\n",
        "\n",
        "        return loss, metrics\n",
        "\n",
        "    def decode(self, words_to_id, chars_to_id, mask):\n",
        "        out = self.forward(words_to_id, chars_to_id)\n",
        "        predicted_id = self.crf.decode(out, mask)\n",
        "        return predicted_id\n",
        "\n",
        "    def _eval(self, valid_dataloader):\n",
        "\n",
        "        self.eval()\n",
        "\n",
        "        true_labels = []\n",
        "        predict_labels = []\n",
        "\n",
        "        for batch in valid_dataloader:\n",
        "            batch = {k: v.to(self.device) for k, v in batch.items()}\n",
        "            words_to_id = batch['tokens'].to(device)\n",
        "            chars_to_id = batch['chars'].to(device)\n",
        "            label = batch['labels'].to(device)\n",
        "            mask = batch['mask'].to(device)\n",
        "\n",
        "            out = self.decode(words_to_id, chars_to_id, mask)\n",
        "\n",
        "            for out_sentence, label_sentence in zip(out, label.tolist()):\n",
        "                    for predict_label, true_label in zip(out_sentence,label_sentence):\n",
        "                        true_labels.append(self.idx2tag[true_label])\n",
        "                        predict_labels.append(self.idx2tag[predict_label])\n",
        "        report = classification_report(true_labels, predict_labels, output_dict=True)\n",
        "        return report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Архитектура составлена по аналогии с классической задачей, но из-за блокировок в коллабе не успели корректно обучить модель (обучение полностью аналогично СNN-BiLSTM-CRF только модель наследуется из данного класса)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Er5ggu-pFyU"
      },
      "source": [
        "## Часть 3. [2 балла] Извлечение событий\n",
        "\n",
        "1. Используйте BiLSTM на эмбеддингах слов для извлечения триггеров событий.\n",
        "\n",
        "2. Замените часть модели на  словах  на ELMo и/или BERT.  Должна получиться модель ELMo / BERT + BiLSTM.\n",
        "\n",
        "[бонус] Предобучите BiLSTM как языковую модель. Дообучите ее для извлечения триггеров.\n",
        "\n",
        "[бонус] Дообучите BERT для извлечения триггеров событий."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvIeWwPQPVRj",
        "outputId": "3a6ef13d-67d5-4d74-e27d-62a727941c3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'litbank' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/dbamman/litbank.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDJnIc1nPici",
        "outputId": "e81890a6-5ea4-4957-e962-72a6f14de6c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from collections import defaultdict, Counter\n",
        "import plotly.express as px\n",
        "from sklearn.manifold import TSNE\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import json\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import os\n",
        "from copy import deepcopy\n",
        "from tqdm.auto import tqdm\n",
        "from collections import defaultdict\n",
        "# from transformers import AutoTokenizer, BertModel\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_Twr7wRP7qn"
      },
      "outputs": [],
      "source": [
        "root = '/content/litbank/events/tsv'\n",
        "files = sorted(list(os.walk(root))[0][-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOkMiURQaBrj"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def train_dev_test_split(data, train_percent = 80, dev_percent = 10, test_percent = 10):\n",
        "  random.shuffle(data)\n",
        "  train_size = int(len(data) * train_percent / 100)\n",
        "  train_data = data[:train_size]\n",
        "  dev_size = int(len(data) * dev_percent / 100)\n",
        "  dev_data = data[train_size:train_size+dev_size]\n",
        "  test_size = int(len(data) * test_percent / 100)\n",
        "  test_data = data[train_size+dev_size:]\n",
        "  return train_data, dev_data, test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPoo4jEdQM6S"
      },
      "outputs": [],
      "source": [
        "train_data, dev_data, test_data = train_dev_test_split(files, 80, 10, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "bb5eac2b687a44a0a2e10eb2b3525615",
            "9382661f7ebe4dafbbdc306304967062",
            "c611084cbcf24d2cbaea645697acbff3",
            "ad9d90edfbdf43479bf658a6266e2ccf",
            "e2cd3b91a1d448d98d1fc879ab0c5a25",
            "269e209a424e494baf87572a34821a60",
            "552d17bec2d4452eb6347d111abcb793",
            "7299fb4082874330b66ea3e34b702968",
            "ad695260b0084523b04b1cb675a6b7f4",
            "68d86a53dbf84bc1a44e9d00aab94fe9",
            "9f9157399f544ef5b258bbf43461f15a",
            "b27aba8a6d094e73af31bfe4e8f69f67",
            "3d99111cd9e544c9b50c5890db2cfa90",
            "befd3bee5ace40b4829bdab7dc61c235",
            "58b0c8e7e3d449a881d0885a3a887057",
            "9797857a31d44520afc3d92dd651f2ac",
            "43b2e87d388643c18441536a14da0aaf",
            "4f6c355e55e443b59d4d4b01a705c5ee",
            "3c2aa114920642e6b5ed8fba403d7870",
            "119080bf0bf8462db2f402092d178334",
            "ad49e9ae09134c52a38e9f2acba28b1b",
            "1b3f41ff37c14e14b7da6d187a2cb21d",
            "a250b9cad465427abf9573de198e5756",
            "2b41dd5492bb46ecaf60919545498a76",
            "29fee70b60ca42289bbb209ed2e5e3f0",
            "65f8eb5d86274448ae868d6a3f9c375e",
            "870ecf601cd9455fb9442d835af700e8",
            "27db7a2bec1844a5b73b59f2ebb2ec52",
            "be7036bee0184f648ce56f94b24f7309",
            "bab53bb6e63a44cbb40713d0496149ab",
            "74f3dd750b784644a3bfe93d49f3755f",
            "f51aa73f2939421bac81ad9e80b520cb",
            "7c6e81acf7084eaa8206ba070ba377cd"
          ]
        },
        "id": "dFjHGfGqQOBi",
        "outputId": "07a9947a-b276-46fc-8d65-e8d42265133f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb5eac2b687a44a0a2e10eb2b3525615",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/80 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b27aba8a6d094e73af31bfe4e8f69f67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a250b9cad465427abf9573de198e5756",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from string import punctuation\n",
        "\n",
        "def prepare_sents(root, files, verbose=True):\n",
        "    sents, labels = [], []\n",
        "    for f in tqdm(files, disable=not verbose):\n",
        "        path = os.path.join(root, f)\n",
        "        df = pd.read_csv(path, sep='\\t', quoting=3, header=None)\n",
        "        words = list(df[0])\n",
        "        labels_list = list(df[1])\n",
        "        sentences = []\n",
        "        labels_sent = []\n",
        "        for i in range(len(words)):\n",
        "            if words[i] not in ['.', '!', '?', '...', '']:\n",
        "                if words[i] in punctuation:\n",
        "                    continue\n",
        "                sentences.append(words[i].lower())\n",
        "                labels_sent.append(labels_list[i])\n",
        "            elif sentences != []:\n",
        "                sents.append(sentences)\n",
        "                labels.append(labels_sent)\n",
        "                sentences = []\n",
        "                labels_sent = []\n",
        "    return sents, labels\n",
        "\n",
        "sents_train, labels_train = prepare_sents(root, train_data)\n",
        "sents_dev, labels_dev = prepare_sents(root, dev_data)\n",
        "sents_test, labels_test = prepare_sents(root, test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgOhBqjZTMEw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5KUvOvgTQgs"
      },
      "outputs": [],
      "source": [
        "def vectorizing(sent, embed_dim=768):\n",
        "    sent_raw = ' '.join(sent)\n",
        "    encoded = bert_tokenizer.encode_plus(sent_raw, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        output = bert_model(**encoded)\n",
        "        states = output.hidden_states\n",
        "        concat = torch.stack(states[-4:]).sum(0).squeeze()\n",
        "    embeds = np.zeros((len(sent), embed_dim))\n",
        "    for idx in range(len(sent)):\n",
        "        token_ids_word = np.where(np.array(encoded.word_ids()) == idx)\n",
        "        embed = concat[token_ids_word].mean(dim=0)\n",
        "        embeds[idx] = embed\n",
        "    return embeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "2c01bdf6b3874219ba423433e930ad82",
            "9be8960de25f419bb25f04dad828c0a9",
            "e7f367e15bd64d8ea009517173583859",
            "572e22046bff4ea8aff649a0a9ddad14",
            "68a42e3d82f444e1b4f4a6f190fa4c25",
            "989498143bab4a0db82e026d066b3971",
            "7ea55667a2684c46b8dfbaa148404d84",
            "4087aff715614ed29d8e029f67372d6f",
            "7756f16e77b441d2838b7dfe4c93190b",
            "33a77d32129946d6871c8c54ea3e412f",
            "eed8c7c61752402d83d461dc87932e8e"
          ]
        },
        "id": "BnD99dvoVwdr",
        "outputId": "1515e90e-7297-4c01-a05e-5f4ca288cd3c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c01bdf6b3874219ba423433e930ad82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/7086 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "embeds_train = []\n",
        "for sent in tqdm(sents_train):\n",
        "    embed = vectorizing(sent)\n",
        "    embeds_train.append(embed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "47d3ef9101a04f66a6b9e0cec0b1d8c5",
            "4d7f1347c7b840d18461e6838748d4e6",
            "9eaca80b8ad84f8ba1211147259f1c2a",
            "ecff9e83c5f14cd0aa67a0a1da9e8572",
            "cc0238acb5714ebaab368e91b0dc998d",
            "67077f40a3c74c479fcdd0fd883b8eb8",
            "3221c6092d3741009243681b1712fbbe",
            "7c3bc9806e6045b1876992baf7554d18",
            "461c0975ed724c7db913db17dceb23cd",
            "40128a38636349c1b0634cdbd8fdea48",
            "17c693495fd047b0b2d6f47f39b35b9b"
          ]
        },
        "id": "jC1k_3t4WMcU",
        "outputId": "942c540c-8ff5-4cac-a21e-efaa5f26b7c5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47d3ef9101a04f66a6b9e0cec0b1d8c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/902 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "embeds_dev = []\n",
        "for sent in tqdm(sents_dev):\n",
        "    embed = vectorizing(sent)\n",
        "    embeds_dev.append(embed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djiAw9WyUFlF"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, sents, labels):\n",
        "        self.sents = sents\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sents)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sent = self.sents[idx]\n",
        "        label = self.labels[idx]\n",
        "        sent_len = len(sent)\n",
        "\n",
        "        label = (np.array(label) == 'EVENT').astype(np.int32)\n",
        "\n",
        "        return {\n",
        "            'sample': sent,\n",
        "            'label': label,\n",
        "            'length': sent_len\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75nX8Tl8U8dH"
      },
      "outputs": [],
      "source": [
        "train_ds = CustomDataset(embeds_train, labels_train)\n",
        "val_ds = CustomDataset(embeds_dev, labels_dev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtJeVZWQVL6E"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, hidden_dim=256, num_layers=1, lr_scheduler=None, lr_scheduler_type=None):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=num_layers, bidirectional=True, dropout=0.3)\n",
        "        self.lr_scheduler = lr_scheduler\n",
        "        self.lr_scheduler_type = lr_scheduler_type\n",
        "        self.criterion = nn.CrossEntropyLoss(weight=torch.tensor([1., 24.]))\n",
        "        self.embedder = nn.Linear(embedding_dim, embedding_dim // 2)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, 2)\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, x, length):\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(x, length, batch_first=True, enforce_sorted=False)\n",
        "        output, (h_n, c_n) = self.rnn(packed)\n",
        "        return self.hidden2tag(output.data)\n",
        "\n",
        "    def compute_all(self, batch):\n",
        "        sample = batch['sample'].float()\n",
        "        length = batch['length'].cpu()\n",
        "        labels = batch['label']\n",
        "\n",
        "        logits = self.forward(sample, length)\n",
        "\n",
        "        labels = nn.utils.rnn.pack_padded_sequence(labels.float(), length, batch_first=True, enforce_sorted=False).data\n",
        "\n",
        "        loss = self.criterion(logits, labels.long())\n",
        "        acc = f1_score(labels.long().cpu().numpy(), torch.argmax(logits, axis=1).detach().cpu().numpy())\n",
        "\n",
        "        metrics = dict(acc=acc, loss=loss.item())\n",
        "        return loss, metrics\n",
        "\n",
        "    def post_train_batch(self):\n",
        "        if self.lr_scheduler is not None and self.lr_scheduler_type == 'per_batch':\n",
        "            self.lr_scheduler.step()\n",
        "\n",
        "    def post_val_stage(self, val_loss):\n",
        "        if self.lr_scheduler is not None and self.lr_scheduler_type == 'per_epoch':\n",
        "            self.lr_scheduler.step(val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHYhwaQbVQv3"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    return {'sample': torch.nn.utils.rnn.pad_sequence([torch.tensor(d['sample']) for d in batch], batch_first=True),\n",
        "            'label': torch.nn.utils.rnn.pad_sequence([torch.tensor(d['label']) for d in batch], batch_first=True),\n",
        "            'length': torch.tensor([d['length'] for d in batch])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHrKIfhNVYjC"
      },
      "outputs": [],
      "source": [
        "class model_training:\n",
        "    def __init__(self, model, optimizer, train_loader, val_loader, output_folder: str = '/content/drive/MyDrive/rbg_unet/', batch_size: int = 4):\n",
        "        self.output_folder = output_folder\n",
        "        self.tboard_log_dir = './tboard_logs/'\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.train_dataset = train_loader\n",
        "        self.val_dataset = val_loader\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        shutil.rmtree(self.tboard_log_dir, ignore_errors=True)\n",
        "        if os.path.exists(self.output_folder):\n",
        "            self.load_checkpoint()\n",
        "        else:\n",
        "            os.makedirs(self.output_folder)\n",
        "            self.global_step = 0\n",
        "            self.prev_epoch = 0\n",
        "            self.best_loss = float('inf')\n",
        "            self.global_step = 0\n",
        "        self.train_writer = SummaryWriter(log_dir=self.tboard_log_dir + \"train/\")\n",
        "        self.val_writer = SummaryWriter(log_dir=self.tboard_log_dir + \"val/\")\n",
        "        self.cache = self.cache_states()\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        checkpoint = torch.load(os.path.join(self.output_folder, 'last_checkpoint.pth'))\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.prev_epoch = checkpoint['epoch_num']\n",
        "        self.best_loss = checkpoint['loss']\n",
        "        self.global_step = checkpoint['global_step']\n",
        "        shutil.copytree(os.path.join(self.output_folder, 'tboard_logs'), self.tboard_log_dir)\n",
        "        return\n",
        "\n",
        "    def save_checkpoint(self, path, model_state_dict, optimizer_state_dict, loss, epoch_num):\n",
        "        if os.path.exists(os.path.join(self.output_folder, 'tboard_logs')):\n",
        "            shutil.rmtree(os.path.join(self.output_folder, 'tboard_logs'))\n",
        "        shutil.copytree(self.tboard_log_dir, os.path.join(self.output_folder, 'tboard_logs'))\n",
        "        torch.save({\n",
        "            'model_state_dict': model_state_dict,\n",
        "            'optimizer_state_dict': optimizer_state_dict,\n",
        "            'loss': loss,\n",
        "            'epoch_num': epoch_num,\n",
        "            'global_step': self.global_step,\n",
        "        }, path)\n",
        "\n",
        "    def train(self, num_epochs: int):\n",
        "        model = self.model\n",
        "        optimizer = self.optimizer\n",
        "\n",
        "        for epoch in range(self.prev_epoch + 1, self.prev_epoch + num_epochs + 1):\n",
        "            model.train()\n",
        "            for batch in tqdm(train_loader, desc='Epoch {}'.format(epoch)):\n",
        "                batch = {k: v.float() for k, v in batch.items()}\n",
        "                loss, details = model.compute_all(batch)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                model.post_train_batch()\n",
        "                for k, v in details.items():\n",
        "                    self.train_writer.add_scalar(k, v, global_step=self.global_step)\n",
        "                self.train_writer.flush()\n",
        "                self.global_step += 1\n",
        "\n",
        "            model.eval()\n",
        "            val_accs = []\n",
        "            val_losses = []\n",
        "            val_logs = defaultdict(list)\n",
        "            for batch in tqdm(val_loader):\n",
        "                batch = {k: v.float() for k, v in batch.items()}\n",
        "                loss, details = model.compute_all(batch)\n",
        "                val_losses.append(loss.item())\n",
        "                val_accs.append(details['acc'])\n",
        "                for k, v in details.items():\n",
        "                    val_logs[k].append(v)\n",
        "            val_logs = {k: np.mean(v) for k, v in val_logs.items()}\n",
        "\n",
        "            for k, v in val_logs.items():\n",
        "                self.val_writer.add_scalar(k, v, global_step=self.global_step)\n",
        "            self.val_writer.flush()\n",
        "\n",
        "            val_acc = np.mean(val_accs)\n",
        "            val_loss = np.mean(val_losses)\n",
        "            print('Epoch #{} Loss: {} Accuracy: {}'.format(epoch, val_loss, val_acc))\n",
        "\n",
        "            model.post_val_stage(val_loss)\n",
        "            self.save_checkpoint(os.path.join(self.output_folder, 'last_checkpoint.pth'), model.state_dict(), optimizer.state_dict(), val_loss, epoch)\n",
        "            if val_loss < self.best_loss:\n",
        "                self.save_checkpoint(os.path.join(self.output_folder, 'best_checkpoint.pth'), model.state_dict(), optimizer.state_dict(), val_loss, epoch)\n",
        "                self.best_loss = val_loss\n",
        "\n",
        "    def cache_states(self):\n",
        "        return {'model_state': deepcopy(self.model.state_dict()), 'optimizer_state': deepcopy(self.optimizer.state_dict())}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqjyUhmJVZ0G"
      },
      "outputs": [],
      "source": [
        "batch_size=16\n",
        "\n",
        "model = BiLSTM(hidden_dim=64, num_layers=4)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "\n",
        "train_loader = DataLoader(train_ds, shuffle=True, pin_memory=True, batch_size=batch_size, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_ds, shuffle=False, pin_memory=True, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "training = model_training(model, opt, train_loader, val_loader, batch_size=16, output_folder = '/content/drive/MyDrive/ner-2/testing33')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293,
          "referenced_widgets": [
            "a4129a90ff504727b6deff5b04738769",
            "ad9ac7c54d7d45d28f7c5957529e5eb1",
            "be0447e56e8d4dedb482ea986e0b58ff",
            "ed57754c04254ba4a7452ad4e267077e",
            "ba5415150b484d80868638c3b0c727e2",
            "7d96d3135c4a405b84112ebe53ac23c1",
            "d3aed6bd57864ef0ab062e6a4e134b81",
            "db6158679b654e25bd1e9970de05ff8c",
            "2fbbd521a25b495486d59fd8da155200",
            "0c0d150075424b09a7f8a563bf0e6e76",
            "303f5207f0124914a0813d8a9a3599b1",
            "3379b666d6f143658f47dc52523717f7",
            "f161ddca3ee64bbc81e9015db9c738b9",
            "49d2549603694f6e8ba21b9aaeb1aa25",
            "f2b71ff37e0544e6a3d7a4dc1fa28b5c",
            "1fb2bf4ad6214f739ff5e480a65e9a9e",
            "219bdfcec54f4a03857bd71cdaa0d6f3",
            "256deff42f2d493fae706cbb7c8bc0f3",
            "3434b06835c74c45a115102dd2d0a5d5",
            "2ea2ab09ab8646c29490051e281dcfe4",
            "3e215522e2e642c69d8b1672b827ea97",
            "39504a6d9763448190a30b50f088b7cc",
            "68fc9784cb394dc7b30939a1b19be7d4",
            "703d9557ae2e41428612a1506e5f80be",
            "df2b48bab2d64a86adc7e06d1cb7d217",
            "6d45cf889a4d48f08d6de31b1510df01",
            "327a8ca3ab2949e3ac3ef1da45741b38",
            "9fb91f4958be492b86e80bc65191f3b7",
            "500065e551b04604a083f7b3eb80c692",
            "b88107f04167482595fb0e71e8bdcc30",
            "6d816b6c1b6e40878bbe141341d80a46",
            "e28e34979ad44ed986c6176a52e5c261",
            "d79a304bb89c4ae29ba492cb155d2164",
            "45b0d0747b5e4dc08bf6854146cb5101",
            "668305ed76f643adbb750d51d8e2d80e",
            "abf26c3899cd461a868c96c8994a58d4",
            "6a1c6b6af8584424837a17d761423b95",
            "7863f9522d0844a9b176e94282d2a142",
            "b914817d39de4eb28ea5cc67d180133f",
            "fafab3ad6dc64affad2c8ae82affa6e3",
            "4404bcc6cc1c472696db91b0ca8784c2",
            "780ad8ef18c74c8899424b8a39ca8835",
            "879e5df5725749e2b047f22431fdd24d",
            "0215bd9a81b64db18b0883813b71a3cb",
            "7f0ca0baa3dd43f8953627dd2cd8be5a",
            "28b8654b2660475680771adbcdb922b1",
            "2dcd7c2e329247448963525c877a5c81",
            "96b94081119e4924a0767aace060c069",
            "ed81634ecdee4ff28e0c3e7ae4c0c787",
            "cef07c1d7d294213a810ba6ac425ab63",
            "8f3934cd9a45438892086c3928eb32ca",
            "bfc205df3fb04ec890756581a512f990",
            "8b04a827e0f0404aa6a29690078928fe",
            "b33978bcb79245f09ddec32c2306567b",
            "42a8a89e41b04f83ba7b340832d3f97d",
            "4a3c2692bf904a01abd42deba26cd27c",
            "282c10f11f1d4bb8b78fba2596df157c",
            "7b6af89eda7a4dffa782f2b4cca717f3",
            "17c04812b5c14aaf97d752164317ee30",
            "bf6e02299dbf44cb8e7099ac64092c93",
            "2c9e8c5d9a4249a28fd59072bd32f120",
            "c35ded1b5a0d428cb1e4856ad8b10293",
            "f0695b5e221d4173987f869b0b20bbe4",
            "df77b41cbea2453b95dfeb2aa4b86ee9",
            "77fed715a8a8400fbfb235a93efba76b",
            "d35507e1159f44dab84e33dee4a44eb8",
            "2126e688692d41e2ad170f0e6e420a37",
            "a7f6b518fac94fa8b49bbf2c9a45c4b6",
            "baa51672e4564537b668f9117a3cb1a1",
            "5f03533609c9437f81b6e1450d267294",
            "7e8c1493258a413389da213d9fbdb970",
            "c867b0dc9dd24c5684fb9ea7e8a0b83f",
            "caf9b533c0da412785f1db48749b8fad",
            "e906c016609548c18f22fe10f27381a8",
            "ed2422c0e7e24e2abb192352bfe328d0",
            "e0dd20758b62444fbaa4d46c640fb031",
            "95fd1204cd58417eb22e2a55d29922d4"
          ]
        },
        "id": "QgZqNsACbllt",
        "outputId": "96f657b4-5db0-41a5-dfe9-a097018b1566"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4129a90ff504727b6deff5b04738769",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 15:   0%|          | 0/443 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3379b666d6f143658f47dc52523717f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/57 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #15 Loss: 0.1814485943892546 Accuracy: 0.41423061310291637\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "68fc9784cb394dc7b30939a1b19be7d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 16:   0%|          | 0/443 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45b0d0747b5e4dc08bf6854146cb5101",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/57 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #16 Loss: 0.1905989033872621 Accuracy: 0.44180158822343085\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f0ca0baa3dd43f8953627dd2cd8be5a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 17:   0%|          | 0/443 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a3c2692bf904a01abd42deba26cd27c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/57 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #17 Loss: 0.18950096609299644 Accuracy: 0.4185751916106374\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2126e688692d41e2ad170f0e6e420a37",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 18:   0%|          | 0/443 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "training.train(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL_NDqmTn6Re"
      },
      "source": [
        "## Часть 4. [2 балла] Одновременное извлечение именованных сущностей и событий\n",
        "1. Обучите модель для совместного извлечения именованных сущностей и триггеров событий. У модели должен быть общий энкодер (например, CNN + BiLSMT, ELMo + BiLSTM, BERT + BiLSTM) и два декодера: один отвечает за извлечение именнованных сущностей, другой отвечает за извлечение триггеров событий.\n",
        "\n",
        "[бонус] Добавьте в модель механизм внимания, так, как это покажется вам разумным.\n",
        "\n",
        "[бонус] Визуализируйте карты механизма внимания."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw1XocEOn6Re"
      },
      "source": [
        "Структура корпуса устроена так.\n",
        "Первый уровень:\n",
        "* entities -- разметка по сущностям\n",
        "* events -- разметка по сущностям\n",
        "\n",
        "В корпусе используются 6 типов именованных сущностей: PER, LOC, ORG, FAC, GPE, VEH (имена, локации, организации, помещения, топонимы, средства перемещния), допускаются вложенные сущности.\n",
        "\n",
        "События выражается одним словом - *триггером*, которое может быть глагом, прилагательным и существительным. В корпусе описаны события, которые действительно происходят и не имеют гипотетического характера.\n",
        "Пример: she *walked* rapidly and resolutely, здесь *walked* -- триггер события. Типы событий не заданы.\n",
        "\n",
        "Второй уровень:\n",
        "* brat -- рабочие файлы инструмента разметки brat, ann-файлы содержат разметку, txt-файлы – сырые тексты\n",
        "* tsv -- tsv-файлы содержат разметку в IOB формате\n",
        "\n",
        "Обучите модель для совместного извлечения именованных сущностей и триггеров событий. У модели должен быть общий энкодер BERT + BiLSMT и два декодера: один отвечает за извлечение именнованных сущностей, другой отвечает за извлечение триггеров событий.\n",
        "Также:\n",
        "Добавьте в модель механизм внимания, так, как это покажется вам разумным.\n",
        "Визуализируйте карты механизма внимания."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfG2LYRCKKKM"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlkjq5IHoAG4",
        "outputId": "2c350549-b9f0-455e-9dc9-fe4c61ba5e30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'litbank' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/dbamman/litbank.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4kMziSPn6Rf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "root = 'litbank/entities/tsv'\n",
        "files = sorted([\n",
        "    f\"{root}/{file}\"\n",
        "    for file in list(os.walk(root))[0][-1]])\n",
        "\n",
        "assert len(files) == 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DynKJ7xen6Rf"
      },
      "outputs": [],
      "source": [
        "from string import punctuation\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "def get_sents(path):\n",
        "    df_entities = pd.read_csv(path, sep='\\t', quoting=3, header=None)\n",
        "    df_events = pd.read_csv(path.replace('/entities/', '/events/'), sep='\\t', quoting=3, header=None)\n",
        "    df = pd.concat([\n",
        "            df_entities[[0, 1]],\n",
        "            df_events,\n",
        "        ], ignore_index=True, axis=1).dropna(subset=[0])\n",
        "\n",
        "    if len(df[df[0] != df[2]]):\n",
        "        raise ValueError(f\"Something go wrong, file = {path}\")\n",
        "\n",
        "    words = list(df[0])\n",
        "    entities_labels = list(df[1])\n",
        "    events_labels = list(df[3])\n",
        "\n",
        "    assert len(words) == len(entities_labels) and len(words) == len(events_labels), 'Что-то не так'\n",
        "    sentences = [[]]\n",
        "    entities_labels_sent = [[]]\n",
        "    events_labels_sent = [[]]\n",
        "    for i in range(len(words)):\n",
        "        if words[i] not in ['.', '!', '?', '...', '']:\n",
        "            if words[i] in punctuation:\n",
        "                continue\n",
        "            sentences[-1].append(words[i].lower())\n",
        "            entities_labels_sent[-1].append(entities_labels[i])\n",
        "            events_labels_sent[-1].append(events_labels[i])\n",
        "        elif sentences[-1] != []:\n",
        "            sentences.append([])\n",
        "            entities_labels_sent.append([])\n",
        "            events_labels_sent.append([])\n",
        "    return sentences, entities_labels_sent, events_labels_sent\n",
        "\n",
        "def prepare_sents(files, verbose=True):\n",
        "    sents, ent_labels, event_labels = [], [], []\n",
        "    for f in tqdm(files, disable=not verbose):\n",
        "        sent, ent, event = get_sents(f)\n",
        "        if sent[-1] == []:\n",
        "            sent = sent[:-1]\n",
        "            ent = ent[:-1]\n",
        "            event = event[:-1]\n",
        "        sents += sent\n",
        "        ent_labels += ent\n",
        "        event_labels += event\n",
        "    return sents, ent_labels, event_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPehgvzpn6Rf",
        "outputId": "57b46e6a-26f1-477a-e24b-7f02e6b5b811"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 112.91it/s]\n"
          ]
        }
      ],
      "source": [
        "sents, ent_labels, event_labels = prepare_sents(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgmY2vyjn6Rf"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_dataset(sents, ent_labels, event_labels, train_size=0.8, random_state=42):\n",
        "    \"\"\"\n",
        "    Split dataset into train, dev, and test sets.\n",
        "\n",
        "    Parameters:\n",
        "    sents (list): List of sentences.\n",
        "    ent_labels (list): List of entity labels.\n",
        "    event_labels (list): List of event labels.\n",
        "    train_size (float): The proportion of the dataset to include in the train split.\n",
        "    random_state (int): Controls the shuffling applied to the data before applying the split.\n",
        "\n",
        "    Returns:\n",
        "    train_data, dev_data, test_data (tuple): Train, dev, and test splits.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create data indices for train, dev, and test splits\n",
        "    data_size = len(sents)\n",
        "    train_indices, remaining_indices = train_test_split(\n",
        "        range(data_size), train_size=train_size, random_state=random_state\n",
        "    )\n",
        "    dev_indices, test_indices = train_test_split(\n",
        "        remaining_indices, train_size=0.5, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Split data into train, dev, and test splits\n",
        "    train_data = [sents[i] for i in train_indices], [ent_labels[i] for i in train_indices], [event_labels[i] for i in train_indices]\n",
        "    dev_data = [sents[i] for i in dev_indices], [ent_labels[i] for i in dev_indices], [event_labels[i] for i in dev_indices]\n",
        "    test_data = [sents[i] for i in test_indices], [ent_labels[i] for i in test_indices], [event_labels[i] for i in test_indices]\n",
        "\n",
        "    return train_data, dev_data, test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WezkGHpn6Rf",
        "outputId": "63264a3c-c771-4c01-d10d-33f529810ab4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['lead', 'him', 'not', 'into', 'temptation'],\n",
              " ['O', 'O', 'O', 'O', 'O'],\n",
              " ['O', 'O', 'O', 'O', 'O'])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data, dev_data, test_data = split_dataset(sents, ent_labels, event_labels, 0.8, random_state=42)\n",
        "\n",
        "train_data[0][0], train_data[1][0], train_data[2][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bM0tS81ZpGb8"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsqW1PUKouM6"
      },
      "outputs": [],
      "source": [
        "def flatten(lst):\n",
        "    result = []\n",
        "    for i in lst:\n",
        "        if isinstance(i, list):\n",
        "            result.extend(flatten(i))\n",
        "        else:\n",
        "            result.append(i)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqrcYIMKpWxy",
        "outputId": "5e07b6a8-be27-4bb3-bf5d-f38cd735e919"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['B-FAC',\n",
              "  'B-GPE',\n",
              "  'B-LOC',\n",
              "  'B-ORG',\n",
              "  'B-PER',\n",
              "  'B-VEH',\n",
              "  'I-FAC',\n",
              "  'I-GPE',\n",
              "  'I-LOC',\n",
              "  'I-ORG',\n",
              "  'I-PER',\n",
              "  'I-VEH',\n",
              "  'O'],\n",
              " ['EVENT', 'O'])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sorted(set(flatten(ent_labels))), sorted(set(flatten(event_labels)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9lwU6oVojU7"
      },
      "outputs": [],
      "source": [
        "entities_dict = {\n",
        "    'B-FAC': 12,\n",
        "    'B-GPE': 11,\n",
        "    'B-LOC': 10,\n",
        "    'B-ORG': 9,\n",
        "    'B-PER': 8,\n",
        "    'B-VEH': 7,\n",
        "    'I-FAC': 6,\n",
        "    'I-GPE': 5,\n",
        "    'I-LOC': 4,\n",
        "    'I-ORG': 3,\n",
        "    'I-PER': 2,\n",
        "    'I-VEH': 1,\n",
        "    'O': 0,\n",
        "}\n",
        "triggers_dict = {\n",
        "    'EVENT': 1,\n",
        "    'O': 0,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8E-BCRCn6Rg"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from transformers import DistilBertTokenizer\n",
        "import torch\n",
        "\n",
        "class EntityEventDataset(Dataset):\n",
        "    def __init__(self, sentences, entities, triggers):\n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "        self.sentences = sentences\n",
        "        self.entities = entities\n",
        "        self.triggers = triggers\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        entities = self.entities[idx]\n",
        "        triggers = self.triggers[idx]\n",
        "        encoding = self.tokenizer.encode_plus(sentence, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "        entity_ids = torch.tensor([entities_dict[key] for key in entities])\n",
        "        trigger_ids = torch.tensor([triggers_dict[key] for key in triggers])\n",
        "        return {'input_ids': input_ids,\n",
        "                'attention_mask': attention_mask,\n",
        "                'entity_ids': entity_ids,\n",
        "                'trigger_ids': trigger_ids}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMLr2eQan6Rg"
      },
      "outputs": [],
      "source": [
        "train_dataset = EntityEventDataset(*train_data)\n",
        "dev_dataset = EntityEventDataset(*dev_data)\n",
        "test_dataset = EntityEventDataset(*test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zouKoWE3n6Rh",
        "outputId": "8b8712ad-8d01-40bf-f81d-386a9ed625c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogmEgP40n6Rh"
      },
      "outputs": [],
      "source": [
        "from torch.nn import LSTM, Linear\n",
        "from transformers import DistilBertModel\n",
        "from tqdm import tqdm, trange\n",
        "from IPython.display import clear_output\n",
        "import gc\n",
        "gc.enable()\n",
        "\n",
        "class EntityEventModel(torch.nn.Module):\n",
        "    def __init__(self, hidden_dim, dropout_prob, device):\n",
        "        super(EntityEventModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dropout_prob = dropout_prob\n",
        "        self.device = device\n",
        "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.lstm = LSTM(768, hidden_dim, num_layers=1, batch_first=True, dropout=dropout_prob, bidirectional=True)\n",
        "        self.entity_classifier = Linear(hidden_dim * 2, 13)\n",
        "        self.trigger_classifier = Linear(hidden_dim * 2, 2)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.entity_classifier.bias.data.zero_()\n",
        "        self.entity_classifier.weight.data.uniform_(-initrange, initrange)\n",
        "        self.trigger_classifier.bias.data.zero_()\n",
        "        self.trigger_classifier.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, entity_ids, trigger_ids):\n",
        "        bert_outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        outputs, _ = self.lstm(bert_outputs.last_hidden_state)\n",
        "        entity_outputs = torch.sigmoid(self.entity_classifier(outputs)).squeeze()\n",
        "        trigger_outputs = torch.sigmoid(self.trigger_classifier(outputs)).squeeze()\n",
        "\n",
        "        entity_outputs = entity_outputs[attention_mask == 1][1:-1]\n",
        "        trigger_outputs = trigger_outputs[attention_mask == 1][1:-1]\n",
        "        return entity_outputs, trigger_outputs\n",
        "\n",
        "    def train_model(self, train_loader, valid_loader, num_epochs, lr):\n",
        "        self.to(self.device)\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "        loss_function = torch.nn.BCEWithLogitsLoss()\n",
        "        for epoch in range(num_epochs):\n",
        "            # clear_output()\n",
        "            print(f\"Epoch {epoch + 1}\")\n",
        "            self.train()\n",
        "            total_loss = 0\n",
        "            for data in tqdm(train_loader):\n",
        "                input_ids = data['input_ids'].to(self.device)\n",
        "                attention_mask = data['attention_mask'].to(self.device)\n",
        "                entity_ids = data['entity_ids'].to(self.device)\n",
        "                trigger_ids = data['trigger_ids'].to(self.device)\n",
        "                outputs = self(input_ids, attention_mask, entity_ids, trigger_ids)\n",
        "\n",
        "                entity_loss = loss_function(outputs[0], torch.nn.functional.one_hot(entity_ids, 13).float())\n",
        "                trigger_loss = loss_function(outputs[1], torch.nn.functional.one_hot(trigger_ids, 2).float())\n",
        "                loss = entity_loss + trigger_loss\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "            avg_train_loss = total_loss / len(train_loader)\n",
        "            valid_loss = self.evaluate_model(valid_loader)\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.6f} | Valid Loss: {valid_loss:.6f}\")\n",
        "            print()\n",
        "\n",
        "\n",
        "    def evaluate_model(self, valid_loader):\n",
        "      self.eval()\n",
        "      total_loss = 0\n",
        "      loss_function = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for data in valid_loader:\n",
        "              input_ids = data['input_ids'].to(self.device)\n",
        "              attention_mask = data['attention_mask'].to(self.device)\n",
        "              entity_ids = data['entity_ids'].to(self.device)\n",
        "              trigger_ids = data['trigger_ids'].to(self.device)\n",
        "              outputs = self(input_ids, attention_mask, entity_ids, trigger_ids)\n",
        "\n",
        "              entity_loss = loss_function(outputs[0], torch.nn.functional.one_hot(entity_ids, 13).float())\n",
        "              trigger_loss = loss_function(outputs[1], torch.nn.functional.one_hot(trigger_ids, 2).float())\n",
        "              loss = entity_loss + trigger_loss\n",
        "              total_loss += loss.item()\n",
        "\n",
        "      avg_valid_loss = total_loss / len(valid_loader)\n",
        "      return avg_valid_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAmUEg5cFH6F",
        "outputId": "1cd62258-0904-4c0a-8a31-cbf013513e71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6939/6939 [02:52<00:00, 40.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 | Train Loss: 1.656332 | Valid Loss: 1.656118\n",
            "\n",
            "Epoch 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6939/6939 [02:52<00:00, 40.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/5 | Train Loss: 1.656320 | Valid Loss: 1.656118\n",
            "\n",
            "Epoch 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6939/6939 [02:52<00:00, 40.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/5 | Train Loss: 1.656289 | Valid Loss: 1.656118\n",
            "\n",
            "Epoch 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6939/6939 [02:54<00:00, 39.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/5 | Train Loss: 1.656287 | Valid Loss: 1.656118\n",
            "\n",
            "Epoch 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6939/6939 [02:58<00:00, 38.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/5 | Train Loss: 1.656327 | Valid Loss: 1.656118\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = EntityEventModel(256, 0.2, device)\n",
        "model.train_model(train_dataset, dev_dataset, 5, 0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD4-cjvaKxwg"
      },
      "source": [
        "## Часть 5. [1 балл] Итоги\n",
        "Напишите краткое резюме проделанной работы. Сравните результаты всех разработанных моделей. Что помогло вам в выполнении работы, чего не хватало?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4ALOcVOK1pI"
      },
      "source": [
        "**Часть 1.**\n",
        "Был выполнен частотный анализ сущностей и событий. Также события были разделены по кластерам.\n",
        "\n",
        "**Часть 2.**\n",
        "В выполнении работы помогли ссылки на корпус и на статью по извлечению именованных сущностей.\n",
        "\n",
        "В части 2 выполнено извлечение сущностей.\n",
        "Все модели достаточно долго работают, но дают неплохие результаты.\n",
        "\n",
        "Не хватило времени для более глубокой работы с моделями, так как обучение занимает большое время. Также много времени заняла отладка конвертируемости.\n",
        "Еще хотелось бы, чтобы в семинарах было что-то посложнее, чем то, что в них есть сейчас. Для выполнения задания они совсем не помогли. Либо нужно начинать задание с более базовых моделей, чтобы быстрее понять принципы и перейти к усложению.\n",
        "\n",
        "Преимущества:\n",
        "1. За счет использования CNNBiLSTMCRF получилось хорошее качество, так как мы использовали сочетание семантического и синтаксического представлений.\n",
        "2. При добавлении BERT качество осталось хорошим, так как BERT - одна из самых сильных моделей для текстовой классификации.\n",
        "\n",
        "Недостатки:\n",
        "1. Память + время\n",
        "\n",
        "**Часть 3.**\n",
        "В части 3 выполнено извлечение триггеров событий.\n",
        "В выполнении работы помогли ссылки на корпус.\n",
        "\n",
        "Были ощутимые проблемы с конвертируемостью данных во время препроцессинга.\n",
        "\n",
        "Преимущества:\n",
        "1. Хорошее качество модели при использовани BERT, так как BERT - одна из самых сильных моделей для текстовой классификации.\n",
        "\n",
        "Недостатки:\n",
        "1. Память + время\n",
        "\n",
        "**Часть 4.**\n",
        "В данной работе была предпринята попытка создания модели для обнаружения сущностей и триггеров с использованием BERT и BiLSTM.\n",
        "\n",
        "Преимущества:\n",
        "\n",
        "Использование билинейного слоя позволяет раздельно обучать классификаторы сущностей и триггеров, что улучшает общую производительность модели.\n",
        "\n",
        "Недостатки:\n",
        "\n",
        "Размер модели, включая предварительно обученную модель BERT, может быть большим для работы на устройствах с ограниченными ресурсами. (у нас так и случилось)\n",
        "\n",
        "Для улучшения модели было бы полезно использовать более сложные архитектуры, такие как трансформеры с вложенными масками, которые в состоянии лучше захватывать зависимости между токенами в предложении."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyhw6nEmNBiy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "CchqBqgFY7hM",
        "Q0sN7KFPpAs1"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
